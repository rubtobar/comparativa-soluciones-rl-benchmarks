{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A2C.ipynb","provenance":[{"file_id":"1DGlGW9kjd_C2gesvamv4PtK4d1OvGhkM","timestamp":1624894126227},{"file_id":"https://github.com/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/rl-baselines-zoo.ipynb","timestamp":1624881354046}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"76vpKaVvcBDM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625070796010,"user_tz":-120,"elapsed":39260,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"920714a0-5bf6-4578-cd9d-8fabce84d307"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OfyBs0XscOpi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625070799531,"user_tz":-120,"elapsed":1047,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"0b5423a4-ff32-4ddc-820b-f7c72eb624b0"},"source":["%cd /content/drive/MyDrive/Colab\\ Notebooks/TFM/Seaquest"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/TFM/Seaquest\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AXVDDlTn02M9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625070809944,"user_tz":-120,"elapsed":7549,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"3aac3ffa-93f2-4dc1-c259-39144ebf220e"},"source":["!apt-get install swig cmake ffmpeg freeglut3-dev xvfb"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","freeglut3-dev is already the newest version (2.8.1-3).\n","freeglut3-dev set to manually installed.\n","cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n","ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n","The following additional packages will be installed:\n","  swig3.0\n","Suggested packages:\n","  swig-doc swig-examples swig3.0-examples swig3.0-doc\n","The following NEW packages will be installed:\n","  swig swig3.0 xvfb\n","0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n","Need to get 1,885 kB of archives.\n","After this operation, 8,093 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n","Fetched 1,885 kB in 1s (1,359 kB/s)\n","Selecting previously unselected package swig3.0.\n","(Reading database ... 160772 files and directories currently installed.)\n","Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n","Unpacking swig3.0 (3.0.12-1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n","Unpacking swig (3.0.12-1) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n","Setting up swig3.0 (3.0.12-1) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n","Setting up swig (3.0.12-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kDjF3qRg7oGH"},"source":["## Clone RL Baselines3 Zoo Repo"]},{"cell_type":"code","metadata":{"id":"SCjGikdT1DFy"},"source":["#!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REMQlh-ezyVt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625070809945,"user_tz":-120,"elapsed":10,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"df728970-d44e-4b51-8587-a30c22485f02"},"source":["%cd rl-baselines3-zoo/"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/TFM/Seaquest/rl-baselines3-zoo\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5tmD_QTBqTMb"},"source":["### Install pip dependencies"]},{"cell_type":"code","metadata":{"id":"OWIDzgJTqShY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625071088676,"user_tz":-120,"elapsed":277052,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"d59d6c9a-74fc-4046-b01a-284b6c507254"},"source":["!pip install -r requirements.txt"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting stable-baselines3[docs,extra,tests]>=1.1.0a11\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/e9/286bc266fb268267b016f130b177ac8dcf4566a7d0ccea93c698b5cfb130/stable_baselines3-1.1.0a11-py3-none-any.whl (172kB)\n","\u001b[K     |████████████████████████████████| 174kB 8.7MB/s \n","\u001b[?25hCollecting box2d-py==2.3.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n","\u001b[K     |████████████████████████████████| 450kB 42.0MB/s \n","\u001b[?25hCollecting pybullet\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/6d/60b97ffc579db665bdd87f2cb47fe1215ae770fbbc1add84ebf36ddca63b/pybullet-3.1.7.tar.gz (79.0MB)\n","\u001b[K     |████████████████████████████████| 79.0MB 38kB/s \n","\u001b[?25hCollecting gym-minigrid\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/d2/e2b0fc791c9d22c70ea48df762133d7ad930ea09e1bfa95a24a1c86ddf18/gym_minigrid-1.0.2-py3-none-any.whl (47kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n","\u001b[?25hCollecting scikit-optimize\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n","\u001b[K     |████████████████████████████████| 102kB 13.6MB/s \n","\u001b[?25hCollecting optuna\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/18/b49ca91cf592747e19f2d333c2a86cd7c81895b922a5a09adf6335471576/optuna-2.8.0-py3-none-any.whl (301kB)\n","\u001b[K     |████████████████████████████████| 307kB 52.7MB/s \n","\u001b[?25hCollecting pytablewriter\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/da/10dd501deac8fe84f5ecf83f314a5372676284c8084d497ab15f32958676/pytablewriter-0.61.0-py3-none-any.whl (99kB)\n","\u001b[K     |████████████████████████████████| 102kB 14.1MB/s \n","\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.11.1)\n","Collecting pyyaml>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n","\u001b[K     |████████████████████████████████| 645kB 43.4MB/s \n","\u001b[?25hCollecting sb3-contrib>=1.1.0a11\n","  Downloading https://files.pythonhosted.org/packages/81/e1/cdd3c126cb905b2275b0563aeb19f5e5d12f04cd37372825a8bfb05053a3/sb3_contrib-1.1.0a11-py3-none-any.whl\n","Collecting cloudpickle>=1.5.0\n","  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n","\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'atari-py' candidate (version 0.2.6 at https://files.pythonhosted.org/packages/8f/ba/1d22e9d2f332f07aaa57041f5dd569c2cb40a92bd6374a0b743ec3dfae97/atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl#sha256=d9e2c25d39783867c2f29d1dd9d3a659fc56036456d07dc9efe8bd7bb31a07d7 (from https://pypi.org/simple/atari-py/))\n","Reason for being yanked: re-release with new wheels\u001b[0m\n","Collecting atari-py==0.2.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/ba/1d22e9d2f332f07aaa57041f5dd569c2cb40a92bd6374a0b743ec3dfae97/atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (2.8MB)\n","\u001b[K     |████████████████████████████████| 2.8MB 36.4MB/s \n","\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (4.4.1)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.9.0+cu102)\n","Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.17.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.1.5)\n","Collecting sphinxcontrib.spelling; extra == \"docs\"\n","  Downloading https://files.pythonhosted.org/packages/50/9d/7fd15b645c7eec20c6fe85b392ae296ccd893fec8179645dc81d0bae4ad8/sphinxcontrib_spelling-7.2.1-py3-none-any.whl\n","Collecting sphinx-rtd-theme; extra == \"docs\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/24/2475e8f83519b54b2148d4a56eb1111f9cec630d088c3ffc214492c12107/sphinx_rtd_theme-0.5.2-py2.py3-none-any.whl (9.1MB)\n","\u001b[K     |████████████████████████████████| 9.2MB 31.9MB/s \n","\u001b[?25hCollecting sphinx-autodoc-typehints; extra == \"docs\"\n","  Downloading https://files.pythonhosted.org/packages/25/04/f59887284d9ea7e5e1473b74177fc8fca43c949a683750c733a154ba8148/sphinx_autodoc_typehints-1.12.0-py3-none-any.whl\n","Collecting sphinx-autobuild; extra == \"docs\"\n","  Downloading https://files.pythonhosted.org/packages/7e/7d/8fb7557b6c9298d2bcda57f4d070de443c6355dfb475582378e2aa16a02c/sphinx_autobuild-2021.3.14-py3-none-any.whl\n","Requirement already satisfied: sphinx; extra == \"docs\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.8.5)\n","Requirement already satisfied: opencv-python; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (4.1.2.30)\n","Requirement already satisfied: tensorboard>=2.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.5.0)\n","Requirement already satisfied: psutil; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (5.4.8)\n","Requirement already satisfied: pillow; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (7.1.2)\n","Collecting pytest-env; extra == \"tests\"\n","  Downloading https://files.pythonhosted.org/packages/f9/6e/31efb8dc1d17052c12f39262223e94038bfcc4cc7a124235630a6d50f166/pytest-env-0.6.2.tar.gz\n","Collecting flake8-bugbear; extra == \"tests\"\n","  Downloading https://files.pythonhosted.org/packages/4d/e4/180dac4cd20c20b94fea1f5ea05814dab0ae50593c6e8791f02086db5d86/flake8_bugbear-21.4.3-py36.py37.py38-none-any.whl\n","Collecting pytest-cov; extra == \"tests\"\n","  Downloading https://files.pythonhosted.org/packages/ba/84/576b071aef9ac9301e5c0ff35d117e12db50b87da6f12e745e9c5f745cc2/pytest_cov-2.12.1-py2.py3-none-any.whl\n","Collecting pytest-xdist; extra == \"tests\"\n","  Downloading https://files.pythonhosted.org/packages/6b/fb/8908bb19c3e073b43727db01d4b488cb75fdc00f76ee9b703d23c7119738/pytest_xdist-2.3.0-py3-none-any.whl\n","Collecting isort>=5.0; extra == \"tests\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/3f/4e39910865572d2ff209e601d9c1d15180ef1b735538a0c7bc2d15b63ac6/isort-5.9.1-py3-none-any.whl (105kB)\n","\u001b[K     |████████████████████████████████| 112kB 56.4MB/s \n","\u001b[?25hRequirement already satisfied: pytest; extra == \"tests\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (3.6.4)\n","Collecting pytype; extra == \"tests\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/55/40eebf8326c1be4536cb984e543ef6644c01b0e02f6e537ddaee7ea5f664/pytype-2021.6.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0MB)\n","\u001b[K     |████████████████████████████████| 2.0MB 31.4MB/s \n","\u001b[?25hCollecting black; extra == \"tests\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d0/154973fbb48aeda17cd117507872079de82408bb16f6f2ead3d05be68bd6/black-21.6b0-py3-none-any.whl (140kB)\n","\u001b[K     |████████████████████████████████| 143kB 56.7MB/s \n","\u001b[?25hCollecting flake8>=3.8; extra == \"tests\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/80/35a0716e5d5101e643404dabd20f07f5528a21f3ef4032d31a49c913237b/flake8-3.9.2-py2.py3-none-any.whl (73kB)\n","\u001b[K     |████████████████████████████████| 81kB 12.2MB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r requirements.txt (line 5)) (1.0.1)\n","Collecting pyaml>=16.9\n","  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r requirements.txt (line 5)) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r requirements.txt (line 5)) (0.22.2.post1)\n","Collecting colorlog\n","  Downloading https://files.pythonhosted.org/packages/32/e6/e9ddc6fa1104fda718338b341e4b3dc31cd8039ab29e52fc73b508515361/colorlog-5.0.1-py2.py3-none-any.whl\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->-r requirements.txt (line 6)) (1.4.18)\n","Collecting cmaes>=0.8.2\n","  Downloading https://files.pythonhosted.org/packages/01/1f/43b01223a0366171f474320c6e966c39a11587287f098a5f09809b45e05f/cmaes-0.8.2-py3-none-any.whl\n","Collecting cliff\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/11/aea1cacbd4cf8262809c4d6f95dcb3f2802594de1f51c5bd454d69bf15c5/cliff-3.8.0-py3-none-any.whl (80kB)\n","\u001b[K     |████████████████████████████████| 81kB 12.1MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna->-r requirements.txt (line 6)) (4.41.1)\n","Collecting alembic\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/80/ef186e599a57d0e4cb78fc76e0bfc2e6953fa9716b2a5cf2de0117ed8eb5/alembic-1.6.5-py2.py3-none-any.whl (164kB)\n","\u001b[K     |████████████████████████████████| 174kB 59.9MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->-r requirements.txt (line 6)) (20.9)\n","Collecting DataProperty<2,>=0.50.1\n","  Downloading https://files.pythonhosted.org/packages/b9/5f/c773c362fcba227d6a4021225cb0213b51849c7dc9c93004d34d9004078b/DataProperty-0.50.1-py3-none-any.whl\n","Collecting mbstrdecoder<2,>=1.0.0\n","  Downloading https://files.pythonhosted.org/packages/e8/f6/0e6bb50c3c6380a4982c87d80e70b2f6e366523a57a0c58594aea472206d/mbstrdecoder-1.0.1-py3-none-any.whl\n","Collecting tcolorpy<1,>=0.0.5\n","  Downloading https://files.pythonhosted.org/packages/96/73/2a73a7d53df3708636dae4e817814d07e455efd53897476f3863925cf0af/tcolorpy-0.1.1-py3-none-any.whl\n","Collecting tabledata<2,>=1.1.3\n","  Downloading https://files.pythonhosted.org/packages/85/93/4c695da7e6589e1e4b513c02d5b562dcc5afacb8a2f6cac8eb2ac2e88833/tabledata-1.1.4-py3-none-any.whl\n","Collecting pathvalidate<3,>=2.3.0\n","  Downloading https://files.pythonhosted.org/packages/87/55/7d63b78986f1f8764180b84ee3e8a47c583ec059d32c98d8fba7fc0dc1ae/pathvalidate-2.4.1-py3-none-any.whl\n","Collecting typepy[datetime]<2,>=1.1.4\n","  Downloading https://files.pythonhosted.org/packages/60/3a/1239e59924250d9c2dd1d5b84748da82d15aaa241b3ceeffa08aa5eba589/typepy-1.1.5-py3-none-any.whl\n","Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter->-r requirements.txt (line 7)) (57.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py==0.2.6->-r requirements.txt (line 13)) (1.15.0)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->-r requirements.txt (line 14)) (1.3.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (3.7.4.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.5.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.3.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2018.9)\n","Collecting PyEnchant>=3.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/f1/162fc6975068098e3327358216b70bbecba1d8004438c3bc8fe9f9378a89/pyenchant-3.2.1-py3-none-any.whl (55kB)\n","\u001b[K     |████████████████████████████████| 61kB 9.9MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (4.5.0)\n","Collecting docutils<0.17\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/44/8a15e45ffa96e6cf82956dd8d7af9e666357e16b0d93b253903475ee947f/docutils-0.16-py2.py3-none-any.whl (548kB)\n","\u001b[K     |████████████████████████████████| 552kB 45.0MB/s \n","\u001b[?25hCollecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Collecting livereload\n","  Downloading https://files.pythonhosted.org/packages/bd/60/6640b819e858562ef6684abac60593b7369fe0a8a064df426d3ab0ab894d/livereload-2.6.3.tar.gz\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.2.0)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.6.1)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.2.4)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.1.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.9.1)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.7.12)\n","Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.11.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.8.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.36.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.12.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.6.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.31.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.4.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.34.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (3.12.4)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from flake8-bugbear; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (21.2.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytest-cov; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.10.2)\n","Collecting coverage>=5.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/e0/fc9f7bd9b84e6b41d0aad1a113e36714aac0c0a9b307aca5f9af443bc50f/coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242kB)\n","\u001b[K     |████████████████████████████████| 245kB 54.7MB/s \n","\u001b[?25hCollecting pytest-forked\n","  Downloading https://files.pythonhosted.org/packages/9d/88/77eeb091b4fa79f28c08718f6e6ebff5827d9d1c1dd9974218ddfbe031ee/pytest_forked-1.3.0-py2.py3-none-any.whl\n","Collecting execnet>=1.1\n","  Downloading https://files.pythonhosted.org/packages/81/c0/3072ecc23f4c5e0a1af35e3a222855cfd9c80a1a105ca67be3b6172637dd/execnet-1.9.0-py2.py3-none-any.whl\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (8.8.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.7.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.4.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.10.0)\n","Collecting importlab>=0.6.1\n","  Downloading https://files.pythonhosted.org/packages/3a/68/97775a75cb03949ecfda7ac61182e11ad22d4e9ae9d2f61c3417de9b2aac/importlab-0.6.1.tar.gz\n","Collecting typed-ast>=1.4.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n","\u001b[K     |████████████████████████████████| 747kB 37.0MB/s \n","\u001b[?25hCollecting ninja>=1.10.0.post2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n","\u001b[K     |████████████████████████████████| 112kB 61.7MB/s \n","\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.4.4)\n","Collecting mypy-extensions>=0.4.3\n","  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n","Collecting pathspec<1,>=0.8.1\n","  Downloading https://files.pythonhosted.org/packages/29/29/a465741a3d97ea3c17d21eaad4c64205428bde56742360876c4391f930d4/pathspec-0.8.1-py2.py3-none-any.whl\n","Collecting regex>=2020.1.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/28/5f08d8841013ccf72cd95dfff2500fe7fb39467af12c5e7b802d8381d811/regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720kB)\n","\u001b[K     |████████████████████████████████| 727kB 39.2MB/s \n","\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (7.1.2)\n","Collecting mccabe<0.7.0,>=0.6.0\n","  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n","Collecting pyflakes<2.4.0,>=2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/11/2a745612f1d3cbbd9c69ba14b1b43a35a2f5c3c81cd0124508c52c64307f/pyflakes-2.3.1-py2.py3-none-any.whl (68kB)\n","\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n","\u001b[?25hCollecting pycodestyle<2.8.0,>=2.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/cc/227251b1471f129bc35e966bb0fceb005969023926d744139642d847b7ae/pycodestyle-2.7.0-py2.py3-none-any.whl (41kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n","\u001b[?25hRequirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->-r requirements.txt (line 6)) (1.1.0)\n","Collecting stevedore>=2.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n","\u001b[?25hCollecting cmd2>=1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/6a/e929ec70ca05c5962f6541ef29fb9c207dd41f0f2333680fa39f44fa4357/cmd2-2.1.1-py3-none-any.whl (140kB)\n","\u001b[K     |████████████████████████████████| 143kB 56.0MB/s \n","\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e0/1d4702dd81121d04a477c272d47ee5b6bc970d1a0990b11befa275c55cf2/pbr-5.6.0-py2.py3-none-any.whl (111kB)\n","\u001b[K     |████████████████████████████████| 112kB 56.9MB/s \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->-r requirements.txt (line 6)) (2.1.0)\n","Collecting python-editor>=0.3\n","  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n","Collecting Mako\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/54/dbc07fbb20865d3b78fdb7cf7fa713e2cba4f87f71100074ef2dc9f9d1f7/Mako-1.1.4-py2.py3-none-any.whl (75kB)\n","\u001b[K     |████████████████████████████████| 81kB 12.7MB/s \n","\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->-r requirements.txt (line 7)) (3.0.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.16.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->sphinxcontrib.spelling; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (3.4.1)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from livereload->sphinx-autobuild; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (5.1.1)\n","Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.1.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx; extra == \"docs\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.0.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.7/dist-packages (from importlab>=0.6.1->pytype; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (2.5.1)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->-r requirements.txt (line 6)) (0.2.5)\n","Collecting pyperclip>=1.6\n","  Downloading https://files.pythonhosted.org/packages/a7/2c/4c64579f847bd5d539803c8b909e54ba087a79d01bb3aba433a95879a6c5/pyperclip-1.8.2.tar.gz\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (3.1.1)\n","Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2->importlab>=0.6.1->pytype; extra == \"tests\"->stable-baselines3[docs,extra,tests]>=1.1.0a11->-r requirements.txt (line 1)) (4.4.2)\n","Building wheels for collected packages: pybullet, pytest-env, livereload, importlab, pyperclip\n","  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pybullet: filename=pybullet-3.1.7-cp37-cp37m-linux_x86_64.whl size=89750737 sha256=4ace5f57dbaefa46f8a0cd28da0c75cd7b9b5ea2127765987c58235d8a2c3036\n","  Stored in directory: /root/.cache/pip/wheels/30/56/e6/fce8276a2f30165f7ac31089bb72f390fa16b87328651e1a5a\n","  Building wheel for pytest-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytest-env: filename=pytest_env-0.6.2-cp37-none-any.whl size=2374 sha256=9447493dbc7cdc4f13ce54bf6837b7b2d645c0c7eb2e09642444c43cca4e6628\n","  Stored in directory: /root/.cache/pip/wheels/75/fc/26/357bc139b726977ab08817308ce76d96a26bdbe2329a43b15a\n","  Building wheel for livereload (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for livereload: filename=livereload-2.6.3-py2.py3-none-any.whl size=24726 sha256=cfdb882bfe01f331c9e852f21316f0f1487676cc0830f474c0fe9fc7e0d7f3fe\n","  Stored in directory: /root/.cache/pip/wheels/f2/a1/fa/00775311351a48f78b4a0e40588eefdd6f2731cc8115d6957d\n","  Building wheel for importlab (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for importlab: filename=importlab-0.6.1-py2.py3-none-any.whl size=21304 sha256=a6083decd178f2bc09315e7796e940a02e6fe8ce398ae31acaa50937b880b188\n","  Stored in directory: /root/.cache/pip/wheels/bf/96/8c/797cf6981feefedd5a95412849761432267806a82dd8d985f3\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-cp37-none-any.whl size=11136 sha256=867652cff7450dc303867e1974da33d9bd3fb9f8018c883c3b4dd0c6d15c096d\n","  Stored in directory: /root/.cache/pip/wheels/25/af/b8/3407109267803f4015e1ee2ff23be0c8c19ce4008665931ee1\n","Successfully built pybullet pytest-env livereload importlab pyperclip\n","\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: sphinxcontrib-spelling 7.2.1 has requirement Sphinx>=3.0.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: sphinx-autodoc-typehints 1.12.0 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pytest-cov 2.12.1 has requirement pytest>=4.6, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pytest-forked 1.3.0 has requirement pytest>=3.10, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pytest-xdist 2.3.0 has requirement pytest>=6.0.0, but you'll have pytest 3.6.4 which is incompatible.\u001b[0m\n","Installing collected packages: cloudpickle, PyEnchant, sphinxcontrib.spelling, docutils, sphinx-rtd-theme, sphinx-autodoc-typehints, colorama, livereload, sphinx-autobuild, atari-py, pytest-env, mccabe, pyflakes, pycodestyle, flake8, flake8-bugbear, coverage, pytest-cov, pytest-forked, execnet, pytest-xdist, isort, importlab, pyyaml, typed-ast, ninja, pytype, mypy-extensions, pathspec, regex, black, stable-baselines3, box2d-py, pybullet, gym-minigrid, pyaml, scikit-optimize, colorlog, cmaes, pbr, stevedore, pyperclip, cmd2, cliff, python-editor, Mako, alembic, optuna, mbstrdecoder, typepy, DataProperty, tcolorpy, tabledata, pathvalidate, pytablewriter, sb3-contrib\n","  Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","  Found existing installation: docutils 0.17.1\n","    Uninstalling docutils-0.17.1:\n","      Successfully uninstalled docutils-0.17.1\n","  Found existing installation: atari-py 0.2.9\n","    Uninstalling atari-py-0.2.9:\n","      Successfully uninstalled atari-py-0.2.9\n","  Found existing installation: coverage 3.7.1\n","    Uninstalling coverage-3.7.1:\n","      Successfully uninstalled coverage-3.7.1\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","Successfully installed DataProperty-0.50.1 Mako-1.1.4 PyEnchant-3.2.1 alembic-1.6.5 atari-py-0.2.6 black-21.6b0 box2d-py-2.3.8 cliff-3.8.0 cloudpickle-1.6.0 cmaes-0.8.2 cmd2-2.1.1 colorama-0.4.4 colorlog-5.0.1 coverage-5.5 docutils-0.16 execnet-1.9.0 flake8-3.9.2 flake8-bugbear-21.4.3 gym-minigrid-1.0.2 importlab-0.6.1 isort-5.9.1 livereload-2.6.3 mbstrdecoder-1.0.1 mccabe-0.6.1 mypy-extensions-0.4.3 ninja-1.10.0.post2 optuna-2.8.0 pathspec-0.8.1 pathvalidate-2.4.1 pbr-5.6.0 pyaml-20.4.0 pybullet-3.1.7 pycodestyle-2.7.0 pyflakes-2.3.1 pyperclip-1.8.2 pytablewriter-0.61.0 pytest-cov-2.12.1 pytest-env-0.6.2 pytest-forked-1.3.0 pytest-xdist-2.3.0 python-editor-1.0.4 pytype-2021.6.17 pyyaml-5.4.1 regex-2021.4.4 sb3-contrib-1.1.0a11 scikit-optimize-0.8.1 sphinx-autobuild-2021.3.14 sphinx-autodoc-typehints-1.12.0 sphinx-rtd-theme-0.5.2 sphinxcontrib.spelling stable-baselines3-1.1.0a11 stevedore-3.3.0 tabledata-1.1.4 tcolorpy-0.1.1 typed-ast-1.4.3 typepy-1.1.5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6gJ-pAbF7zRZ"},"source":["## Train an SeaquestNoFrameskip-v4 DQN\n","\n","Steps: 1M\n","\n","Env: Seaquest\n"]},{"cell_type":"code","metadata":{"id":"9bIR_N7R11XI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625074382474,"user_tz":-120,"elapsed":3293807,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"8396b392-e6f1-4c7a-db8e-8324fb30ac1a"},"source":["!python train.py --algo a2c --env SeaquestNoFrameskip-v4 --n-timesteps 1000000 -tb logs"],"execution_count":6,"outputs":[{"output_type":"stream","text":["========== SeaquestNoFrameskip-v4 ==========\n","Seed: 2360720576\n","Default hyperparameters for environment (ones being tuned will be overridden):\n","OrderedDict([('ent_coef', 0.01),\n","             ('env_wrapper',\n","              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n","             ('frame_stack', 4),\n","             ('n_envs', 16),\n","             ('n_timesteps', 10000000.0),\n","             ('policy', 'CnnPolicy'),\n","             ('policy_kwargs',\n","              'dict(optimizer_class=RMSpropTFLike, '\n","              'optimizer_kwargs=dict(eps=1e-5))'),\n","             ('vf_coef', 0.25)])\n","Using 16 environments\n","Overwriting n_timesteps with n=1000000\n","Creating test environment\n","Stacking 4 frames\n","Wrapping into a VecTransposeImage\n","Stacking 4 frames\n","Wrapping into a VecTransposeImage\n","Using cuda device\n","Log path: logs/a2c/SeaquestNoFrameskip-v4_1\n","2021-06-30 16:38:37.375241: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","Logging to logs/SeaquestNoFrameskip-v4/A2C_1\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.66e+03 |\n","|    ep_rew_mean        | 36       |\n","| time/                 |          |\n","|    fps                | 506      |\n","|    iterations         | 100      |\n","|    time_elapsed       | 15       |\n","|    total_timesteps    | 8000     |\n","| train/                |          |\n","|    entropy_loss       | -2.89    |\n","|    explained_variance | -1.94    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 99       |\n","|    policy_loss        | -0.0169  |\n","|    value_loss         | 0.00103  |\n","------------------------------------\n","Eval num_timesteps=10000, episode_reward=120.00 +/- 60.66\n","Episode length: 2676.80 +/- 883.87\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.68e+03 |\n","|    mean_reward        | 120      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.92e+03 |\n","|    ep_rew_mean        | 62.7     |\n","| time/                 |          |\n","|    fps                | 422      |\n","|    iterations         | 200      |\n","|    time_elapsed       | 37       |\n","|    total_timesteps    | 16000    |\n","| train/                |          |\n","|    entropy_loss       | -2.89    |\n","|    explained_variance | -0.026   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 199      |\n","|    policy_loss        | 0.187    |\n","|    value_loss         | 0.0826   |\n","------------------------------------\n","Eval num_timesteps=20000, episode_reward=88.00 +/- 41.18\n","Episode length: 2048.00 +/- 390.96\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.05e+03 |\n","|    mean_reward        | 88       |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.04e+03 |\n","|    ep_rew_mean        | 71.9     |\n","| time/                 |          |\n","|    fps                | 414      |\n","|    iterations         | 300      |\n","|    time_elapsed       | 57       |\n","|    total_timesteps    | 24000    |\n","| train/                |          |\n","|    entropy_loss       | -2.89    |\n","|    explained_variance | -0.78    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 299      |\n","|    policy_loss        | 0.00439  |\n","|    value_loss         | 0.00365  |\n","------------------------------------\n","Eval num_timesteps=30000, episode_reward=56.00 +/- 32.00\n","Episode length: 1917.20 +/- 272.28\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.92e+03 |\n","|    mean_reward        | 56       |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.05e+03 |\n","|    ep_rew_mean        | 74.2     |\n","| time/                 |          |\n","|    fps                | 412      |\n","|    iterations         | 400      |\n","|    time_elapsed       | 77       |\n","|    total_timesteps    | 32000    |\n","| train/                |          |\n","|    entropy_loss       | -2.89    |\n","|    explained_variance | -0.21    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 399      |\n","|    policy_loss        | -0.0282  |\n","|    value_loss         | 0.0048   |\n","------------------------------------\n","Eval num_timesteps=40000, episode_reward=80.00 +/- 37.95\n","Episode length: 2095.60 +/- 343.32\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.1e+03  |\n","|    mean_reward        | 80       |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.11e+03 |\n","|    ep_rew_mean        | 81.5     |\n","| time/                 |          |\n","|    fps                | 410      |\n","|    iterations         | 500      |\n","|    time_elapsed       | 97       |\n","|    total_timesteps    | 40000    |\n","| train/                |          |\n","|    entropy_loss       | -2.89    |\n","|    explained_variance | 0.313    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 499      |\n","|    policy_loss        | -0.0932  |\n","|    value_loss         | 0.00816  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.1e+03  |\n","|    ep_rew_mean        | 77.9     |\n","| time/                 |          |\n","|    fps                | 430      |\n","|    iterations         | 600      |\n","|    time_elapsed       | 111      |\n","|    total_timesteps    | 48000    |\n","| train/                |          |\n","|    entropy_loss       | -2.89    |\n","|    explained_variance | 0.0829   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 599      |\n","|    policy_loss        | 0.144    |\n","|    value_loss         | 0.0668   |\n","------------------------------------\n","Eval num_timesteps=50000, episode_reward=76.00 +/- 78.38\n","Episode length: 2017.80 +/- 664.20\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.02e+03 |\n","|    mean_reward        | 76       |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.08e+03 |\n","|    ep_rew_mean        | 77.8     |\n","| time/                 |          |\n","|    fps                | 426      |\n","|    iterations         | 700      |\n","|    time_elapsed       | 131      |\n","|    total_timesteps    | 56000    |\n","| train/                |          |\n","|    entropy_loss       | -2.89    |\n","|    explained_variance | 0.642    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 699      |\n","|    policy_loss        | -0.0206  |\n","|    value_loss         | 0.00215  |\n","------------------------------------\n","Eval num_timesteps=60000, episode_reward=112.00 +/- 81.58\n","Episode length: 2357.20 +/- 826.98\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.36e+03 |\n","|    mean_reward        | 112      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.07e+03 |\n","|    ep_rew_mean        | 77.8     |\n","| time/                 |          |\n","|    fps                | 419      |\n","|    iterations         | 800      |\n","|    time_elapsed       | 152      |\n","|    total_timesteps    | 64000    |\n","| train/                |          |\n","|    entropy_loss       | -2.89    |\n","|    explained_variance | 0.596    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 799      |\n","|    policy_loss        | -0.102   |\n","|    value_loss         | 0.0125   |\n","------------------------------------\n","Eval num_timesteps=70000, episode_reward=88.00 +/- 64.00\n","Episode length: 2186.60 +/- 807.24\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.19e+03 |\n","|    mean_reward        | 88       |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.06e+03 |\n","|    ep_rew_mean        | 79       |\n","| time/                 |          |\n","|    fps                | 416      |\n","|    iterations         | 900      |\n","|    time_elapsed       | 172      |\n","|    total_timesteps    | 72000    |\n","| train/                |          |\n","|    entropy_loss       | -2.89    |\n","|    explained_variance | 0.778    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 899      |\n","|    policy_loss        | -0.0985  |\n","|    value_loss         | 0.00788  |\n","------------------------------------\n","Eval num_timesteps=80000, episode_reward=88.00 +/- 88.18\n","Episode length: 2059.60 +/- 604.58\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.06e+03 |\n","|    mean_reward        | 88       |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.05e+03 |\n","|    ep_rew_mean        | 77.6     |\n","| time/                 |          |\n","|    fps                | 415      |\n","|    iterations         | 1000     |\n","|    time_elapsed       | 192      |\n","|    total_timesteps    | 80000    |\n","| train/                |          |\n","|    entropy_loss       | -2.89    |\n","|    explained_variance | 0.498    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 999      |\n","|    policy_loss        | -0.0827  |\n","|    value_loss         | 0.0389   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.03e+03 |\n","|    ep_rew_mean        | 77.6     |\n","| time/                 |          |\n","|    fps                | 426      |\n","|    iterations         | 1100     |\n","|    time_elapsed       | 206      |\n","|    total_timesteps    | 88000    |\n","| train/                |          |\n","|    entropy_loss       | -2.88    |\n","|    explained_variance | 0.864    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1099     |\n","|    policy_loss        | -0.0645  |\n","|    value_loss         | 0.00548  |\n","------------------------------------\n","Eval num_timesteps=90000, episode_reward=88.00 +/- 27.13\n","Episode length: 2310.00 +/- 389.92\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.31e+03 |\n","|    mean_reward        | 88       |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.07e+03 |\n","|    ep_rew_mean        | 83.2     |\n","| time/                 |          |\n","|    fps                | 422      |\n","|    iterations         | 1200     |\n","|    time_elapsed       | 227      |\n","|    total_timesteps    | 96000    |\n","| train/                |          |\n","|    entropy_loss       | -2.88    |\n","|    explained_variance | 0.483    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1199     |\n","|    policy_loss        | 0.0517   |\n","|    value_loss         | 0.0357   |\n","------------------------------------\n","Eval num_timesteps=100000, episode_reward=108.00 +/- 83.52\n","Episode length: 2358.80 +/- 694.69\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.36e+03 |\n","|    mean_reward        | 108      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.18e+03 |\n","|    ep_rew_mean        | 93.2     |\n","| time/                 |          |\n","|    fps                | 419      |\n","|    iterations         | 1300     |\n","|    time_elapsed       | 248      |\n","|    total_timesteps    | 104000   |\n","| train/                |          |\n","|    entropy_loss       | -2.88    |\n","|    explained_variance | 0.92     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1299     |\n","|    policy_loss        | -0.0377  |\n","|    value_loss         | 0.0022   |\n","------------------------------------\n","Eval num_timesteps=110000, episode_reward=152.00 +/- 108.52\n","Episode length: 2713.40 +/- 1095.76\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.71e+03 |\n","|    mean_reward        | 152      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.2e+03  |\n","|    ep_rew_mean        | 98.2     |\n","| time/                 |          |\n","|    fps                | 414      |\n","|    iterations         | 1400     |\n","|    time_elapsed       | 270      |\n","|    total_timesteps    | 112000   |\n","| train/                |          |\n","|    entropy_loss       | -2.88    |\n","|    explained_variance | 0.85     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1399     |\n","|    policy_loss        | -0.0888  |\n","|    value_loss         | 0.0136   |\n","------------------------------------\n","Eval num_timesteps=120000, episode_reward=156.00 +/- 70.88\n","Episode length: 2760.40 +/- 902.24\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.76e+03 |\n","|    mean_reward        | 156      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.32e+03 |\n","|    ep_rew_mean        | 109      |\n","| time/                 |          |\n","|    fps                | 410      |\n","|    iterations         | 1500     |\n","|    time_elapsed       | 292      |\n","|    total_timesteps    | 120000   |\n","| train/                |          |\n","|    entropy_loss       | -2.88    |\n","|    explained_variance | 0.752    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1499     |\n","|    policy_loss        | 0.0647   |\n","|    value_loss         | 0.0161   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.4e+03  |\n","|    ep_rew_mean        | 118      |\n","| time/                 |          |\n","|    fps                | 418      |\n","|    iterations         | 1600     |\n","|    time_elapsed       | 306      |\n","|    total_timesteps    | 128000   |\n","| train/                |          |\n","|    entropy_loss       | -2.85    |\n","|    explained_variance | 0.581    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1599     |\n","|    policy_loss        | 0.0683   |\n","|    value_loss         | 0.0426   |\n","------------------------------------\n","Eval num_timesteps=130000, episode_reward=164.00 +/- 99.12\n","Episode length: 2760.40 +/- 1035.19\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.76e+03 |\n","|    mean_reward        | 164      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.4e+03  |\n","|    ep_rew_mean        | 121      |\n","| time/                 |          |\n","|    fps                | 414      |\n","|    iterations         | 1700     |\n","|    time_elapsed       | 328      |\n","|    total_timesteps    | 136000   |\n","| train/                |          |\n","|    entropy_loss       | -2.84    |\n","|    explained_variance | 0.953    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1699     |\n","|    policy_loss        | -0.0347  |\n","|    value_loss         | 0.00261  |\n","------------------------------------\n","Eval num_timesteps=140000, episode_reward=308.00 +/- 156.26\n","Episode length: 4098.20 +/- 1574.68\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.1e+03  |\n","|    mean_reward        | 308      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.55e+03 |\n","|    ep_rew_mean        | 143      |\n","| time/                 |          |\n","|    fps                | 405      |\n","|    iterations         | 1800     |\n","|    time_elapsed       | 354      |\n","|    total_timesteps    | 144000   |\n","| train/                |          |\n","|    entropy_loss       | -2.84    |\n","|    explained_variance | 0.649    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1799     |\n","|    policy_loss        | 0.0525   |\n","|    value_loss         | 0.0479   |\n","------------------------------------\n","Eval num_timesteps=150000, episode_reward=248.00 +/- 134.22\n","Episode length: 3604.20 +/- 1491.95\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.6e+03  |\n","|    mean_reward        | 248      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.6e+03  |\n","|    ep_rew_mean        | 149      |\n","| time/                 |          |\n","|    fps                | 400      |\n","|    iterations         | 1900     |\n","|    time_elapsed       | 379      |\n","|    total_timesteps    | 152000   |\n","| train/                |          |\n","|    entropy_loss       | -2.78    |\n","|    explained_variance | 0.463    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1899     |\n","|    policy_loss        | 0.313    |\n","|    value_loss         | 0.162    |\n","------------------------------------\n","Eval num_timesteps=160000, episode_reward=228.00 +/- 43.08\n","Episode length: 3193.20 +/- 439.01\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.19e+03 |\n","|    mean_reward        | 228      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.66e+03 |\n","|    ep_rew_mean        | 161      |\n","| time/                 |          |\n","|    fps                | 397      |\n","|    iterations         | 2000     |\n","|    time_elapsed       | 402      |\n","|    total_timesteps    | 160000   |\n","| train/                |          |\n","|    entropy_loss       | -2.83    |\n","|    explained_variance | 0.953    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1999     |\n","|    policy_loss        | -0.177   |\n","|    value_loss         | 0.00846  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.75e+03 |\n","|    ep_rew_mean        | 175      |\n","| time/                 |          |\n","|    fps                | 403      |\n","|    iterations         | 2100     |\n","|    time_elapsed       | 416      |\n","|    total_timesteps    | 168000   |\n","| train/                |          |\n","|    entropy_loss       | -2.84    |\n","|    explained_variance | 0.762    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2099     |\n","|    policy_loss        | -0.154   |\n","|    value_loss         | 0.018    |\n","------------------------------------\n","Eval num_timesteps=170000, episode_reward=160.00 +/- 127.12\n","Episode length: 2691.20 +/- 1255.21\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.69e+03 |\n","|    mean_reward        | 160      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.86e+03 |\n","|    ep_rew_mean        | 187      |\n","| time/                 |          |\n","|    fps                | 401      |\n","|    iterations         | 2200     |\n","|    time_elapsed       | 438      |\n","|    total_timesteps    | 176000   |\n","| train/                |          |\n","|    entropy_loss       | -2.8     |\n","|    explained_variance | 0.791    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2199     |\n","|    policy_loss        | -0.0368  |\n","|    value_loss         | 0.0276   |\n","------------------------------------\n","Eval num_timesteps=180000, episode_reward=304.00 +/- 69.74\n","Episode length: 3786.60 +/- 885.81\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.79e+03 |\n","|    mean_reward        | 304      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.98e+03 |\n","|    ep_rew_mean        | 203      |\n","| time/                 |          |\n","|    fps                | 396      |\n","|    iterations         | 2300     |\n","|    time_elapsed       | 463      |\n","|    total_timesteps    | 184000   |\n","| train/                |          |\n","|    entropy_loss       | -2.82    |\n","|    explained_variance | 0.96     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2299     |\n","|    policy_loss        | 0.0028   |\n","|    value_loss         | 0.00613  |\n","------------------------------------\n","Eval num_timesteps=190000, episode_reward=324.00 +/- 163.66\n","Episode length: 4017.80 +/- 1528.48\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.02e+03 |\n","|    mean_reward        | 324      |\n","| rollout/              |          |\n","|    ep_len_mean        | 3.08e+03 |\n","|    ep_rew_mean        | 214      |\n","| time/                 |          |\n","|    fps                | 391      |\n","|    iterations         | 2400     |\n","|    time_elapsed       | 489      |\n","|    total_timesteps    | 192000   |\n","| train/                |          |\n","|    entropy_loss       | -2.83    |\n","|    explained_variance | 0.495    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2399     |\n","|    policy_loss        | 0.058    |\n","|    value_loss         | 0.0635   |\n","------------------------------------\n","Eval num_timesteps=200000, episode_reward=184.00 +/- 83.33\n","Episode length: 2751.80 +/- 809.54\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.75e+03 |\n","|    mean_reward        | 184      |\n","| rollout/              |          |\n","|    ep_len_mean        | 3.15e+03 |\n","|    ep_rew_mean        | 226      |\n","| time/                 |          |\n","|    fps                | 390      |\n","|    iterations         | 2500     |\n","|    time_elapsed       | 511      |\n","|    total_timesteps    | 200000   |\n","| train/                |          |\n","|    entropy_loss       | -2.76    |\n","|    explained_variance | 0.37     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2499     |\n","|    policy_loss        | 0.131    |\n","|    value_loss         | 0.0777   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 3.2e+03  |\n","|    ep_rew_mean        | 233      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 2600     |\n","|    time_elapsed       | 525      |\n","|    total_timesteps    | 208000   |\n","| train/                |          |\n","|    entropy_loss       | -2.81    |\n","|    explained_variance | 0.545    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2599     |\n","|    policy_loss        | 0.0287   |\n","|    value_loss         | 0.0223   |\n","------------------------------------\n","Eval num_timesteps=210000, episode_reward=332.00 +/- 62.74\n","Episode length: 4097.80 +/- 629.85\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.1e+03  |\n","|    mean_reward        | 332      |\n","| rollout/              |          |\n","|    ep_len_mean        | 3.36e+03 |\n","|    ep_rew_mean        | 249      |\n","| time/                 |          |\n","|    fps                | 391      |\n","|    iterations         | 2700     |\n","|    time_elapsed       | 551      |\n","|    total_timesteps    | 216000   |\n","| train/                |          |\n","|    entropy_loss       | -2.78    |\n","|    explained_variance | 0.919    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2699     |\n","|    policy_loss        | -0.0428  |\n","|    value_loss         | 0.00695  |\n","------------------------------------\n","Eval num_timesteps=220000, episode_reward=232.00 +/- 113.56\n","Episode length: 3232.60 +/- 942.90\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.23e+03 |\n","|    mean_reward        | 232      |\n","| rollout/              |          |\n","|    ep_len_mean        | 3.44e+03 |\n","|    ep_rew_mean        | 257      |\n","| time/                 |          |\n","|    fps                | 389      |\n","|    iterations         | 2800     |\n","|    time_elapsed       | 575      |\n","|    total_timesteps    | 224000   |\n","| train/                |          |\n","|    entropy_loss       | -2.8     |\n","|    explained_variance | 0.41     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2799     |\n","|    policy_loss        | 0.124    |\n","|    value_loss         | 0.163    |\n","------------------------------------\n","Eval num_timesteps=230000, episode_reward=288.00 +/- 34.87\n","Episode length: 3502.60 +/- 180.00\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.5e+03  |\n","|    mean_reward        | 288      |\n","| rollout/              |          |\n","|    ep_len_mean        | 3.57e+03 |\n","|    ep_rew_mean        | 269      |\n","| time/                 |          |\n","|    fps                | 386      |\n","|    iterations         | 2900     |\n","|    time_elapsed       | 600      |\n","|    total_timesteps    | 232000   |\n","| train/                |          |\n","|    entropy_loss       | -2.8     |\n","|    explained_variance | 0.62     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2899     |\n","|    policy_loss        | 0.135    |\n","|    value_loss         | 0.0838   |\n","------------------------------------\n","Eval num_timesteps=240000, episode_reward=256.00 +/- 105.38\n","Episode length: 3501.00 +/- 1152.43\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.5e+03  |\n","|    mean_reward        | 256      |\n","| rollout/              |          |\n","|    ep_len_mean        | 3.65e+03 |\n","|    ep_rew_mean        | 277      |\n","| time/                 |          |\n","|    fps                | 384      |\n","|    iterations         | 3000     |\n","|    time_elapsed       | 624      |\n","|    total_timesteps    | 240000   |\n","| train/                |          |\n","|    entropy_loss       | -2.75    |\n","|    explained_variance | 0.962    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2999     |\n","|    policy_loss        | -0.0883  |\n","|    value_loss         | 0.00661  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 3.68e+03 |\n","|    ep_rew_mean        | 279      |\n","| time/                 |          |\n","|    fps                | 388      |\n","|    iterations         | 3100     |\n","|    time_elapsed       | 638      |\n","|    total_timesteps    | 248000   |\n","| train/                |          |\n","|    entropy_loss       | -2.79    |\n","|    explained_variance | 0.601    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3099     |\n","|    policy_loss        | 0.26     |\n","|    value_loss         | 0.182    |\n","------------------------------------\n","Eval num_timesteps=250000, episode_reward=284.00 +/- 78.38\n","Episode length: 3690.80 +/- 866.45\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.69e+03 |\n","|    mean_reward        | 284      |\n","| rollout/              |          |\n","|    ep_len_mean        | 3.76e+03 |\n","|    ep_rew_mean        | 285      |\n","| time/                 |          |\n","|    fps                | 385      |\n","|    iterations         | 3200     |\n","|    time_elapsed       | 663      |\n","|    total_timesteps    | 256000   |\n","| train/                |          |\n","|    entropy_loss       | -2.79    |\n","|    explained_variance | 0.698    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3199     |\n","|    policy_loss        | 0.0626   |\n","|    value_loss         | 0.0389   |\n","------------------------------------\n","Eval num_timesteps=260000, episode_reward=348.00 +/- 124.32\n","Episode length: 4015.40 +/- 1208.19\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.02e+03 |\n","|    mean_reward        | 348      |\n","| rollout/              |          |\n","|    ep_len_mean        | 3.78e+03 |\n","|    ep_rew_mean        | 288      |\n","| time/                 |          |\n","|    fps                | 382      |\n","|    iterations         | 3300     |\n","|    time_elapsed       | 689      |\n","|    total_timesteps    | 264000   |\n","| train/                |          |\n","|    entropy_loss       | -2.79    |\n","|    explained_variance | 0.63     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3299     |\n","|    policy_loss        | 0.196    |\n","|    value_loss         | 0.0911   |\n","------------------------------------\n","Eval num_timesteps=270000, episode_reward=468.00 +/- 39.19\n","Episode length: 5607.60 +/- 523.25\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.61e+03 |\n","|    mean_reward        | 468      |\n","| rollout/              |          |\n","|    ep_len_mean        | 3.8e+03  |\n","|    ep_rew_mean        | 290      |\n","| time/                 |          |\n","|    fps                | 377      |\n","|    iterations         | 3400     |\n","|    time_elapsed       | 720      |\n","|    total_timesteps    | 272000   |\n","| train/                |          |\n","|    entropy_loss       | -2.75    |\n","|    explained_variance | 0.904    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3399     |\n","|    policy_loss        | -0.0387  |\n","|    value_loss         | 0.00507  |\n","------------------------------------\n","Eval num_timesteps=280000, episode_reward=384.00 +/- 80.40\n","Episode length: 4557.20 +/- 745.47\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.56e+03 |\n","|    mean_reward        | 384      |\n","| rollout/              |          |\n","|    ep_len_mean        | 3.93e+03 |\n","|    ep_rew_mean        | 302      |\n","| time/                 |          |\n","|    fps                | 374      |\n","|    iterations         | 3500     |\n","|    time_elapsed       | 748      |\n","|    total_timesteps    | 280000   |\n","| train/                |          |\n","|    entropy_loss       | -2.78    |\n","|    explained_variance | 0.206    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3499     |\n","|    policy_loss        | 0.355    |\n","|    value_loss         | 0.162    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 4.03e+03 |\n","|    ep_rew_mean        | 312      |\n","| time/                 |          |\n","|    fps                | 378      |\n","|    iterations         | 3600     |\n","|    time_elapsed       | 761      |\n","|    total_timesteps    | 288000   |\n","| train/                |          |\n","|    entropy_loss       | -2.76    |\n","|    explained_variance | 0.796    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3599     |\n","|    policy_loss        | -0.0397  |\n","|    value_loss         | 0.0294   |\n","------------------------------------\n","Eval num_timesteps=290000, episode_reward=452.00 +/- 104.77\n","Episode length: 5389.00 +/- 1101.62\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.39e+03 |\n","|    mean_reward        | 452      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.17e+03 |\n","|    ep_rew_mean        | 324      |\n","| time/                 |          |\n","|    fps                | 373      |\n","|    iterations         | 3700     |\n","|    time_elapsed       | 791      |\n","|    total_timesteps    | 296000   |\n","| train/                |          |\n","|    entropy_loss       | -2.73    |\n","|    explained_variance | 0.919    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3699     |\n","|    policy_loss        | -0.126   |\n","|    value_loss         | 0.0186   |\n","------------------------------------\n","Eval num_timesteps=300000, episode_reward=476.00 +/- 147.19\n","Episode length: 5829.20 +/- 1645.42\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.83e+03 |\n","|    mean_reward        | 476      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.22e+03 |\n","|    ep_rew_mean        | 328      |\n","| time/                 |          |\n","|    fps                | 369      |\n","|    iterations         | 3800     |\n","|    time_elapsed       | 823      |\n","|    total_timesteps    | 304000   |\n","| train/                |          |\n","|    entropy_loss       | -2.75    |\n","|    explained_variance | 0.727    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3799     |\n","|    policy_loss        | -0.0105  |\n","|    value_loss         | 0.0529   |\n","------------------------------------\n","Eval num_timesteps=310000, episode_reward=472.00 +/- 77.56\n","Episode length: 5815.20 +/- 925.63\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.82e+03 |\n","|    mean_reward        | 472      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.34e+03 |\n","|    ep_rew_mean        | 336      |\n","| time/                 |          |\n","|    fps                | 364      |\n","|    iterations         | 3900     |\n","|    time_elapsed       | 855      |\n","|    total_timesteps    | 312000   |\n","| train/                |          |\n","|    entropy_loss       | -2.77    |\n","|    explained_variance | 0.712    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3899     |\n","|    policy_loss        | 0.026    |\n","|    value_loss         | 0.0633   |\n","------------------------------------\n","Eval num_timesteps=320000, episode_reward=380.00 +/- 50.60\n","Episode length: 4738.20 +/- 506.05\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.74e+03 |\n","|    mean_reward        | 380      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.35e+03 |\n","|    ep_rew_mean        | 336      |\n","| time/                 |          |\n","|    fps                | 362      |\n","|    iterations         | 4000     |\n","|    time_elapsed       | 883      |\n","|    total_timesteps    | 320000   |\n","| train/                |          |\n","|    entropy_loss       | -2.79    |\n","|    explained_variance | 0.712    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3999     |\n","|    policy_loss        | 0.0785   |\n","|    value_loss         | 0.0373   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 4.42e+03 |\n","|    ep_rew_mean        | 345      |\n","| time/                 |          |\n","|    fps                | 365      |\n","|    iterations         | 4100     |\n","|    time_elapsed       | 897      |\n","|    total_timesteps    | 328000   |\n","| train/                |          |\n","|    entropy_loss       | -2.78    |\n","|    explained_variance | 0.587    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4099     |\n","|    policy_loss        | -0.204   |\n","|    value_loss         | 0.0452   |\n","------------------------------------\n","Eval num_timesteps=330000, episode_reward=424.00 +/- 101.51\n","Episode length: 5145.40 +/- 1158.31\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.15e+03 |\n","|    mean_reward        | 424      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.47e+03 |\n","|    ep_rew_mean        | 350      |\n","| time/                 |          |\n","|    fps                | 362      |\n","|    iterations         | 4200     |\n","|    time_elapsed       | 926      |\n","|    total_timesteps    | 336000   |\n","| train/                |          |\n","|    entropy_loss       | -2.76    |\n","|    explained_variance | 0.797    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4199     |\n","|    policy_loss        | 0.0693   |\n","|    value_loss         | 0.0413   |\n","------------------------------------\n","Eval num_timesteps=340000, episode_reward=324.00 +/- 83.33\n","Episode length: 4405.80 +/- 808.09\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.41e+03 |\n","|    mean_reward        | 324      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.48e+03 |\n","|    ep_rew_mean        | 351      |\n","| time/                 |          |\n","|    fps                | 360      |\n","|    iterations         | 4300     |\n","|    time_elapsed       | 953      |\n","|    total_timesteps    | 344000   |\n","| train/                |          |\n","|    entropy_loss       | -2.79    |\n","|    explained_variance | 0.722    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4299     |\n","|    policy_loss        | 0.104    |\n","|    value_loss         | 0.0514   |\n","------------------------------------\n","Eval num_timesteps=350000, episode_reward=516.00 +/- 177.27\n","Episode length: 6016.40 +/- 1724.94\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.02e+03 |\n","|    mean_reward        | 516      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.53e+03 |\n","|    ep_rew_mean        | 356      |\n","| time/                 |          |\n","|    fps                | 357      |\n","|    iterations         | 4400     |\n","|    time_elapsed       | 985      |\n","|    total_timesteps    | 352000   |\n","| train/                |          |\n","|    entropy_loss       | -2.73    |\n","|    explained_variance | 0.963    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4399     |\n","|    policy_loss        | -0.0765  |\n","|    value_loss         | 0.00497  |\n","------------------------------------\n","Eval num_timesteps=360000, episode_reward=392.00 +/- 138.33\n","Episode length: 4764.20 +/- 1548.85\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.76e+03 |\n","|    mean_reward        | 392      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.64e+03 |\n","|    ep_rew_mean        | 364      |\n","| time/                 |          |\n","|    fps                | 355      |\n","|    iterations         | 4500     |\n","|    time_elapsed       | 1014     |\n","|    total_timesteps    | 360000   |\n","| train/                |          |\n","|    entropy_loss       | -2.72    |\n","|    explained_variance | 0.845    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4499     |\n","|    policy_loss        | -0.176   |\n","|    value_loss         | 0.0209   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 4.81e+03 |\n","|    ep_rew_mean        | 380      |\n","| time/                 |          |\n","|    fps                | 358      |\n","|    iterations         | 4600     |\n","|    time_elapsed       | 1027     |\n","|    total_timesteps    | 368000   |\n","| train/                |          |\n","|    entropy_loss       | -2.75    |\n","|    explained_variance | 0.746    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4599     |\n","|    policy_loss        | 0.0844   |\n","|    value_loss         | 0.0395   |\n","------------------------------------\n","Eval num_timesteps=370000, episode_reward=408.00 +/- 124.96\n","Episode length: 5143.60 +/- 1361.28\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.14e+03 |\n","|    mean_reward        | 408      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.83e+03 |\n","|    ep_rew_mean        | 381      |\n","| time/                 |          |\n","|    fps                | 355      |\n","|    iterations         | 4700     |\n","|    time_elapsed       | 1056     |\n","|    total_timesteps    | 376000   |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.858    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4699     |\n","|    policy_loss        | -0.191   |\n","|    value_loss         | 0.0224   |\n","------------------------------------\n","Eval num_timesteps=380000, episode_reward=444.00 +/- 137.64\n","Episode length: 5361.80 +/- 1468.40\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.36e+03 |\n","|    mean_reward        | 444      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.92e+03 |\n","|    ep_rew_mean        | 391      |\n","| time/                 |          |\n","|    fps                | 353      |\n","|    iterations         | 4800     |\n","|    time_elapsed       | 1086     |\n","|    total_timesteps    | 384000   |\n","| train/                |          |\n","|    entropy_loss       | -2.71    |\n","|    explained_variance | 0.837    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4799     |\n","|    policy_loss        | 0.00161  |\n","|    value_loss         | 0.0522   |\n","------------------------------------\n","Eval num_timesteps=390000, episode_reward=340.00 +/- 91.21\n","Episode length: 4256.40 +/- 890.27\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.26e+03 |\n","|    mean_reward        | 340      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.96e+03 |\n","|    ep_rew_mean        | 395      |\n","| time/                 |          |\n","|    fps                | 351      |\n","|    iterations         | 4900     |\n","|    time_elapsed       | 1113     |\n","|    total_timesteps    | 392000   |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.472    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4899     |\n","|    policy_loss        | -0.0556  |\n","|    value_loss         | 0.0971   |\n","------------------------------------\n","Eval num_timesteps=400000, episode_reward=496.00 +/- 151.47\n","Episode length: 6154.60 +/- 1393.79\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.15e+03 |\n","|    mean_reward        | 496      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.02e+03 |\n","|    ep_rew_mean        | 401      |\n","| time/                 |          |\n","|    fps                | 348      |\n","|    iterations         | 5000     |\n","|    time_elapsed       | 1146     |\n","|    total_timesteps    | 400000   |\n","| train/                |          |\n","|    entropy_loss       | -2.71    |\n","|    explained_variance | 0.929    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4999     |\n","|    policy_loss        | 0.0381   |\n","|    value_loss         | 0.0102   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.04e+03 |\n","|    ep_rew_mean        | 403      |\n","| time/                 |          |\n","|    fps                | 351      |\n","|    iterations         | 5100     |\n","|    time_elapsed       | 1159     |\n","|    total_timesteps    | 408000   |\n","| train/                |          |\n","|    entropy_loss       | -2.69    |\n","|    explained_variance | 0.729    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5099     |\n","|    policy_loss        | 0.322    |\n","|    value_loss         | 0.164    |\n","------------------------------------\n","Eval num_timesteps=410000, episode_reward=332.00 +/- 95.16\n","Episode length: 4221.40 +/- 1025.60\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.22e+03 |\n","|    mean_reward        | 332      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.02e+03 |\n","|    ep_rew_mean        | 399      |\n","| time/                 |          |\n","|    fps                | 350      |\n","|    iterations         | 5200     |\n","|    time_elapsed       | 1186     |\n","|    total_timesteps    | 416000   |\n","| train/                |          |\n","|    entropy_loss       | -2.71    |\n","|    explained_variance | 0.903    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5199     |\n","|    policy_loss        | 0.0364   |\n","|    value_loss         | 0.0105   |\n","------------------------------------\n","Eval num_timesteps=420000, episode_reward=428.00 +/- 137.75\n","Episode length: 5442.00 +/- 1453.52\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.44e+03 |\n","|    mean_reward        | 428      |\n","| rollout/              |          |\n","|    ep_len_mean        | 4.99e+03 |\n","|    ep_rew_mean        | 397      |\n","| time/                 |          |\n","|    fps                | 348      |\n","|    iterations         | 5300     |\n","|    time_elapsed       | 1216     |\n","|    total_timesteps    | 424000   |\n","| train/                |          |\n","|    entropy_loss       | -2.72    |\n","|    explained_variance | 0.672    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5299     |\n","|    policy_loss        | 0.0349   |\n","|    value_loss         | 0.0313   |\n","------------------------------------\n","Eval num_timesteps=430000, episode_reward=332.00 +/- 113.56\n","Episode length: 4202.60 +/- 1134.07\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.2e+03  |\n","|    mean_reward        | 332      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.1e+03  |\n","|    ep_rew_mean        | 409      |\n","| time/                 |          |\n","|    fps                | 347      |\n","|    iterations         | 5400     |\n","|    time_elapsed       | 1243     |\n","|    total_timesteps    | 432000   |\n","| train/                |          |\n","|    entropy_loss       | -2.71    |\n","|    explained_variance | 0.856    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5399     |\n","|    policy_loss        | 0.112    |\n","|    value_loss         | 0.055    |\n","------------------------------------\n","Eval num_timesteps=440000, episode_reward=484.00 +/- 69.74\n","Episode length: 5935.60 +/- 644.81\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.94e+03 |\n","|    mean_reward        | 484      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.07e+03 |\n","|    ep_rew_mean        | 408      |\n","| time/                 |          |\n","|    fps                | 345      |\n","|    iterations         | 5500     |\n","|    time_elapsed       | 1274     |\n","|    total_timesteps    | 440000   |\n","| train/                |          |\n","|    entropy_loss       | -2.65    |\n","|    explained_variance | 0.831    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5499     |\n","|    policy_loss        | 0.144    |\n","|    value_loss         | 0.0515   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.13e+03 |\n","|    ep_rew_mean        | 414      |\n","| time/                 |          |\n","|    fps                | 347      |\n","|    iterations         | 5600     |\n","|    time_elapsed       | 1288     |\n","|    total_timesteps    | 448000   |\n","| train/                |          |\n","|    entropy_loss       | -2.65    |\n","|    explained_variance | 0.554    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5599     |\n","|    policy_loss        | -0.0446  |\n","|    value_loss         | 0.0503   |\n","------------------------------------\n","Eval num_timesteps=450000, episode_reward=456.00 +/- 82.37\n","Episode length: 5591.20 +/- 907.42\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.59e+03 |\n","|    mean_reward        | 456      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.15e+03 |\n","|    ep_rew_mean        | 415      |\n","| time/                 |          |\n","|    fps                | 345      |\n","|    iterations         | 5700     |\n","|    time_elapsed       | 1319     |\n","|    total_timesteps    | 456000   |\n","| train/                |          |\n","|    entropy_loss       | -2.65    |\n","|    explained_variance | 0.803    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5699     |\n","|    policy_loss        | -0.0459  |\n","|    value_loss         | 0.0605   |\n","------------------------------------\n","Eval num_timesteps=460000, episode_reward=424.00 +/- 84.29\n","Episode length: 5090.60 +/- 849.11\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.09e+03 |\n","|    mean_reward        | 424      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.2e+03  |\n","|    ep_rew_mean        | 419      |\n","| time/                 |          |\n","|    fps                | 344      |\n","|    iterations         | 5800     |\n","|    time_elapsed       | 1348     |\n","|    total_timesteps    | 464000   |\n","| train/                |          |\n","|    entropy_loss       | -2.59    |\n","|    explained_variance | 0.8      |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5799     |\n","|    policy_loss        | 0.0753   |\n","|    value_loss         | 0.0701   |\n","------------------------------------\n","Eval num_timesteps=470000, episode_reward=428.00 +/- 72.22\n","Episode length: 5219.40 +/- 916.72\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.22e+03 |\n","|    mean_reward        | 428      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.24e+03 |\n","|    ep_rew_mean        | 423      |\n","| time/                 |          |\n","|    fps                | 342      |\n","|    iterations         | 5900     |\n","|    time_elapsed       | 1378     |\n","|    total_timesteps    | 472000   |\n","| train/                |          |\n","|    entropy_loss       | -2.58    |\n","|    explained_variance | 0.804    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5899     |\n","|    policy_loss        | -0.0562  |\n","|    value_loss         | 0.0311   |\n","------------------------------------\n","Eval num_timesteps=480000, episode_reward=340.00 +/- 133.27\n","Episode length: 4470.20 +/- 1453.26\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.47e+03 |\n","|    mean_reward        | 340      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.29e+03 |\n","|    ep_rew_mean        | 427      |\n","| time/                 |          |\n","|    fps                | 341      |\n","|    iterations         | 6000     |\n","|    time_elapsed       | 1405     |\n","|    total_timesteps    | 480000   |\n","| train/                |          |\n","|    entropy_loss       | -2.74    |\n","|    explained_variance | 0.728    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5999     |\n","|    policy_loss        | 0.0288   |\n","|    value_loss         | 0.0314   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.26e+03 |\n","|    ep_rew_mean        | 424      |\n","| time/                 |          |\n","|    fps                | 343      |\n","|    iterations         | 6100     |\n","|    time_elapsed       | 1419     |\n","|    total_timesteps    | 488000   |\n","| train/                |          |\n","|    entropy_loss       | -2.64    |\n","|    explained_variance | 0.64     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6099     |\n","|    policy_loss        | -0.133   |\n","|    value_loss         | 0.064    |\n","------------------------------------\n","Eval num_timesteps=490000, episode_reward=448.00 +/- 68.82\n","Episode length: 5463.00 +/- 687.26\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.46e+03 |\n","|    mean_reward        | 448      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.27e+03 |\n","|    ep_rew_mean        | 427      |\n","| time/                 |          |\n","|    fps                | 342      |\n","|    iterations         | 6200     |\n","|    time_elapsed       | 1449     |\n","|    total_timesteps    | 496000   |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.897    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6199     |\n","|    policy_loss        | -0.00632 |\n","|    value_loss         | 0.022    |\n","------------------------------------\n","Eval num_timesteps=500000, episode_reward=424.00 +/- 58.51\n","Episode length: 5132.60 +/- 770.84\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.13e+03 |\n","|    mean_reward        | 424      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.35e+03 |\n","|    ep_rew_mean        | 436      |\n","| time/                 |          |\n","|    fps                | 340      |\n","|    iterations         | 6300     |\n","|    time_elapsed       | 1478     |\n","|    total_timesteps    | 504000   |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.743    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6299     |\n","|    policy_loss        | -0.121   |\n","|    value_loss         | 0.0752   |\n","------------------------------------\n","Eval num_timesteps=510000, episode_reward=476.00 +/- 148.81\n","Episode length: 5726.00 +/- 1289.71\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.73e+03 |\n","|    mean_reward        | 476      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.41e+03 |\n","|    ep_rew_mean        | 442      |\n","| time/                 |          |\n","|    fps                | 338      |\n","|    iterations         | 6400     |\n","|    time_elapsed       | 1510     |\n","|    total_timesteps    | 512000   |\n","| train/                |          |\n","|    entropy_loss       | -2.55    |\n","|    explained_variance | 0.787    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6399     |\n","|    policy_loss        | 0.187    |\n","|    value_loss         | 0.0559   |\n","------------------------------------\n","Eval num_timesteps=520000, episode_reward=440.00 +/- 75.89\n","Episode length: 5380.60 +/- 789.10\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.38e+03 |\n","|    mean_reward        | 440      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.42e+03 |\n","|    ep_rew_mean        | 442      |\n","| time/                 |          |\n","|    fps                | 337      |\n","|    iterations         | 6500     |\n","|    time_elapsed       | 1540     |\n","|    total_timesteps    | 520000   |\n","| train/                |          |\n","|    entropy_loss       | -2.51    |\n","|    explained_variance | 0.585    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6499     |\n","|    policy_loss        | -0.0325  |\n","|    value_loss         | 0.1      |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.46e+03 |\n","|    ep_rew_mean        | 445      |\n","| time/                 |          |\n","|    fps                | 339      |\n","|    iterations         | 6600     |\n","|    time_elapsed       | 1554     |\n","|    total_timesteps    | 528000   |\n","| train/                |          |\n","|    entropy_loss       | -2.23    |\n","|    explained_variance | 0.848    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6599     |\n","|    policy_loss        | -0.0393  |\n","|    value_loss         | 0.0436   |\n","------------------------------------\n","Eval num_timesteps=530000, episode_reward=468.00 +/- 41.18\n","Episode length: 5632.80 +/- 453.72\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.63e+03 |\n","|    mean_reward        | 468      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.49e+03 |\n","|    ep_rew_mean        | 450      |\n","| time/                 |          |\n","|    fps                | 337      |\n","|    iterations         | 6700     |\n","|    time_elapsed       | 1585     |\n","|    total_timesteps    | 536000   |\n","| train/                |          |\n","|    entropy_loss       | -2.65    |\n","|    explained_variance | 0.928    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6699     |\n","|    policy_loss        | -0.0215  |\n","|    value_loss         | 0.0104   |\n","------------------------------------\n","Eval num_timesteps=540000, episode_reward=492.00 +/- 82.56\n","Episode length: 5824.40 +/- 710.76\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.82e+03 |\n","|    mean_reward        | 492      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.44e+03 |\n","|    ep_rew_mean        | 447      |\n","| time/                 |          |\n","|    fps                | 336      |\n","|    iterations         | 6800     |\n","|    time_elapsed       | 1617     |\n","|    total_timesteps    | 544000   |\n","| train/                |          |\n","|    entropy_loss       | -2.5     |\n","|    explained_variance | 0.722    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6799     |\n","|    policy_loss        | 0.0406   |\n","|    value_loss         | 0.0404   |\n","------------------------------------\n","Eval num_timesteps=550000, episode_reward=484.00 +/- 150.94\n","Episode length: 5682.00 +/- 1598.41\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.68e+03 |\n","|    mean_reward        | 484      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.42e+03 |\n","|    ep_rew_mean        | 444      |\n","| time/                 |          |\n","|    fps                | 334      |\n","|    iterations         | 6900     |\n","|    time_elapsed       | 1648     |\n","|    total_timesteps    | 552000   |\n","| train/                |          |\n","|    entropy_loss       | -2.53    |\n","|    explained_variance | 0.835    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6899     |\n","|    policy_loss        | 0.0506   |\n","|    value_loss         | 0.0677   |\n","------------------------------------\n","Eval num_timesteps=560000, episode_reward=476.00 +/- 183.48\n","Episode length: 5919.40 +/- 1971.03\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.92e+03 |\n","|    mean_reward        | 476      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.34e+03 |\n","|    ep_rew_mean        | 437      |\n","| time/                 |          |\n","|    fps                | 333      |\n","|    iterations         | 7000     |\n","|    time_elapsed       | 1680     |\n","|    total_timesteps    | 560000   |\n","| train/                |          |\n","|    entropy_loss       | -2.42    |\n","|    explained_variance | 0.925    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6999     |\n","|    policy_loss        | -0.00102 |\n","|    value_loss         | 0.0121   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.32e+03 |\n","|    ep_rew_mean        | 434      |\n","| time/                 |          |\n","|    fps                | 335      |\n","|    iterations         | 7100     |\n","|    time_elapsed       | 1693     |\n","|    total_timesteps    | 568000   |\n","| train/                |          |\n","|    entropy_loss       | -2.73    |\n","|    explained_variance | 0.799    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7099     |\n","|    policy_loss        | 0.0179   |\n","|    value_loss         | 0.0327   |\n","------------------------------------\n","Eval num_timesteps=570000, episode_reward=404.00 +/- 111.28\n","Episode length: 4931.80 +/- 1057.31\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.93e+03 |\n","|    mean_reward        | 404      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.33e+03 |\n","|    ep_rew_mean        | 436      |\n","| time/                 |          |\n","|    fps                | 334      |\n","|    iterations         | 7200     |\n","|    time_elapsed       | 1722     |\n","|    total_timesteps    | 576000   |\n","| train/                |          |\n","|    entropy_loss       | -2.54    |\n","|    explained_variance | 0.874    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7199     |\n","|    policy_loss        | -0.187   |\n","|    value_loss         | 0.0129   |\n","------------------------------------\n","Eval num_timesteps=580000, episode_reward=340.00 +/- 125.22\n","Episode length: 4193.80 +/- 1185.12\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.19e+03 |\n","|    mean_reward        | 340      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.3e+03  |\n","|    ep_rew_mean        | 434      |\n","| time/                 |          |\n","|    fps                | 333      |\n","|    iterations         | 7300     |\n","|    time_elapsed       | 1748     |\n","|    total_timesteps    | 584000   |\n","| train/                |          |\n","|    entropy_loss       | -2.61    |\n","|    explained_variance | 0.929    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7299     |\n","|    policy_loss        | -0.00352 |\n","|    value_loss         | 0.0207   |\n","------------------------------------\n","Eval num_timesteps=590000, episode_reward=472.00 +/- 142.32\n","Episode length: 5647.60 +/- 1500.58\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.65e+03 |\n","|    mean_reward        | 472      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.37e+03 |\n","|    ep_rew_mean        | 440      |\n","| time/                 |          |\n","|    fps                | 332      |\n","|    iterations         | 7400     |\n","|    time_elapsed       | 1780     |\n","|    total_timesteps    | 592000   |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.797    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7399     |\n","|    policy_loss        | 0.0319   |\n","|    value_loss         | 0.0298   |\n","------------------------------------\n","Eval num_timesteps=600000, episode_reward=464.00 +/- 57.13\n","Episode length: 5481.00 +/- 780.58\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.48e+03 |\n","|    mean_reward        | 464      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.32e+03 |\n","|    ep_rew_mean        | 436      |\n","| time/                 |          |\n","|    fps                | 331      |\n","|    iterations         | 7500     |\n","|    time_elapsed       | 1811     |\n","|    total_timesteps    | 600000   |\n","| train/                |          |\n","|    entropy_loss       | -2.67    |\n","|    explained_variance | 0.948    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7499     |\n","|    policy_loss        | 0.0399   |\n","|    value_loss         | 0.0136   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.29e+03 |\n","|    ep_rew_mean        | 433      |\n","| time/                 |          |\n","|    fps                | 332      |\n","|    iterations         | 7600     |\n","|    time_elapsed       | 1826     |\n","|    total_timesteps    | 608000   |\n","| train/                |          |\n","|    entropy_loss       | -2.57    |\n","|    explained_variance | 0.825    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7599     |\n","|    policy_loss        | 0.141    |\n","|    value_loss         | 0.0569   |\n","------------------------------------\n","Eval num_timesteps=610000, episode_reward=472.00 +/- 143.44\n","Episode length: 5760.60 +/- 1472.05\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.76e+03 |\n","|    mean_reward        | 472      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.29e+03 |\n","|    ep_rew_mean        | 434      |\n","| time/                 |          |\n","|    fps                | 331      |\n","|    iterations         | 7700     |\n","|    time_elapsed       | 1857     |\n","|    total_timesteps    | 616000   |\n","| train/                |          |\n","|    entropy_loss       | -2.58    |\n","|    explained_variance | 0.812    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7699     |\n","|    policy_loss        | -0.0158  |\n","|    value_loss         | 0.0207   |\n","------------------------------------\n","Eval num_timesteps=620000, episode_reward=508.00 +/- 167.14\n","Episode length: 6142.40 +/- 1799.27\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.14e+03 |\n","|    mean_reward        | 508      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.34e+03 |\n","|    ep_rew_mean        | 437      |\n","| time/                 |          |\n","|    fps                | 330      |\n","|    iterations         | 7800     |\n","|    time_elapsed       | 1889     |\n","|    total_timesteps    | 624000   |\n","| train/                |          |\n","|    entropy_loss       | -2.68    |\n","|    explained_variance | 0.787    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7799     |\n","|    policy_loss        | 0.109    |\n","|    value_loss         | 0.0565   |\n","------------------------------------\n","Eval num_timesteps=630000, episode_reward=428.00 +/- 192.08\n","Episode length: 5364.40 +/- 1926.38\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.36e+03 |\n","|    mean_reward        | 428      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.41e+03 |\n","|    ep_rew_mean        | 443      |\n","| time/                 |          |\n","|    fps                | 329      |\n","|    iterations         | 7900     |\n","|    time_elapsed       | 1919     |\n","|    total_timesteps    | 632000   |\n","| train/                |          |\n","|    entropy_loss       | -2.64    |\n","|    explained_variance | 0.68     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7899     |\n","|    policy_loss        | 0.0175   |\n","|    value_loss         | 0.0342   |\n","------------------------------------\n","Eval num_timesteps=640000, episode_reward=476.00 +/- 75.26\n","Episode length: 5999.00 +/- 885.52\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6e+03    |\n","|    mean_reward        | 476      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.41e+03 |\n","|    ep_rew_mean        | 442      |\n","| time/                 |          |\n","|    fps                | 327      |\n","|    iterations         | 8000     |\n","|    time_elapsed       | 1951     |\n","|    total_timesteps    | 640000   |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.812    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7999     |\n","|    policy_loss        | -0.116   |\n","|    value_loss         | 0.026    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.43e+03 |\n","|    ep_rew_mean        | 445      |\n","| time/                 |          |\n","|    fps                | 329      |\n","|    iterations         | 8100     |\n","|    time_elapsed       | 1965     |\n","|    total_timesteps    | 648000   |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.832    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8099     |\n","|    policy_loss        | -0.234   |\n","|    value_loss         | 0.028    |\n","------------------------------------\n","Eval num_timesteps=650000, episode_reward=468.00 +/- 51.54\n","Episode length: 5628.80 +/- 536.38\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.63e+03 |\n","|    mean_reward        | 468      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.4e+03  |\n","|    ep_rew_mean        | 443      |\n","| time/                 |          |\n","|    fps                | 328      |\n","|    iterations         | 8200     |\n","|    time_elapsed       | 1996     |\n","|    total_timesteps    | 656000   |\n","| train/                |          |\n","|    entropy_loss       | -2.59    |\n","|    explained_variance | 0.829    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8199     |\n","|    policy_loss        | -0.0665  |\n","|    value_loss         | 0.0208   |\n","------------------------------------\n","Eval num_timesteps=660000, episode_reward=416.00 +/- 134.10\n","Episode length: 5213.00 +/- 1255.25\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.21e+03 |\n","|    mean_reward        | 416      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.39e+03 |\n","|    ep_rew_mean        | 442      |\n","| time/                 |          |\n","|    fps                | 327      |\n","|    iterations         | 8300     |\n","|    time_elapsed       | 2025     |\n","|    total_timesteps    | 664000   |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.854    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8299     |\n","|    policy_loss        | -0.0445  |\n","|    value_loss         | 0.0359   |\n","------------------------------------\n","Eval num_timesteps=670000, episode_reward=516.00 +/- 93.30\n","Episode length: 6344.80 +/- 1013.19\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.34e+03 |\n","|    mean_reward        | 516      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.33e+03 |\n","|    ep_rew_mean        | 435      |\n","| time/                 |          |\n","|    fps                | 326      |\n","|    iterations         | 8400     |\n","|    time_elapsed       | 2058     |\n","|    total_timesteps    | 672000   |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.871    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8399     |\n","|    policy_loss        | 0.0215   |\n","|    value_loss         | 0.0311   |\n","------------------------------------\n","Eval num_timesteps=680000, episode_reward=588.00 +/- 92.61\n","Episode length: 7106.60 +/- 915.97\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 7.11e+03 |\n","|    mean_reward        | 588      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.41e+03 |\n","|    ep_rew_mean        | 444      |\n","| time/                 |          |\n","|    fps                | 324      |\n","|    iterations         | 8500     |\n","|    time_elapsed       | 2094     |\n","|    total_timesteps    | 680000   |\n","| train/                |          |\n","|    entropy_loss       | -2.56    |\n","|    explained_variance | 0.754    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8499     |\n","|    policy_loss        | 0.0382   |\n","|    value_loss         | 0.0182   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.45e+03 |\n","|    ep_rew_mean        | 449      |\n","| time/                 |          |\n","|    fps                | 326      |\n","|    iterations         | 8600     |\n","|    time_elapsed       | 2107     |\n","|    total_timesteps    | 688000   |\n","| train/                |          |\n","|    entropy_loss       | -2.39    |\n","|    explained_variance | 0.634    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8599     |\n","|    policy_loss        | -0.0865  |\n","|    value_loss         | 0.0378   |\n","------------------------------------\n","Eval num_timesteps=690000, episode_reward=512.00 +/- 71.11\n","Episode length: 6179.80 +/- 624.40\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.18e+03 |\n","|    mean_reward        | 512      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.5e+03  |\n","|    ep_rew_mean        | 453      |\n","| time/                 |          |\n","|    fps                | 325      |\n","|    iterations         | 8700     |\n","|    time_elapsed       | 2140     |\n","|    total_timesteps    | 696000   |\n","| train/                |          |\n","|    entropy_loss       | -2.6     |\n","|    explained_variance | 0.792    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8699     |\n","|    policy_loss        | 0.233    |\n","|    value_loss         | 0.0834   |\n","------------------------------------\n","Eval num_timesteps=700000, episode_reward=448.00 +/- 151.05\n","Episode length: 5531.40 +/- 1300.49\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.53e+03 |\n","|    mean_reward        | 448      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.52e+03 |\n","|    ep_rew_mean        | 454      |\n","| time/                 |          |\n","|    fps                | 324      |\n","|    iterations         | 8800     |\n","|    time_elapsed       | 2170     |\n","|    total_timesteps    | 704000   |\n","| train/                |          |\n","|    entropy_loss       | -2.59    |\n","|    explained_variance | 0.831    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8799     |\n","|    policy_loss        | -0.0323  |\n","|    value_loss         | 0.0334   |\n","------------------------------------\n","Eval num_timesteps=710000, episode_reward=512.00 +/- 79.60\n","Episode length: 6005.80 +/- 766.75\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.01e+03 |\n","|    mean_reward        | 512      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.57e+03 |\n","|    ep_rew_mean        | 458      |\n","| time/                 |          |\n","|    fps                | 323      |\n","|    iterations         | 8900     |\n","|    time_elapsed       | 2202     |\n","|    total_timesteps    | 712000   |\n","| train/                |          |\n","|    entropy_loss       | -2.6     |\n","|    explained_variance | 0.472    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8899     |\n","|    policy_loss        | 0.214    |\n","|    value_loss         | 0.105    |\n","------------------------------------\n","Eval num_timesteps=720000, episode_reward=452.00 +/- 20.40\n","Episode length: 5459.40 +/- 272.43\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.46e+03 |\n","|    mean_reward        | 452      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.61e+03 |\n","|    ep_rew_mean        | 462      |\n","| time/                 |          |\n","|    fps                | 322      |\n","|    iterations         | 9000     |\n","|    time_elapsed       | 2233     |\n","|    total_timesteps    | 720000   |\n","| train/                |          |\n","|    entropy_loss       | -2.64    |\n","|    explained_variance | 0.667    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8999     |\n","|    policy_loss        | 0.00199  |\n","|    value_loss         | 0.0635   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.63e+03 |\n","|    ep_rew_mean        | 464      |\n","| time/                 |          |\n","|    fps                | 324      |\n","|    iterations         | 9100     |\n","|    time_elapsed       | 2246     |\n","|    total_timesteps    | 728000   |\n","| train/                |          |\n","|    entropy_loss       | -2.58    |\n","|    explained_variance | 0.855    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9099     |\n","|    policy_loss        | -0.128   |\n","|    value_loss         | 0.041    |\n","------------------------------------\n","Eval num_timesteps=730000, episode_reward=392.00 +/- 48.33\n","Episode length: 4890.00 +/- 608.30\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.89e+03 |\n","|    mean_reward        | 392      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.62e+03 |\n","|    ep_rew_mean        | 463      |\n","| time/                 |          |\n","|    fps                | 323      |\n","|    iterations         | 9200     |\n","|    time_elapsed       | 2275     |\n","|    total_timesteps    | 736000   |\n","| train/                |          |\n","|    entropy_loss       | -2.67    |\n","|    explained_variance | 0.778    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9199     |\n","|    policy_loss        | 0.0678   |\n","|    value_loss         | 0.0306   |\n","------------------------------------\n","Eval num_timesteps=740000, episode_reward=592.00 +/- 46.65\n","Episode length: 6919.20 +/- 381.09\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.92e+03 |\n","|    mean_reward        | 592      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.64e+03 |\n","|    ep_rew_mean        | 466      |\n","| time/                 |          |\n","|    fps                | 322      |\n","|    iterations         | 9300     |\n","|    time_elapsed       | 2310     |\n","|    total_timesteps    | 744000   |\n","| train/                |          |\n","|    entropy_loss       | -2.59    |\n","|    explained_variance | 0.914    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9299     |\n","|    policy_loss        | -0.0334  |\n","|    value_loss         | 0.0173   |\n","------------------------------------\n","Eval num_timesteps=750000, episode_reward=444.00 +/- 128.62\n","Episode length: 5492.80 +/- 1393.04\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.49e+03 |\n","|    mean_reward        | 444      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.7e+03  |\n","|    ep_rew_mean        | 473      |\n","| time/                 |          |\n","|    fps                | 321      |\n","|    iterations         | 9400     |\n","|    time_elapsed       | 2340     |\n","|    total_timesteps    | 752000   |\n","| train/                |          |\n","|    entropy_loss       | -2.53    |\n","|    explained_variance | 0.352    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9399     |\n","|    policy_loss        | -0.12    |\n","|    value_loss         | 0.0504   |\n","------------------------------------\n","Eval num_timesteps=760000, episode_reward=468.00 +/- 169.52\n","Episode length: 5743.60 +/- 1824.30\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.74e+03 |\n","|    mean_reward        | 468      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.74e+03 |\n","|    ep_rew_mean        | 478      |\n","| time/                 |          |\n","|    fps                | 320      |\n","|    iterations         | 9500     |\n","|    time_elapsed       | 2371     |\n","|    total_timesteps    | 760000   |\n","| train/                |          |\n","|    entropy_loss       | -2.65    |\n","|    explained_variance | 0.843    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9499     |\n","|    policy_loss        | 0.00683  |\n","|    value_loss         | 0.0387   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.73e+03 |\n","|    ep_rew_mean        | 478      |\n","| time/                 |          |\n","|    fps                | 321      |\n","|    iterations         | 9600     |\n","|    time_elapsed       | 2385     |\n","|    total_timesteps    | 768000   |\n","| train/                |          |\n","|    entropy_loss       | -2.6     |\n","|    explained_variance | 0.632    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9599     |\n","|    policy_loss        | -0.103   |\n","|    value_loss         | 0.0849   |\n","------------------------------------\n","Eval num_timesteps=770000, episode_reward=532.00 +/- 136.59\n","Episode length: 6481.60 +/- 1564.91\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.48e+03 |\n","|    mean_reward        | 532      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.77e+03 |\n","|    ep_rew_mean        | 482      |\n","| time/                 |          |\n","|    fps                | 320      |\n","|    iterations         | 9700     |\n","|    time_elapsed       | 2418     |\n","|    total_timesteps    | 776000   |\n","| train/                |          |\n","|    entropy_loss       | -2.63    |\n","|    explained_variance | 0.937    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9699     |\n","|    policy_loss        | -0.0748  |\n","|    value_loss         | 0.0126   |\n","------------------------------------\n","Eval num_timesteps=780000, episode_reward=524.00 +/- 139.37\n","Episode length: 6261.60 +/- 1400.68\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.26e+03 |\n","|    mean_reward        | 524      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.76e+03 |\n","|    ep_rew_mean        | 481      |\n","| time/                 |          |\n","|    fps                | 319      |\n","|    iterations         | 9800     |\n","|    time_elapsed       | 2451     |\n","|    total_timesteps    | 784000   |\n","| train/                |          |\n","|    entropy_loss       | -2.65    |\n","|    explained_variance | 0.497    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9799     |\n","|    policy_loss        | 0.00603  |\n","|    value_loss         | 0.0734   |\n","------------------------------------\n","Eval num_timesteps=790000, episode_reward=432.00 +/- 136.59\n","Episode length: 5405.60 +/- 1535.57\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.41e+03 |\n","|    mean_reward        | 432      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.74e+03 |\n","|    ep_rew_mean        | 479      |\n","| time/                 |          |\n","|    fps                | 319      |\n","|    iterations         | 9900     |\n","|    time_elapsed       | 2481     |\n","|    total_timesteps    | 792000   |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.578    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9899     |\n","|    policy_loss        | 0.113    |\n","|    value_loss         | 0.125    |\n","------------------------------------\n","Eval num_timesteps=800000, episode_reward=548.00 +/- 49.96\n","Episode length: 6309.60 +/- 255.73\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.31e+03 |\n","|    mean_reward        | 548      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.77e+03 |\n","|    ep_rew_mean        | 480      |\n","| time/                 |          |\n","|    fps                | 318      |\n","|    iterations         | 10000    |\n","|    time_elapsed       | 2514     |\n","|    total_timesteps    | 800000   |\n","| train/                |          |\n","|    entropy_loss       | -2.58    |\n","|    explained_variance | 0.667    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9999     |\n","|    policy_loss        | -0.0668  |\n","|    value_loss         | 0.0232   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 5.86e+03 |\n","|    ep_rew_mean        | 487      |\n","| time/                 |          |\n","|    fps                | 319      |\n","|    iterations         | 10100    |\n","|    time_elapsed       | 2528     |\n","|    total_timesteps    | 808000   |\n","| train/                |          |\n","|    entropy_loss       | -2.52    |\n","|    explained_variance | 0.59     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10099    |\n","|    policy_loss        | -0.152   |\n","|    value_loss         | 0.118    |\n","------------------------------------\n","Eval num_timesteps=810000, episode_reward=392.00 +/- 129.37\n","Episode length: 4878.40 +/- 1124.72\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 4.88e+03 |\n","|    mean_reward        | 392      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.95e+03 |\n","|    ep_rew_mean        | 496      |\n","| time/                 |          |\n","|    fps                | 319      |\n","|    iterations         | 10200    |\n","|    time_elapsed       | 2556     |\n","|    total_timesteps    | 816000   |\n","| train/                |          |\n","|    entropy_loss       | -2.63    |\n","|    explained_variance | 0.869    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10199    |\n","|    policy_loss        | -0.14    |\n","|    value_loss         | 0.0185   |\n","------------------------------------\n","Eval num_timesteps=820000, episode_reward=544.00 +/- 86.16\n","Episode length: 6399.60 +/- 786.15\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.4e+03  |\n","|    mean_reward        | 544      |\n","| rollout/              |          |\n","|    ep_len_mean        | 5.98e+03 |\n","|    ep_rew_mean        | 498      |\n","| time/                 |          |\n","|    fps                | 318      |\n","|    iterations         | 10300    |\n","|    time_elapsed       | 2590     |\n","|    total_timesteps    | 824000   |\n","| train/                |          |\n","|    entropy_loss       | -2.57    |\n","|    explained_variance | 0.887    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10299    |\n","|    policy_loss        | -0.276   |\n","|    value_loss         | 0.0489   |\n","------------------------------------\n","Eval num_timesteps=830000, episode_reward=620.00 +/- 71.55\n","Episode length: 7301.60 +/- 805.27\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 7.3e+03  |\n","|    mean_reward        | 620      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6e+03    |\n","|    ep_rew_mean        | 500      |\n","| time/                 |          |\n","|    fps                | 316      |\n","|    iterations         | 10400    |\n","|    time_elapsed       | 2626     |\n","|    total_timesteps    | 832000   |\n","| train/                |          |\n","|    entropy_loss       | -2.6     |\n","|    explained_variance | 0.805    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10399    |\n","|    policy_loss        | 0.134    |\n","|    value_loss         | 0.0402   |\n","------------------------------------\n","Eval num_timesteps=840000, episode_reward=528.00 +/- 179.15\n","Episode length: 6304.20 +/- 1943.91\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.3e+03  |\n","|    mean_reward        | 528      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.07e+03 |\n","|    ep_rew_mean        | 507      |\n","| time/                 |          |\n","|    fps                | 315      |\n","|    iterations         | 10500    |\n","|    time_elapsed       | 2659     |\n","|    total_timesteps    | 840000   |\n","| train/                |          |\n","|    entropy_loss       | -2.62    |\n","|    explained_variance | 0.775    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10499    |\n","|    policy_loss        | 0.103    |\n","|    value_loss         | 0.064    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 6.13e+03 |\n","|    ep_rew_mean        | 513      |\n","| time/                 |          |\n","|    fps                | 317      |\n","|    iterations         | 10600    |\n","|    time_elapsed       | 2672     |\n","|    total_timesteps    | 848000   |\n","| train/                |          |\n","|    entropy_loss       | -2.59    |\n","|    explained_variance | 0.827    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10599    |\n","|    policy_loss        | -0.0613  |\n","|    value_loss         | 0.0718   |\n","------------------------------------\n","Eval num_timesteps=850000, episode_reward=600.00 +/- 132.66\n","Episode length: 7256.60 +/- 1437.94\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 7.26e+03 |\n","|    mean_reward        | 600      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.13e+03 |\n","|    ep_rew_mean        | 514      |\n","| time/                 |          |\n","|    fps                | 316      |\n","|    iterations         | 10700    |\n","|    time_elapsed       | 2708     |\n","|    total_timesteps    | 856000   |\n","| train/                |          |\n","|    entropy_loss       | -2.58    |\n","|    explained_variance | 0.735    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10699    |\n","|    policy_loss        | 0.00821  |\n","|    value_loss         | 0.0463   |\n","------------------------------------\n","Eval num_timesteps=860000, episode_reward=512.00 +/- 80.60\n","Episode length: 6184.20 +/- 915.36\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.18e+03 |\n","|    mean_reward        | 512      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.19e+03 |\n","|    ep_rew_mean        | 520      |\n","| time/                 |          |\n","|    fps                | 315      |\n","|    iterations         | 10800    |\n","|    time_elapsed       | 2741     |\n","|    total_timesteps    | 864000   |\n","| train/                |          |\n","|    entropy_loss       | -2.55    |\n","|    explained_variance | 0.487    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10799    |\n","|    policy_loss        | -0.107   |\n","|    value_loss         | 0.0696   |\n","------------------------------------\n","Eval num_timesteps=870000, episode_reward=524.00 +/- 86.16\n","Episode length: 6381.20 +/- 782.61\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.38e+03 |\n","|    mean_reward        | 524      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.23e+03 |\n","|    ep_rew_mean        | 524      |\n","| time/                 |          |\n","|    fps                | 314      |\n","|    iterations         | 10900    |\n","|    time_elapsed       | 2774     |\n","|    total_timesteps    | 872000   |\n","| train/                |          |\n","|    entropy_loss       | -2.49    |\n","|    explained_variance | 0.823    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10899    |\n","|    policy_loss        | 0.015    |\n","|    value_loss         | 0.0261   |\n","------------------------------------\n","Eval num_timesteps=880000, episode_reward=500.00 +/- 107.33\n","Episode length: 6038.00 +/- 1007.58\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.04e+03 |\n","|    mean_reward        | 500      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.27e+03 |\n","|    ep_rew_mean        | 528      |\n","| time/                 |          |\n","|    fps                | 313      |\n","|    iterations         | 11000    |\n","|    time_elapsed       | 2806     |\n","|    total_timesteps    | 880000   |\n","| train/                |          |\n","|    entropy_loss       | -2.62    |\n","|    explained_variance | 0.84     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10999    |\n","|    policy_loss        | -0.00507 |\n","|    value_loss         | 0.0348   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 6.36e+03 |\n","|    ep_rew_mean        | 536      |\n","| time/                 |          |\n","|    fps                | 314      |\n","|    iterations         | 11100    |\n","|    time_elapsed       | 2820     |\n","|    total_timesteps    | 888000   |\n","| train/                |          |\n","|    entropy_loss       | -2.61    |\n","|    explained_variance | 0.862    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11099    |\n","|    policy_loss        | 0.151    |\n","|    value_loss         | 0.0448   |\n","------------------------------------\n","Eval num_timesteps=890000, episode_reward=464.00 +/- 88.90\n","Episode length: 5629.40 +/- 888.26\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.63e+03 |\n","|    mean_reward        | 464      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.39e+03 |\n","|    ep_rew_mean        | 538      |\n","| time/                 |          |\n","|    fps                | 314      |\n","|    iterations         | 11200    |\n","|    time_elapsed       | 2850     |\n","|    total_timesteps    | 896000   |\n","| train/                |          |\n","|    entropy_loss       | -2.65    |\n","|    explained_variance | 0.65     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11199    |\n","|    policy_loss        | 0.193    |\n","|    value_loss         | 0.118    |\n","------------------------------------\n","Eval num_timesteps=900000, episode_reward=532.00 +/- 123.03\n","Episode length: 6201.60 +/- 1349.82\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.2e+03  |\n","|    mean_reward        | 532      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.4e+03  |\n","|    ep_rew_mean        | 538      |\n","| time/                 |          |\n","|    fps                | 313      |\n","|    iterations         | 11300    |\n","|    time_elapsed       | 2883     |\n","|    total_timesteps    | 904000   |\n","| train/                |          |\n","|    entropy_loss       | -2.61    |\n","|    explained_variance | 0.841    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11299    |\n","|    policy_loss        | 0.0773   |\n","|    value_loss         | 0.0256   |\n","------------------------------------\n","Eval num_timesteps=910000, episode_reward=624.00 +/- 89.80\n","Episode length: 7274.80 +/- 823.97\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 7.27e+03 |\n","|    mean_reward        | 624      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.42e+03 |\n","|    ep_rew_mean        | 540      |\n","| time/                 |          |\n","|    fps                | 312      |\n","|    iterations         | 11400    |\n","|    time_elapsed       | 2919     |\n","|    total_timesteps    | 912000   |\n","| train/                |          |\n","|    entropy_loss       | -2.52    |\n","|    explained_variance | 0.93     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11399    |\n","|    policy_loss        | -0.111   |\n","|    value_loss         | 0.018    |\n","------------------------------------\n","Eval num_timesteps=920000, episode_reward=664.00 +/- 105.38\n","Episode length: 7810.00 +/- 1148.75\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 7.81e+03 |\n","|    mean_reward        | 664      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.43e+03 |\n","|    ep_rew_mean        | 540      |\n","| time/                 |          |\n","|    fps                | 311      |\n","|    iterations         | 11500    |\n","|    time_elapsed       | 2957     |\n","|    total_timesteps    | 920000   |\n","| train/                |          |\n","|    entropy_loss       | -2.58    |\n","|    explained_variance | 0.96     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11499    |\n","|    policy_loss        | 0.00459  |\n","|    value_loss         | 0.0107   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 6.37e+03 |\n","|    ep_rew_mean        | 534      |\n","| time/                 |          |\n","|    fps                | 312      |\n","|    iterations         | 11600    |\n","|    time_elapsed       | 2970     |\n","|    total_timesteps    | 928000   |\n","| train/                |          |\n","|    entropy_loss       | -2.39    |\n","|    explained_variance | 0.884    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11599    |\n","|    policy_loss        | 0.0791   |\n","|    value_loss         | 0.0315   |\n","------------------------------------\n","Eval num_timesteps=930000, episode_reward=548.00 +/- 155.74\n","Episode length: 6566.40 +/- 1584.20\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.57e+03 |\n","|    mean_reward        | 548      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.34e+03 |\n","|    ep_rew_mean        | 532      |\n","| time/                 |          |\n","|    fps                | 311      |\n","|    iterations         | 11700    |\n","|    time_elapsed       | 3004     |\n","|    total_timesteps    | 936000   |\n","| train/                |          |\n","|    entropy_loss       | -2.64    |\n","|    explained_variance | 0.842    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11699    |\n","|    policy_loss        | -0.0563  |\n","|    value_loss         | 0.0273   |\n","------------------------------------\n","Eval num_timesteps=940000, episode_reward=560.00 +/- 119.33\n","Episode length: 6604.20 +/- 1210.10\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.6e+03  |\n","|    mean_reward        | 560      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.38e+03 |\n","|    ep_rew_mean        | 536      |\n","| time/                 |          |\n","|    fps                | 310      |\n","|    iterations         | 11800    |\n","|    time_elapsed       | 3038     |\n","|    total_timesteps    | 944000   |\n","| train/                |          |\n","|    entropy_loss       | -2.5     |\n","|    explained_variance | 0.621    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11799    |\n","|    policy_loss        | 0.0967   |\n","|    value_loss         | 0.0427   |\n","------------------------------------\n","Eval num_timesteps=950000, episode_reward=544.00 +/- 89.80\n","Episode length: 6661.80 +/- 885.70\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.66e+03 |\n","|    mean_reward        | 544      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.39e+03 |\n","|    ep_rew_mean        | 536      |\n","| time/                 |          |\n","|    fps                | 309      |\n","|    iterations         | 11900    |\n","|    time_elapsed       | 3072     |\n","|    total_timesteps    | 952000   |\n","| train/                |          |\n","|    entropy_loss       | -2.61    |\n","|    explained_variance | 0.896    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11899    |\n","|    policy_loss        | -0.221   |\n","|    value_loss         | 0.0201   |\n","------------------------------------\n","Eval num_timesteps=960000, episode_reward=648.00 +/- 73.32\n","Episode length: 7430.40 +/- 660.93\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 7.43e+03 |\n","|    mean_reward        | 648      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.47e+03 |\n","|    ep_rew_mean        | 545      |\n","| time/                 |          |\n","|    fps                | 308      |\n","|    iterations         | 12000    |\n","|    time_elapsed       | 3108     |\n","|    total_timesteps    | 960000   |\n","| train/                |          |\n","|    entropy_loss       | -2.7     |\n","|    explained_variance | 0.959    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11999    |\n","|    policy_loss        | -0.0299  |\n","|    value_loss         | 0.0101   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 6.48e+03 |\n","|    ep_rew_mean        | 547      |\n","| time/                 |          |\n","|    fps                | 310      |\n","|    iterations         | 12100    |\n","|    time_elapsed       | 3122     |\n","|    total_timesteps    | 968000   |\n","| train/                |          |\n","|    entropy_loss       | -2.6     |\n","|    explained_variance | 0.891    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12099    |\n","|    policy_loss        | 0.0414   |\n","|    value_loss         | 0.0403   |\n","------------------------------------\n","Eval num_timesteps=970000, episode_reward=584.00 +/- 63.75\n","Episode length: 6867.60 +/- 791.84\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 6.87e+03 |\n","|    mean_reward        | 584      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.49e+03 |\n","|    ep_rew_mean        | 548      |\n","| time/                 |          |\n","|    fps                | 309      |\n","|    iterations         | 12200    |\n","|    time_elapsed       | 3157     |\n","|    total_timesteps    | 976000   |\n","| train/                |          |\n","|    entropy_loss       | -2.65    |\n","|    explained_variance | 0.943    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12199    |\n","|    policy_loss        | -0.0834  |\n","|    value_loss         | 0.0196   |\n","------------------------------------\n","Eval num_timesteps=980000, episode_reward=476.00 +/- 206.07\n","Episode length: 5711.40 +/- 1937.08\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 5.71e+03 |\n","|    mean_reward        | 476      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.48e+03 |\n","|    ep_rew_mean        | 546      |\n","| time/                 |          |\n","|    fps                | 308      |\n","|    iterations         | 12300    |\n","|    time_elapsed       | 3188     |\n","|    total_timesteps    | 984000   |\n","| train/                |          |\n","|    entropy_loss       | -2.54    |\n","|    explained_variance | 0.881    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12299    |\n","|    policy_loss        | 0.0312   |\n","|    value_loss         | 0.0383   |\n","------------------------------------\n","Eval num_timesteps=990000, episode_reward=604.00 +/- 139.37\n","Episode length: 7087.80 +/- 1422.84\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 7.09e+03 |\n","|    mean_reward        | 604      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.52e+03 |\n","|    ep_rew_mean        | 550      |\n","| time/                 |          |\n","|    fps                | 307      |\n","|    iterations         | 12400    |\n","|    time_elapsed       | 3223     |\n","|    total_timesteps    | 992000   |\n","| train/                |          |\n","|    entropy_loss       | -2.73    |\n","|    explained_variance | 0.833    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12399    |\n","|    policy_loss        | 0.243    |\n","|    value_loss         | 0.0506   |\n","------------------------------------\n","Eval num_timesteps=1000000, episode_reward=664.00 +/- 80.40\n","Episode length: 7732.20 +/- 883.92\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 7.73e+03 |\n","|    mean_reward        | 664      |\n","| rollout/              |          |\n","|    ep_len_mean        | 6.56e+03 |\n","|    ep_rew_mean        | 553      |\n","| time/                 |          |\n","|    fps                | 306      |\n","|    iterations         | 12500    |\n","|    time_elapsed       | 3260     |\n","|    total_timesteps    | 1000000  |\n","| train/                |          |\n","|    entropy_loss       | -2.66    |\n","|    explained_variance | 0.909    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12499    |\n","|    policy_loss        | 0.048    |\n","|    value_loss         | 0.0304   |\n","------------------------------------\n","Saving to logs/a2c/SeaquestNoFrameskip-v4_1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-fHBq73665yD"},"source":["#### Evaluate trained agent\n","\n","\n","You can remove the `--folder logs/` to evaluate pretrained agent."]},{"cell_type":"code","metadata":{"id":"Bw8YuEgU6bT3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625074402359,"user_tz":-120,"elapsed":19893,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"29cd03ad-ec7e-4550-b50b-3c95b9c3eaa6"},"source":["!python enjoy.py --algo a2c --env SeaquestNoFrameskip-v4 --no-render --n-timesteps 5000 --folder logs/"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Loading latest experiment, id=1\n","Loading logs/a2c/SeaquestNoFrameskip-v4_1/SeaquestNoFrameskip-v4.zip\n","Stacking 4 frames\n","Wrapping the env in a VecTransposeImage.\n","Atari Episode Score: 680.00\n","Atari Episode Length 7554\n","Atari Episode Score: 360.00\n","Atari Episode Length 5041\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xVm9QPNVwKXN"},"source":["### Record  a Video"]},{"cell_type":"code","metadata":{"id":"MPyfQxD5z26J","executionInfo":{"status":"ok","timestamp":1625074402360,"user_tz":-120,"elapsed":15,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}}},"source":["# Set up display; otherwise rendering will fail\n","import os\n","os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n","os.environ['DISPLAY'] = ':1'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ip3AauLzwNGP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625074417842,"user_tz":-120,"elapsed":15495,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"3a405b14-6f4b-4a29-9b91-6c579b900bad"},"source":["!python -m utils.record_video --algo a2c --env SeaquestNoFrameskip-v4 --exp-id 0 -f logs/ -n 1000"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Loading latest experiment, id=1\n","Stacking 4 frames\n","Saving video to /content/drive/My Drive/Colab Notebooks/TFM/Seaquest/rl-baselines3-zoo/logs/a2c/SeaquestNoFrameskip-v4_1/videos/final-model-a2c-SeaquestNoFrameskip-v4-step-0-to-step-1000.mp4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"efvw9jSeAZLk"},"source":["## Train an BreakoutNoFrameskip-v4 DQN\n","\n","Steps: 1M\n"]},{"cell_type":"code","metadata":{"id":"VEO4M5UBAZLv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625076561701,"user_tz":-120,"elapsed":2128339,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"2e1301b7-722b-45f2-aac4-ae8b06337548"},"source":["!python train.py --algo a2c --env BreakoutNoFrameskip-v4 --n-timesteps 1000000 -tb logs"],"execution_count":10,"outputs":[{"output_type":"stream","text":["========== BreakoutNoFrameskip-v4 ==========\n","Seed: 3331479400\n","Default hyperparameters for environment (ones being tuned will be overridden):\n","OrderedDict([('ent_coef', 0.01),\n","             ('env_wrapper',\n","              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n","             ('frame_stack', 4),\n","             ('n_envs', 16),\n","             ('n_timesteps', 10000000.0),\n","             ('policy', 'CnnPolicy'),\n","             ('policy_kwargs',\n","              'dict(optimizer_class=RMSpropTFLike, '\n","              'optimizer_kwargs=dict(eps=1e-5))'),\n","             ('vf_coef', 0.25)])\n","Using 16 environments\n","Overwriting n_timesteps with n=1000000\n","Creating test environment\n","Stacking 4 frames\n","Wrapping into a VecTransposeImage\n","Stacking 4 frames\n","Wrapping into a VecTransposeImage\n","Using cuda device\n","Log path: logs/a2c/BreakoutNoFrameskip-v4_1\n","2021-06-30 17:34:05.451512: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","Logging to logs/BreakoutNoFrameskip-v4/A2C_1\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 784      |\n","|    ep_rew_mean        | 1.58     |\n","| time/                 |          |\n","|    fps                | 480      |\n","|    iterations         | 100      |\n","|    time_elapsed       | 16       |\n","|    total_timesteps    | 8000     |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.00216  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 99       |\n","|    policy_loss        | 0.0272   |\n","|    value_loss         | 0.0248   |\n","------------------------------------\n","Eval num_timesteps=10000, episode_reward=1.00 +/- 0.63\n","Episode length: 667.20 +/- 77.02\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 667      |\n","|    mean_reward        | 1        |\n","| rollout/              |          |\n","|    ep_len_mean        | 784      |\n","|    ep_rew_mean        | 1.62     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 200      |\n","|    time_elapsed       | 33       |\n","|    total_timesteps    | 16000    |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | -0.00782 |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 199      |\n","|    policy_loss        | 0.0171   |\n","|    value_loss         | 0.0278   |\n","------------------------------------\n","Eval num_timesteps=20000, episode_reward=0.60 +/- 0.80\n","Episode length: 635.20 +/- 151.53\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 635      |\n","|    mean_reward        | 0.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 787      |\n","|    ep_rew_mean        | 1.69     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 300      |\n","|    time_elapsed       | 50       |\n","|    total_timesteps    | 24000    |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | -0.12    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 299      |\n","|    policy_loss        | -0.0376  |\n","|    value_loss         | 0.0061   |\n","------------------------------------\n","Eval num_timesteps=30000, episode_reward=2.20 +/- 2.04\n","Episode length: 778.80 +/- 188.77\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 779      |\n","|    mean_reward        | 2.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 765      |\n","|    ep_rew_mean        | 1.55     |\n","| time/                 |          |\n","|    fps                | 469      |\n","|    iterations         | 400      |\n","|    time_elapsed       | 68       |\n","|    total_timesteps    | 32000    |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.0304   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 399      |\n","|    policy_loss        | 0.035    |\n","|    value_loss         | 0.0361   |\n","------------------------------------\n","Eval num_timesteps=40000, episode_reward=2.00 +/- 1.79\n","Episode length: 863.00 +/- 312.68\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 863      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 744      |\n","|    ep_rew_mean        | 1.35     |\n","| time/                 |          |\n","|    fps                | 465      |\n","|    iterations         | 500      |\n","|    time_elapsed       | 85       |\n","|    total_timesteps    | 40000    |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | -0.228   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 499      |\n","|    policy_loss        | -0.0262  |\n","|    value_loss         | 0.00293  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 728      |\n","|    ep_rew_mean        | 1.32     |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 600      |\n","|    time_elapsed       | 101      |\n","|    total_timesteps    | 48000    |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | -0.00549 |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 599      |\n","|    policy_loss        | 0.0366   |\n","|    value_loss         | 0.0541   |\n","------------------------------------\n","Eval num_timesteps=50000, episode_reward=2.00 +/- 2.61\n","Episode length: 762.00 +/- 293.76\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 762      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 728      |\n","|    ep_rew_mean        | 1.34     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 700      |\n","|    time_elapsed       | 118      |\n","|    total_timesteps    | 56000    |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.0491   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 699      |\n","|    policy_loss        | -0.0316  |\n","|    value_loss         | 0.00378  |\n","------------------------------------\n","Eval num_timesteps=60000, episode_reward=1.20 +/- 0.98\n","Episode length: 708.20 +/- 162.43\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 708      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 720      |\n","|    ep_rew_mean        | 1.26     |\n","| time/                 |          |\n","|    fps                | 470      |\n","|    iterations         | 800      |\n","|    time_elapsed       | 136      |\n","|    total_timesteps    | 64000    |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.0224   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 799      |\n","|    policy_loss        | -0.00453 |\n","|    value_loss         | 0.000307 |\n","------------------------------------\n","Eval num_timesteps=70000, episode_reward=2.00 +/- 1.10\n","Episode length: 847.80 +/- 149.17\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 848      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 733      |\n","|    ep_rew_mean        | 1.36     |\n","| time/                 |          |\n","|    fps                | 468      |\n","|    iterations         | 900      |\n","|    time_elapsed       | 153      |\n","|    total_timesteps    | 72000    |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.0244   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 899      |\n","|    policy_loss        | 0.168    |\n","|    value_loss         | 0.129    |\n","------------------------------------\n","Eval num_timesteps=80000, episode_reward=0.40 +/- 0.49\n","Episode length: 595.40 +/- 99.54\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 595      |\n","|    mean_reward        | 0.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 754      |\n","|    ep_rew_mean        | 1.46     |\n","| time/                 |          |\n","|    fps                | 469      |\n","|    iterations         | 1000     |\n","|    time_elapsed       | 170      |\n","|    total_timesteps    | 80000    |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.017    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 999      |\n","|    policy_loss        | 0.0399   |\n","|    value_loss         | 0.0362   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 795      |\n","|    ep_rew_mean        | 1.67     |\n","| time/                 |          |\n","|    fps                | 474      |\n","|    iterations         | 1100     |\n","|    time_elapsed       | 185      |\n","|    total_timesteps    | 88000    |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.00302  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1099     |\n","|    policy_loss        | 0.0121   |\n","|    value_loss         | 0.0271   |\n","------------------------------------\n","Eval num_timesteps=90000, episode_reward=0.40 +/- 0.80\n","Episode length: 572.80 +/- 118.67\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 573      |\n","|    mean_reward        | 0.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 805      |\n","|    ep_rew_mean        | 1.71     |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 1200     |\n","|    time_elapsed       | 201      |\n","|    total_timesteps    | 96000    |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | -0.193   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1199     |\n","|    policy_loss        | -0.0176  |\n","|    value_loss         | 0.00206  |\n","------------------------------------\n","Eval num_timesteps=100000, episode_reward=2.40 +/- 2.42\n","Episode length: 828.20 +/- 263.18\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 828      |\n","|    mean_reward        | 2.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 799      |\n","|    ep_rew_mean        | 1.68     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 1300     |\n","|    time_elapsed       | 219      |\n","|    total_timesteps    | 104000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.617    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1299     |\n","|    policy_loss        | -0.0177  |\n","|    value_loss         | 0.000285 |\n","------------------------------------\n","Eval num_timesteps=110000, episode_reward=1.00 +/- 1.10\n","Episode length: 691.60 +/- 217.53\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 692      |\n","|    mean_reward        | 1        |\n","| rollout/              |          |\n","|    ep_len_mean        | 772      |\n","|    ep_rew_mean        | 1.53     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 1400     |\n","|    time_elapsed       | 236      |\n","|    total_timesteps    | 112000   |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.014    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1399     |\n","|    policy_loss        | -0.00194 |\n","|    value_loss         | 0.0426   |\n","------------------------------------\n","Eval num_timesteps=120000, episode_reward=0.80 +/- 1.17\n","Episode length: 644.80 +/- 187.42\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 645      |\n","|    mean_reward        | 0.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 763      |\n","|    ep_rew_mean        | 1.5      |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 1500     |\n","|    time_elapsed       | 253      |\n","|    total_timesteps    | 120000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | -0.0618  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1499     |\n","|    policy_loss        | -0.0323  |\n","|    value_loss         | 0.00362  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 741      |\n","|    ep_rew_mean        | 1.42     |\n","| time/                 |          |\n","|    fps                | 476      |\n","|    iterations         | 1600     |\n","|    time_elapsed       | 268      |\n","|    total_timesteps    | 128000   |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | -0.0319  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1599     |\n","|    policy_loss        | -0.0548  |\n","|    value_loss         | 0.00624  |\n","------------------------------------\n","Eval num_timesteps=130000, episode_reward=1.20 +/- 1.94\n","Episode length: 702.60 +/- 314.01\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 703      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 751      |\n","|    ep_rew_mean        | 1.5      |\n","| time/                 |          |\n","|    fps                | 476      |\n","|    iterations         | 1700     |\n","|    time_elapsed       | 285      |\n","|    total_timesteps    | 136000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.022    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1699     |\n","|    policy_loss        | 0.0545   |\n","|    value_loss         | 0.0636   |\n","------------------------------------\n","Eval num_timesteps=140000, episode_reward=0.80 +/- 0.75\n","Episode length: 676.60 +/- 137.22\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 677      |\n","|    mean_reward        | 0.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 727      |\n","|    ep_rew_mean        | 1.31     |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 1800     |\n","|    time_elapsed       | 302      |\n","|    total_timesteps    | 144000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.45     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1799     |\n","|    policy_loss        | -0.00775 |\n","|    value_loss         | 0.000367 |\n","------------------------------------\n","Eval num_timesteps=150000, episode_reward=0.80 +/- 0.75\n","Episode length: 664.60 +/- 148.76\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 665      |\n","|    mean_reward        | 0.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 686      |\n","|    ep_rew_mean        | 1.01     |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 1900     |\n","|    time_elapsed       | 319      |\n","|    total_timesteps    | 152000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | -0.0211  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1899     |\n","|    policy_loss        | -0.053   |\n","|    value_loss         | 0.0133   |\n","------------------------------------\n","Eval num_timesteps=160000, episode_reward=0.80 +/- 0.75\n","Episode length: 639.60 +/- 113.48\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 640      |\n","|    mean_reward        | 0.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 713      |\n","|    ep_rew_mean        | 1.15     |\n","| time/                 |          |\n","|    fps                | 474      |\n","|    iterations         | 2000     |\n","|    time_elapsed       | 336      |\n","|    total_timesteps    | 160000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.00222  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1999     |\n","|    policy_loss        | 0.0546   |\n","|    value_loss         | 0.0518   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 724      |\n","|    ep_rew_mean        | 1.28     |\n","| time/                 |          |\n","|    fps                | 477      |\n","|    iterations         | 2100     |\n","|    time_elapsed       | 352      |\n","|    total_timesteps    | 168000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.0407   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2099     |\n","|    policy_loss        | 0.0391   |\n","|    value_loss         | 0.0371   |\n","------------------------------------\n","Eval num_timesteps=170000, episode_reward=2.80 +/- 1.72\n","Episode length: 1033.40 +/- 292.06\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.03e+03 |\n","|    mean_reward        | 2.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 738      |\n","|    ep_rew_mean        | 1.4      |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 2200     |\n","|    time_elapsed       | 370      |\n","|    total_timesteps    | 176000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.118    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2199     |\n","|    policy_loss        | -0.0557  |\n","|    value_loss         | 0.0101   |\n","------------------------------------\n","Eval num_timesteps=180000, episode_reward=0.00 +/- 0.00\n","Episode length: 530.20 +/- 14.55\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 530      |\n","|    mean_reward        | 0        |\n","| rollout/              |          |\n","|    ep_len_mean        | 740      |\n","|    ep_rew_mean        | 1.37     |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 2300     |\n","|    time_elapsed       | 387      |\n","|    total_timesteps    | 184000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.0935   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2299     |\n","|    policy_loss        | -0.0441  |\n","|    value_loss         | 0.00609  |\n","------------------------------------\n","Eval num_timesteps=190000, episode_reward=1.00 +/- 1.10\n","Episode length: 676.00 +/- 182.81\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 676      |\n","|    mean_reward        | 1        |\n","| rollout/              |          |\n","|    ep_len_mean        | 735      |\n","|    ep_rew_mean        | 1.32     |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 2400     |\n","|    time_elapsed       | 404      |\n","|    total_timesteps    | 192000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.0736   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2399     |\n","|    policy_loss        | 0.0315   |\n","|    value_loss         | 0.0263   |\n","------------------------------------\n","Eval num_timesteps=200000, episode_reward=0.60 +/- 0.49\n","Episode length: 588.00 +/- 47.16\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 588      |\n","|    mean_reward        | 0.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 752      |\n","|    ep_rew_mean        | 1.4      |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 2500     |\n","|    time_elapsed       | 420      |\n","|    total_timesteps    | 200000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.857    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2499     |\n","|    policy_loss        | -0.0125  |\n","|    value_loss         | 0.000186 |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 757      |\n","|    ep_rew_mean        | 1.46     |\n","| time/                 |          |\n","|    fps                | 477      |\n","|    iterations         | 2600     |\n","|    time_elapsed       | 435      |\n","|    total_timesteps    | 208000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.443    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2599     |\n","|    policy_loss        | -0.0291  |\n","|    value_loss         | 0.00385  |\n","------------------------------------\n","Eval num_timesteps=210000, episode_reward=1.40 +/- 1.36\n","Episode length: 730.00 +/- 205.86\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 730      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 714      |\n","|    ep_rew_mean        | 1.2      |\n","| time/                 |          |\n","|    fps                | 476      |\n","|    iterations         | 2700     |\n","|    time_elapsed       | 453      |\n","|    total_timesteps    | 216000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.14     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2699     |\n","|    policy_loss        | 0.0252   |\n","|    value_loss         | 0.0263   |\n","------------------------------------\n","Eval num_timesteps=220000, episode_reward=2.00 +/- 1.10\n","Episode length: 824.00 +/- 177.42\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 824      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 722      |\n","|    ep_rew_mean        | 1.22     |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 2800     |\n","|    time_elapsed       | 470      |\n","|    total_timesteps    | 224000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.202    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2799     |\n","|    policy_loss        | 0.0637   |\n","|    value_loss         | 0.0436   |\n","------------------------------------\n","Eval num_timesteps=230000, episode_reward=1.20 +/- 0.75\n","Episode length: 713.20 +/- 111.25\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 713      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 766      |\n","|    ep_rew_mean        | 1.48     |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 2900     |\n","|    time_elapsed       | 488      |\n","|    total_timesteps    | 232000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.172    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2899     |\n","|    policy_loss        | 0.0105   |\n","|    value_loss         | 0.0533   |\n","------------------------------------\n","Eval num_timesteps=240000, episode_reward=2.80 +/- 1.72\n","Episode length: 957.20 +/- 298.17\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 957      |\n","|    mean_reward        | 2.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 775      |\n","|    ep_rew_mean        | 1.57     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 3000     |\n","|    time_elapsed       | 506      |\n","|    total_timesteps    | 240000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | -0.0231  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2999     |\n","|    policy_loss        | -0.132   |\n","|    value_loss         | 0.026    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 752      |\n","|    ep_rew_mean        | 1.42     |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 3100     |\n","|    time_elapsed       | 521      |\n","|    total_timesteps    | 248000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.0549   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3099     |\n","|    policy_loss        | 0.0203   |\n","|    value_loss         | 0.0546   |\n","------------------------------------\n","Eval num_timesteps=250000, episode_reward=1.40 +/- 1.36\n","Episode length: 737.80 +/- 230.72\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 738      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 730      |\n","|    ep_rew_mean        | 1.27     |\n","| time/                 |          |\n","|    fps                | 475      |\n","|    iterations         | 3200     |\n","|    time_elapsed       | 538      |\n","|    total_timesteps    | 256000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.187    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3199     |\n","|    policy_loss        | -0.0573  |\n","|    value_loss         | 0.0152   |\n","------------------------------------\n","Eval num_timesteps=260000, episode_reward=1.60 +/- 1.62\n","Episode length: 798.40 +/- 296.59\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 798      |\n","|    mean_reward        | 1.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 737      |\n","|    ep_rew_mean        | 1.31     |\n","| time/                 |          |\n","|    fps                | 474      |\n","|    iterations         | 3300     |\n","|    time_elapsed       | 556      |\n","|    total_timesteps    | 264000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.265    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3299     |\n","|    policy_loss        | 0.0733   |\n","|    value_loss         | 0.0703   |\n","------------------------------------\n","Eval num_timesteps=270000, episode_reward=2.20 +/- 2.23\n","Episode length: 914.20 +/- 404.30\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 914      |\n","|    mean_reward        | 2.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 724      |\n","|    ep_rew_mean        | 1.24     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 3400     |\n","|    time_elapsed       | 574      |\n","|    total_timesteps    | 272000   |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.0461   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3399     |\n","|    policy_loss        | -0.025   |\n","|    value_loss         | 0.00248  |\n","------------------------------------\n","Eval num_timesteps=280000, episode_reward=0.80 +/- 0.75\n","Episode length: 648.20 +/- 117.12\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 648      |\n","|    mean_reward        | 0.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 746      |\n","|    ep_rew_mean        | 1.35     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 3500     |\n","|    time_elapsed       | 591      |\n","|    total_timesteps    | 280000   |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.426    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3499     |\n","|    policy_loss        | -0.0207  |\n","|    value_loss         | 0.00444  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 758      |\n","|    ep_rew_mean        | 1.41     |\n","| time/                 |          |\n","|    fps                | 474      |\n","|    iterations         | 3600     |\n","|    time_elapsed       | 606      |\n","|    total_timesteps    | 288000   |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.207    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3599     |\n","|    policy_loss        | -0.0554  |\n","|    value_loss         | 0.016    |\n","------------------------------------\n","Eval num_timesteps=290000, episode_reward=2.00 +/- 1.90\n","Episode length: 879.20 +/- 331.50\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 879      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 745      |\n","|    ep_rew_mean        | 1.38     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 3700     |\n","|    time_elapsed       | 624      |\n","|    total_timesteps    | 296000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.511    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3699     |\n","|    policy_loss        | -0.00735 |\n","|    value_loss         | 0.00452  |\n","------------------------------------\n","Eval num_timesteps=300000, episode_reward=2.60 +/- 1.62\n","Episode length: 945.40 +/- 262.00\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 945      |\n","|    mean_reward        | 2.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 738      |\n","|    ep_rew_mean        | 1.31     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 3800     |\n","|    time_elapsed       | 642      |\n","|    total_timesteps    | 304000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.905    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3799     |\n","|    policy_loss        | 0.00427  |\n","|    value_loss         | 0.000423 |\n","------------------------------------\n","Eval num_timesteps=310000, episode_reward=1.40 +/- 1.50\n","Episode length: 789.60 +/- 289.48\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 790      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 767      |\n","|    ep_rew_mean        | 1.5      |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 3900     |\n","|    time_elapsed       | 659      |\n","|    total_timesteps    | 312000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.312    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3899     |\n","|    policy_loss        | -0.0524  |\n","|    value_loss         | 0.0129   |\n","------------------------------------\n","Eval num_timesteps=320000, episode_reward=1.60 +/- 1.62\n","Episode length: 819.80 +/- 290.26\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 820      |\n","|    mean_reward        | 1.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 786      |\n","|    ep_rew_mean        | 1.59     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 4000     |\n","|    time_elapsed       | 677      |\n","|    total_timesteps    | 320000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.721    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3999     |\n","|    policy_loss        | -0.00562 |\n","|    value_loss         | 0.00171  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 771      |\n","|    ep_rew_mean        | 1.49     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 4100     |\n","|    time_elapsed       | 692      |\n","|    total_timesteps    | 328000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.489    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4099     |\n","|    policy_loss        | 0.0345   |\n","|    value_loss         | 0.0225   |\n","------------------------------------\n","Eval num_timesteps=330000, episode_reward=4.40 +/- 4.03\n","Episode length: 1161.60 +/- 552.81\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.16e+03 |\n","|    mean_reward        | 4.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 772      |\n","|    ep_rew_mean        | 1.5      |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 4200     |\n","|    time_elapsed       | 711      |\n","|    total_timesteps    | 336000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | -0.0936  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4199     |\n","|    policy_loss        | -0.0963  |\n","|    value_loss         | 0.0154   |\n","------------------------------------\n","Eval num_timesteps=340000, episode_reward=2.20 +/- 3.43\n","Episode length: 808.60 +/- 447.41\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 809      |\n","|    mean_reward        | 2.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 746      |\n","|    ep_rew_mean        | 1.37     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 4300     |\n","|    time_elapsed       | 728      |\n","|    total_timesteps    | 344000   |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.509    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4299     |\n","|    policy_loss        | -0.00734 |\n","|    value_loss         | 0.00522  |\n","------------------------------------\n","Eval num_timesteps=350000, episode_reward=1.60 +/- 1.50\n","Episode length: 747.60 +/- 220.36\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 748      |\n","|    mean_reward        | 1.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 763      |\n","|    ep_rew_mean        | 1.45     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 4400     |\n","|    time_elapsed       | 746      |\n","|    total_timesteps    | 352000   |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.521    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4399     |\n","|    policy_loss        | -0.0145  |\n","|    value_loss         | 0.00126  |\n","------------------------------------\n","Eval num_timesteps=360000, episode_reward=2.00 +/- 1.79\n","Episode length: 890.60 +/- 330.29\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 891      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 772      |\n","|    ep_rew_mean        | 1.49     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 4500     |\n","|    time_elapsed       | 764      |\n","|    total_timesteps    | 360000   |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.597    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4499     |\n","|    policy_loss        | -0.00676 |\n","|    value_loss         | 0.0421   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 749      |\n","|    ep_rew_mean        | 1.37     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 4600     |\n","|    time_elapsed       | 779      |\n","|    total_timesteps    | 368000   |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.799    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4599     |\n","|    policy_loss        | 0.0371   |\n","|    value_loss         | 0.0208   |\n","------------------------------------\n","Eval num_timesteps=370000, episode_reward=0.60 +/- 0.80\n","Episode length: 637.60 +/- 157.81\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 638      |\n","|    mean_reward        | 0.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 727      |\n","|    ep_rew_mean        | 1.28     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 4700     |\n","|    time_elapsed       | 796      |\n","|    total_timesteps    | 376000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.653    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4699     |\n","|    policy_loss        | -0.00735 |\n","|    value_loss         | 0.00537  |\n","------------------------------------\n","Eval num_timesteps=380000, episode_reward=1.40 +/- 0.80\n","Episode length: 713.20 +/- 110.39\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 713      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 723      |\n","|    ep_rew_mean        | 1.26     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 4800     |\n","|    time_elapsed       | 813      |\n","|    total_timesteps    | 384000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.517    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4799     |\n","|    policy_loss        | -0.0227  |\n","|    value_loss         | 0.0192   |\n","------------------------------------\n","Eval num_timesteps=390000, episode_reward=0.20 +/- 0.40\n","Episode length: 548.00 +/- 52.68\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 548      |\n","|    mean_reward        | 0.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 757      |\n","|    ep_rew_mean        | 1.45     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 4900     |\n","|    time_elapsed       | 829      |\n","|    total_timesteps    | 392000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.715    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4899     |\n","|    policy_loss        | 0.0209   |\n","|    value_loss         | 0.00754  |\n","------------------------------------\n","Eval num_timesteps=400000, episode_reward=3.40 +/- 2.94\n","Episode length: 1120.40 +/- 514.09\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.12e+03 |\n","|    mean_reward        | 3.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 771      |\n","|    ep_rew_mean        | 1.55     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 5000     |\n","|    time_elapsed       | 848      |\n","|    total_timesteps    | 400000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.82     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4999     |\n","|    policy_loss        | -0.0577  |\n","|    value_loss         | 0.024    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 773      |\n","|    ep_rew_mean        | 1.63     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 5100     |\n","|    time_elapsed       | 863      |\n","|    total_timesteps    | 408000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.808    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5099     |\n","|    policy_loss        | -0.0498  |\n","|    value_loss         | 0.0313   |\n","------------------------------------\n","Eval num_timesteps=410000, episode_reward=1.80 +/- 1.17\n","Episode length: 825.40 +/- 180.67\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 825      |\n","|    mean_reward        | 1.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 764      |\n","|    ep_rew_mean        | 1.52     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 5200     |\n","|    time_elapsed       | 880      |\n","|    total_timesteps    | 416000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.866    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5199     |\n","|    policy_loss        | -0.0192  |\n","|    value_loss         | 0.0111   |\n","------------------------------------\n","Eval num_timesteps=420000, episode_reward=2.20 +/- 2.04\n","Episode length: 871.00 +/- 344.29\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 871      |\n","|    mean_reward        | 2.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 780      |\n","|    ep_rew_mean        | 1.62     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 5300     |\n","|    time_elapsed       | 898      |\n","|    total_timesteps    | 424000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.523    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5299     |\n","|    policy_loss        | -0.0129  |\n","|    value_loss         | 0.00885  |\n","------------------------------------\n","Eval num_timesteps=430000, episode_reward=4.20 +/- 5.23\n","Episode length: 1069.00 +/- 565.48\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.07e+03 |\n","|    mean_reward        | 4.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 762      |\n","|    ep_rew_mean        | 1.54     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 5400     |\n","|    time_elapsed       | 916      |\n","|    total_timesteps    | 432000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.752    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5399     |\n","|    policy_loss        | 0.0142   |\n","|    value_loss         | 0.0119   |\n","------------------------------------\n","Eval num_timesteps=440000, episode_reward=1.40 +/- 1.20\n","Episode length: 767.00 +/- 220.12\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 767      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 836      |\n","|    ep_rew_mean        | 2.01     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 5500     |\n","|    time_elapsed       | 933      |\n","|    total_timesteps    | 440000   |\n","| train/                |          |\n","|    entropy_loss       | -1.37    |\n","|    explained_variance | 0.883    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5499     |\n","|    policy_loss        | 0.0029   |\n","|    value_loss         | 0.00751  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 786      |\n","|    ep_rew_mean        | 1.69     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 5600     |\n","|    time_elapsed       | 948      |\n","|    total_timesteps    | 448000   |\n","| train/                |          |\n","|    entropy_loss       | -1.37    |\n","|    explained_variance | 0.784    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5599     |\n","|    policy_loss        | -0.0278  |\n","|    value_loss         | 0.0112   |\n","------------------------------------\n","Eval num_timesteps=450000, episode_reward=4.40 +/- 3.38\n","Episode length: 1181.00 +/- 452.65\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.18e+03 |\n","|    mean_reward        | 4.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 805      |\n","|    ep_rew_mean        | 1.92     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 5700     |\n","|    time_elapsed       | 967      |\n","|    total_timesteps    | 456000   |\n","| train/                |          |\n","|    entropy_loss       | -1.37    |\n","|    explained_variance | 0.895    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5699     |\n","|    policy_loss        | 0.0361   |\n","|    value_loss         | 0.0117   |\n","------------------------------------\n","Eval num_timesteps=460000, episode_reward=2.00 +/- 1.90\n","Episode length: 879.80 +/- 346.56\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 880      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 821      |\n","|    ep_rew_mean        | 2.13     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 5800     |\n","|    time_elapsed       | 984      |\n","|    total_timesteps    | 464000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.709    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5799     |\n","|    policy_loss        | 0.0743   |\n","|    value_loss         | 0.056    |\n","------------------------------------\n","Eval num_timesteps=470000, episode_reward=0.20 +/- 0.40\n","Episode length: 558.20 +/- 71.49\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 558      |\n","|    mean_reward        | 0.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 827      |\n","|    ep_rew_mean        | 2.1      |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 5900     |\n","|    time_elapsed       | 1001     |\n","|    total_timesteps    | 472000   |\n","| train/                |          |\n","|    entropy_loss       | -1.37    |\n","|    explained_variance | 0.822    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5899     |\n","|    policy_loss        | 0.0169   |\n","|    value_loss         | 0.00328  |\n","------------------------------------\n","Eval num_timesteps=480000, episode_reward=1.80 +/- 1.72\n","Episode length: 849.00 +/- 311.81\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 849      |\n","|    mean_reward        | 1.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 805      |\n","|    ep_rew_mean        | 1.87     |\n","| time/                 |          |\n","|    fps                | 470      |\n","|    iterations         | 6000     |\n","|    time_elapsed       | 1019     |\n","|    total_timesteps    | 480000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.923    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5999     |\n","|    policy_loss        | -0.0133  |\n","|    value_loss         | 0.00372  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 740      |\n","|    ep_rew_mean        | 1.47     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 6100     |\n","|    time_elapsed       | 1034     |\n","|    total_timesteps    | 488000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.901    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6099     |\n","|    policy_loss        | -0.00314 |\n","|    value_loss         | 0.0349   |\n","------------------------------------\n","Eval num_timesteps=490000, episode_reward=1.20 +/- 1.60\n","Episode length: 721.60 +/- 259.75\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 722      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 735      |\n","|    ep_rew_mean        | 1.39     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 6200     |\n","|    time_elapsed       | 1051     |\n","|    total_timesteps    | 496000   |\n","| train/                |          |\n","|    entropy_loss       | -1.37    |\n","|    explained_variance | 0.869    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6199     |\n","|    policy_loss        | -0.0778  |\n","|    value_loss         | 0.0204   |\n","------------------------------------\n","Eval num_timesteps=500000, episode_reward=1.40 +/- 1.96\n","Episode length: 759.60 +/- 346.13\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 760      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 814      |\n","|    ep_rew_mean        | 1.95     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 6300     |\n","|    time_elapsed       | 1068     |\n","|    total_timesteps    | 504000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.986    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6299     |\n","|    policy_loss        | 0.029    |\n","|    value_loss         | 0.00379  |\n","------------------------------------\n","Eval num_timesteps=510000, episode_reward=1.40 +/- 1.85\n","Episode length: 777.20 +/- 336.88\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 777      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 817      |\n","|    ep_rew_mean        | 1.96     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 6400     |\n","|    time_elapsed       | 1085     |\n","|    total_timesteps    | 512000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.867    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6399     |\n","|    policy_loss        | 0.00479  |\n","|    value_loss         | 0.0134   |\n","------------------------------------\n","Eval num_timesteps=520000, episode_reward=2.20 +/- 1.94\n","Episode length: 903.60 +/- 322.43\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 904      |\n","|    mean_reward        | 2.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 822      |\n","|    ep_rew_mean        | 1.88     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 6500     |\n","|    time_elapsed       | 1103     |\n","|    total_timesteps    | 520000   |\n","| train/                |          |\n","|    entropy_loss       | -1.37    |\n","|    explained_variance | 0.96     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6499     |\n","|    policy_loss        | -0.02    |\n","|    value_loss         | 0.00503  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 770      |\n","|    ep_rew_mean        | 1.57     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 6600     |\n","|    time_elapsed       | 1118     |\n","|    total_timesteps    | 528000   |\n","| train/                |          |\n","|    entropy_loss       | -1.37    |\n","|    explained_variance | 0.899    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6599     |\n","|    policy_loss        | 0.0407   |\n","|    value_loss         | 0.0245   |\n","------------------------------------\n","Eval num_timesteps=530000, episode_reward=1.60 +/- 1.85\n","Episode length: 802.60 +/- 327.44\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 803      |\n","|    mean_reward        | 1.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 761      |\n","|    ep_rew_mean        | 1.51     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 6700     |\n","|    time_elapsed       | 1136     |\n","|    total_timesteps    | 536000   |\n","| train/                |          |\n","|    entropy_loss       | -1.37    |\n","|    explained_variance | 0.834    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6699     |\n","|    policy_loss        | 0.0568   |\n","|    value_loss         | 0.0511   |\n","------------------------------------\n","Eval num_timesteps=540000, episode_reward=4.00 +/- 3.52\n","Episode length: 1032.60 +/- 448.49\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.03e+03 |\n","|    mean_reward        | 4        |\n","| rollout/              |          |\n","|    ep_len_mean        | 816      |\n","|    ep_rew_mean        | 1.92     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 6800     |\n","|    time_elapsed       | 1154     |\n","|    total_timesteps    | 544000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.971    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6799     |\n","|    policy_loss        | 0.00211  |\n","|    value_loss         | 0.00604  |\n","------------------------------------\n","Eval num_timesteps=550000, episode_reward=1.00 +/- 1.55\n","Episode length: 695.00 +/- 263.68\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 695      |\n","|    mean_reward        | 1        |\n","| rollout/              |          |\n","|    ep_len_mean        | 812      |\n","|    ep_rew_mean        | 1.83     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 6900     |\n","|    time_elapsed       | 1171     |\n","|    total_timesteps    | 552000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.862    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6899     |\n","|    policy_loss        | 0.0528   |\n","|    value_loss         | 0.0199   |\n","------------------------------------\n","Eval num_timesteps=560000, episode_reward=1.40 +/- 1.74\n","Episode length: 751.20 +/- 293.61\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 751      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 782      |\n","|    ep_rew_mean        | 1.61     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 7000     |\n","|    time_elapsed       | 1188     |\n","|    total_timesteps    | 560000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.893    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6999     |\n","|    policy_loss        | 0.0649   |\n","|    value_loss         | 0.0133   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 758      |\n","|    ep_rew_mean        | 1.5      |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 7100     |\n","|    time_elapsed       | 1203     |\n","|    total_timesteps    | 568000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.928    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7099     |\n","|    policy_loss        | -0.0398  |\n","|    value_loss         | 0.0192   |\n","------------------------------------\n","Eval num_timesteps=570000, episode_reward=1.60 +/- 1.85\n","Episode length: 796.40 +/- 325.16\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 796      |\n","|    mean_reward        | 1.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 771      |\n","|    ep_rew_mean        | 1.67     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 7200     |\n","|    time_elapsed       | 1220     |\n","|    total_timesteps    | 576000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.893    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7199     |\n","|    policy_loss        | -0.0156  |\n","|    value_loss         | 0.0148   |\n","------------------------------------\n","Eval num_timesteps=580000, episode_reward=1.20 +/- 1.60\n","Episode length: 710.60 +/- 280.16\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 711      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 905      |\n","|    ep_rew_mean        | 2.66     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 7300     |\n","|    time_elapsed       | 1237     |\n","|    total_timesteps    | 584000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.925    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7299     |\n","|    policy_loss        | -0.0356  |\n","|    value_loss         | 0.0216   |\n","------------------------------------\n","Eval num_timesteps=590000, episode_reward=1.20 +/- 0.98\n","Episode length: 698.60 +/- 147.80\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 699      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 928      |\n","|    ep_rew_mean        | 2.76     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 7400     |\n","|    time_elapsed       | 1254     |\n","|    total_timesteps    | 592000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.94     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7399     |\n","|    policy_loss        | -0.00538 |\n","|    value_loss         | 0.014    |\n","------------------------------------\n","Eval num_timesteps=600000, episode_reward=1.00 +/- 2.00\n","Episode length: 696.40 +/- 350.92\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 696      |\n","|    mean_reward        | 1        |\n","| rollout/              |          |\n","|    ep_len_mean        | 868      |\n","|    ep_rew_mean        | 2.23     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 7500     |\n","|    time_elapsed       | 1271     |\n","|    total_timesteps    | 600000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.968    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7499     |\n","|    policy_loss        | 0.018    |\n","|    value_loss         | 0.0053   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 808      |\n","|    ep_rew_mean        | 1.86     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 7600     |\n","|    time_elapsed       | 1286     |\n","|    total_timesteps    | 608000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.859    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7599     |\n","|    policy_loss        | 0.0244   |\n","|    value_loss         | 0.0131   |\n","------------------------------------\n","Eval num_timesteps=610000, episode_reward=1.60 +/- 1.02\n","Episode length: 770.60 +/- 156.01\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 771      |\n","|    mean_reward        | 1.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 780      |\n","|    ep_rew_mean        | 1.68     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 7700     |\n","|    time_elapsed       | 1303     |\n","|    total_timesteps    | 616000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.966    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7699     |\n","|    policy_loss        | 0.0233   |\n","|    value_loss         | 0.0101   |\n","------------------------------------\n","Eval num_timesteps=620000, episode_reward=1.80 +/- 3.60\n","Episode length: 748.20 +/- 443.92\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 748      |\n","|    mean_reward        | 1.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 851      |\n","|    ep_rew_mean        | 2.24     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 7800     |\n","|    time_elapsed       | 1320     |\n","|    total_timesteps    | 624000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.886    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7799     |\n","|    policy_loss        | 0.0952   |\n","|    value_loss         | 0.026    |\n","------------------------------------\n","Eval num_timesteps=630000, episode_reward=1.20 +/- 1.60\n","Episode length: 721.20 +/- 273.35\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 721      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 881      |\n","|    ep_rew_mean        | 2.51     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 7900     |\n","|    time_elapsed       | 1337     |\n","|    total_timesteps    | 632000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.944    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7899     |\n","|    policy_loss        | 0.0579   |\n","|    value_loss         | 0.0109   |\n","------------------------------------\n","Eval num_timesteps=640000, episode_reward=3.20 +/- 3.43\n","Episode length: 965.00 +/- 432.56\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 965      |\n","|    mean_reward        | 3.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 860      |\n","|    ep_rew_mean        | 2.26     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 8000     |\n","|    time_elapsed       | 1355     |\n","|    total_timesteps    | 640000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.933    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7999     |\n","|    policy_loss        | -0.0276  |\n","|    value_loss         | 0.0183   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 836      |\n","|    ep_rew_mean        | 2.09     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 8100     |\n","|    time_elapsed       | 1370     |\n","|    total_timesteps    | 648000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.943    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8099     |\n","|    policy_loss        | 0.0283   |\n","|    value_loss         | 0.00739  |\n","------------------------------------\n","Eval num_timesteps=650000, episode_reward=2.00 +/- 1.67\n","Episode length: 872.80 +/- 291.22\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 873      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 843      |\n","|    ep_rew_mean        | 2.12     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 8200     |\n","|    time_elapsed       | 1387     |\n","|    total_timesteps    | 656000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.88     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8199     |\n","|    policy_loss        | 0.0273   |\n","|    value_loss         | 0.00524  |\n","------------------------------------\n","Eval num_timesteps=660000, episode_reward=1.60 +/- 1.50\n","Episode length: 767.80 +/- 259.85\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 768      |\n","|    mean_reward        | 1.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 817      |\n","|    ep_rew_mean        | 1.95     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 8300     |\n","|    time_elapsed       | 1405     |\n","|    total_timesteps    | 664000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.899    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8299     |\n","|    policy_loss        | -0.0302  |\n","|    value_loss         | 0.00925  |\n","------------------------------------\n","Eval num_timesteps=670000, episode_reward=1.40 +/- 1.50\n","Episode length: 753.60 +/- 254.69\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 754      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 759      |\n","|    ep_rew_mean        | 1.47     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 8400     |\n","|    time_elapsed       | 1422     |\n","|    total_timesteps    | 672000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.962    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8399     |\n","|    policy_loss        | -0.00812 |\n","|    value_loss         | 0.00933  |\n","------------------------------------\n","Eval num_timesteps=680000, episode_reward=1.20 +/- 1.94\n","Episode length: 742.20 +/- 342.17\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 742      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 838      |\n","|    ep_rew_mean        | 2.02     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 8500     |\n","|    time_elapsed       | 1439     |\n","|    total_timesteps    | 680000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.929    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8499     |\n","|    policy_loss        | -0.00197 |\n","|    value_loss         | 0.0132   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 800      |\n","|    ep_rew_mean        | 1.8      |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 8600     |\n","|    time_elapsed       | 1454     |\n","|    total_timesteps    | 688000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.988    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8599     |\n","|    policy_loss        | -0.0209  |\n","|    value_loss         | 0.00511  |\n","------------------------------------\n","Eval num_timesteps=690000, episode_reward=0.40 +/- 0.80\n","Episode length: 584.00 +/- 118.36\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 584      |\n","|    mean_reward        | 0.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 792      |\n","|    ep_rew_mean        | 1.71     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 8700     |\n","|    time_elapsed       | 1470     |\n","|    total_timesteps    | 696000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.974    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8699     |\n","|    policy_loss        | 0.00141  |\n","|    value_loss         | 0.00424  |\n","------------------------------------\n","Eval num_timesteps=700000, episode_reward=1.20 +/- 1.94\n","Episode length: 720.20 +/- 349.23\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 720      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 790      |\n","|    ep_rew_mean        | 1.77     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 8800     |\n","|    time_elapsed       | 1487     |\n","|    total_timesteps    | 704000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.942    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8799     |\n","|    policy_loss        | -0.0178  |\n","|    value_loss         | 0.00772  |\n","------------------------------------\n","Eval num_timesteps=710000, episode_reward=2.20 +/- 1.33\n","Episode length: 889.40 +/- 233.30\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 889      |\n","|    mean_reward        | 2.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 795      |\n","|    ep_rew_mean        | 1.8      |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 8900     |\n","|    time_elapsed       | 1505     |\n","|    total_timesteps    | 712000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.98     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8899     |\n","|    policy_loss        | -0.00277 |\n","|    value_loss         | 0.00461  |\n","------------------------------------\n","Eval num_timesteps=720000, episode_reward=3.80 +/- 3.71\n","Episode length: 1095.80 +/- 503.01\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.1e+03  |\n","|    mean_reward        | 3.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 828      |\n","|    ep_rew_mean        | 1.96     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 9000     |\n","|    time_elapsed       | 1523     |\n","|    total_timesteps    | 720000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.974    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8999     |\n","|    policy_loss        | 0.00722  |\n","|    value_loss         | 0.00593  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 817      |\n","|    ep_rew_mean        | 1.87     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 9100     |\n","|    time_elapsed       | 1538     |\n","|    total_timesteps    | 728000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.87     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9099     |\n","|    policy_loss        | -0.038   |\n","|    value_loss         | 0.00586  |\n","------------------------------------\n","Eval num_timesteps=730000, episode_reward=1.40 +/- 1.96\n","Episode length: 758.80 +/- 344.02\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 759      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 807      |\n","|    ep_rew_mean        | 1.79     |\n","| time/                 |          |\n","|    fps                | 473      |\n","|    iterations         | 9200     |\n","|    time_elapsed       | 1555     |\n","|    total_timesteps    | 736000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.93     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9199     |\n","|    policy_loss        | 0.0302   |\n","|    value_loss         | 0.0151   |\n","------------------------------------\n","Eval num_timesteps=740000, episode_reward=2.80 +/- 4.66\n","Episode length: 824.00 +/- 470.13\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 824      |\n","|    mean_reward        | 2.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 809      |\n","|    ep_rew_mean        | 1.77     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 9300     |\n","|    time_elapsed       | 1573     |\n","|    total_timesteps    | 744000   |\n","| train/                |          |\n","|    entropy_loss       | -1.33    |\n","|    explained_variance | 0.969    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9299     |\n","|    policy_loss        | 0.0215   |\n","|    value_loss         | 0.00817  |\n","------------------------------------\n","Eval num_timesteps=750000, episode_reward=3.40 +/- 3.77\n","Episode length: 994.40 +/- 494.24\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 994      |\n","|    mean_reward        | 3.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 851      |\n","|    ep_rew_mean        | 2.12     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 9400     |\n","|    time_elapsed       | 1591     |\n","|    total_timesteps    | 752000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.954    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9399     |\n","|    policy_loss        | -0.0221  |\n","|    value_loss         | 0.00684  |\n","------------------------------------\n","Eval num_timesteps=760000, episode_reward=2.00 +/- 2.19\n","Episode length: 840.00 +/- 375.55\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 840      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 856      |\n","|    ep_rew_mean        | 2.17     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 9500     |\n","|    time_elapsed       | 1609     |\n","|    total_timesteps    | 760000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.968    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9499     |\n","|    policy_loss        | -0.0172  |\n","|    value_loss         | 0.00715  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 803      |\n","|    ep_rew_mean        | 1.89     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 9600     |\n","|    time_elapsed       | 1625     |\n","|    total_timesteps    | 768000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.956    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9599     |\n","|    policy_loss        | 0.0133   |\n","|    value_loss         | 0.00632  |\n","------------------------------------\n","Eval num_timesteps=770000, episode_reward=4.20 +/- 3.82\n","Episode length: 1146.00 +/- 521.41\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.15e+03 |\n","|    mean_reward        | 4.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 794      |\n","|    ep_rew_mean        | 1.82     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 9700     |\n","|    time_elapsed       | 1643     |\n","|    total_timesteps    | 776000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.909    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9699     |\n","|    policy_loss        | 0.0247   |\n","|    value_loss         | 0.00909  |\n","------------------------------------\n","Eval num_timesteps=780000, episode_reward=2.00 +/- 2.19\n","Episode length: 840.60 +/- 374.22\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 841      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 793      |\n","|    ep_rew_mean        | 1.74     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 9800     |\n","|    time_elapsed       | 1661     |\n","|    total_timesteps    | 784000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.926    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9799     |\n","|    policy_loss        | 0.0463   |\n","|    value_loss         | 0.024    |\n","------------------------------------\n","Eval num_timesteps=790000, episode_reward=1.60 +/- 1.36\n","Episode length: 760.60 +/- 198.97\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 761      |\n","|    mean_reward        | 1.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 851      |\n","|    ep_rew_mean        | 2.11     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 9900     |\n","|    time_elapsed       | 1678     |\n","|    total_timesteps    | 792000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.979    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9899     |\n","|    policy_loss        | -0.045   |\n","|    value_loss         | 0.00575  |\n","------------------------------------\n","Eval num_timesteps=800000, episode_reward=0.80 +/- 1.17\n","Episode length: 650.40 +/- 188.48\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 650      |\n","|    mean_reward        | 0.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 853      |\n","|    ep_rew_mean        | 2.22     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 10000    |\n","|    time_elapsed       | 1696     |\n","|    total_timesteps    | 800000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.988    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9999     |\n","|    policy_loss        | -0.0612  |\n","|    value_loss         | 0.00684  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 869      |\n","|    ep_rew_mean        | 2.31     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 10100    |\n","|    time_elapsed       | 1710     |\n","|    total_timesteps    | 808000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.982    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10099    |\n","|    policy_loss        | -0.0356  |\n","|    value_loss         | 0.0042   |\n","------------------------------------\n","Eval num_timesteps=810000, episode_reward=3.20 +/- 3.71\n","Episode length: 984.20 +/- 492.82\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 984      |\n","|    mean_reward        | 3.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 890      |\n","|    ep_rew_mean        | 2.37     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 10200    |\n","|    time_elapsed       | 1728     |\n","|    total_timesteps    | 816000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.974    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10199    |\n","|    policy_loss        | 0.00321  |\n","|    value_loss         | 0.00711  |\n","------------------------------------\n","Eval num_timesteps=820000, episode_reward=1.00 +/- 1.26\n","Episode length: 676.00 +/- 195.72\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 676      |\n","|    mean_reward        | 1        |\n","| rollout/              |          |\n","|    ep_len_mean        | 845      |\n","|    ep_rew_mean        | 2.05     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 10300    |\n","|    time_elapsed       | 1745     |\n","|    total_timesteps    | 824000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.984    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10299    |\n","|    policy_loss        | -0.00576 |\n","|    value_loss         | 0.00485  |\n","------------------------------------\n","Eval num_timesteps=830000, episode_reward=2.00 +/- 3.10\n","Episode length: 781.20 +/- 352.88\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 781      |\n","|    mean_reward        | 2        |\n","| rollout/              |          |\n","|    ep_len_mean        | 839      |\n","|    ep_rew_mean        | 2.09     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 10400    |\n","|    time_elapsed       | 1763     |\n","|    total_timesteps    | 832000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.983    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10399    |\n","|    policy_loss        | -0.0207  |\n","|    value_loss         | 0.00547  |\n","------------------------------------\n","Eval num_timesteps=840000, episode_reward=0.60 +/- 0.80\n","Episode length: 620.80 +/- 128.78\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 621      |\n","|    mean_reward        | 0.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 867      |\n","|    ep_rew_mean        | 2.35     |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 10500    |\n","|    time_elapsed       | 1779     |\n","|    total_timesteps    | 840000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.965    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10499    |\n","|    policy_loss        | 0.00614  |\n","|    value_loss         | 0.0122   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 826      |\n","|    ep_rew_mean        | 2.04     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 10600    |\n","|    time_elapsed       | 1794     |\n","|    total_timesteps    | 848000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.98     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10599    |\n","|    policy_loss        | -0.00924 |\n","|    value_loss         | 0.00538  |\n","------------------------------------\n","Eval num_timesteps=850000, episode_reward=3.00 +/- 2.45\n","Episode length: 1059.20 +/- 434.99\n","-------------------------------------\n","| eval/                 |           |\n","|    mean_ep_length     | 1.06e+03  |\n","|    mean_reward        | 3         |\n","| rollout/              |           |\n","|    ep_len_mean        | 829       |\n","|    ep_rew_mean        | 2.11      |\n","| time/                 |           |\n","|    fps                | 472       |\n","|    iterations         | 10700     |\n","|    time_elapsed       | 1812      |\n","|    total_timesteps    | 856000    |\n","| train/                |           |\n","|    entropy_loss       | -1.34     |\n","|    explained_variance | 0.991     |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 10699     |\n","|    policy_loss        | -0.000837 |\n","|    value_loss         | 0.0018    |\n","-------------------------------------\n","Eval num_timesteps=860000, episode_reward=1.00 +/- 1.26\n","Episode length: 675.20 +/- 212.37\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 675      |\n","|    mean_reward        | 1        |\n","| rollout/              |          |\n","|    ep_len_mean        | 838      |\n","|    ep_rew_mean        | 2.17     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 10800    |\n","|    time_elapsed       | 1829     |\n","|    total_timesteps    | 864000   |\n","| train/                |          |\n","|    entropy_loss       | -1.33    |\n","|    explained_variance | 0.975    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10799    |\n","|    policy_loss        | -0.0193  |\n","|    value_loss         | 0.00468  |\n","------------------------------------\n","Eval num_timesteps=870000, episode_reward=1.80 +/- 1.83\n","Episode length: 813.60 +/- 321.59\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 814      |\n","|    mean_reward        | 1.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 837      |\n","|    ep_rew_mean        | 2.09     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 10900    |\n","|    time_elapsed       | 1846     |\n","|    total_timesteps    | 872000   |\n","| train/                |          |\n","|    entropy_loss       | -1.33    |\n","|    explained_variance | 0.99     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10899    |\n","|    policy_loss        | -0.021   |\n","|    value_loss         | 0.00391  |\n","------------------------------------\n","Eval num_timesteps=880000, episode_reward=1.40 +/- 1.96\n","Episode length: 761.00 +/- 349.88\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 761      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 864      |\n","|    ep_rew_mean        | 2.22     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 11000    |\n","|    time_elapsed       | 1863     |\n","|    total_timesteps    | 880000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.987    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10999    |\n","|    policy_loss        | -0.0035  |\n","|    value_loss         | 0.00379  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 888      |\n","|    ep_rew_mean        | 2.31     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 11100    |\n","|    time_elapsed       | 1878     |\n","|    total_timesteps    | 888000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.993    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11099    |\n","|    policy_loss        | 0.0003   |\n","|    value_loss         | 0.00173  |\n","------------------------------------\n","Eval num_timesteps=890000, episode_reward=0.80 +/- 1.60\n","Episode length: 652.40 +/- 275.95\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 652      |\n","|    mean_reward        | 0.8      |\n","| rollout/              |          |\n","|    ep_len_mean        | 943      |\n","|    ep_rew_mean        | 2.77     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 11200    |\n","|    time_elapsed       | 1895     |\n","|    total_timesteps    | 896000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.993    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11199    |\n","|    policy_loss        | -0.0036  |\n","|    value_loss         | 0.00236  |\n","------------------------------------\n","Eval num_timesteps=900000, episode_reward=3.60 +/- 3.38\n","Episode length: 1059.80 +/- 446.65\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.06e+03 |\n","|    mean_reward        | 3.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 935      |\n","|    ep_rew_mean        | 2.79     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 11300    |\n","|    time_elapsed       | 1913     |\n","|    total_timesteps    | 904000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.99     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11299    |\n","|    policy_loss        | -0.0186  |\n","|    value_loss         | 0.00256  |\n","------------------------------------\n","Eval num_timesteps=910000, episode_reward=0.40 +/- 0.80\n","Episode length: 586.40 +/- 121.66\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 586      |\n","|    mean_reward        | 0.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 865      |\n","|    ep_rew_mean        | 2.32     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 11400    |\n","|    time_elapsed       | 1930     |\n","|    total_timesteps    | 912000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.989    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11399    |\n","|    policy_loss        | 0.0491   |\n","|    value_loss         | 0.00401  |\n","------------------------------------\n","Eval num_timesteps=920000, episode_reward=1.60 +/- 2.33\n","Episode length: 786.40 +/- 414.06\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 786      |\n","|    mean_reward        | 1.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 805      |\n","|    ep_rew_mean        | 1.98     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 11500    |\n","|    time_elapsed       | 1947     |\n","|    total_timesteps    | 920000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.952    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11499    |\n","|    policy_loss        | -0.0668  |\n","|    value_loss         | 0.0163   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 821      |\n","|    ep_rew_mean        | 2        |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 11600    |\n","|    time_elapsed       | 1962     |\n","|    total_timesteps    | 928000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.995    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11599    |\n","|    policy_loss        | -0.00163 |\n","|    value_loss         | 0.00127  |\n","------------------------------------\n","Eval num_timesteps=930000, episode_reward=1.20 +/- 0.98\n","Episode length: 705.20 +/- 145.43\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 705      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 806      |\n","|    ep_rew_mean        | 1.85     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 11700    |\n","|    time_elapsed       | 1979     |\n","|    total_timesteps    | 936000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.993    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11699    |\n","|    policy_loss        | 0.0176   |\n","|    value_loss         | 0.00191  |\n","------------------------------------\n","Eval num_timesteps=940000, episode_reward=0.60 +/- 0.49\n","Episode length: 592.80 +/- 50.13\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 593      |\n","|    mean_reward        | 0.6      |\n","| rollout/              |          |\n","|    ep_len_mean        | 792      |\n","|    ep_rew_mean        | 1.85     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 11800    |\n","|    time_elapsed       | 1996     |\n","|    total_timesteps    | 944000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.978    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11799    |\n","|    policy_loss        | -0.00841 |\n","|    value_loss         | 0.003    |\n","------------------------------------\n","Eval num_timesteps=950000, episode_reward=3.00 +/- 3.69\n","Episode length: 960.00 +/- 488.88\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 960      |\n","|    mean_reward        | 3        |\n","| rollout/              |          |\n","|    ep_len_mean        | 799      |\n","|    ep_rew_mean        | 1.91     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 11900    |\n","|    time_elapsed       | 2014     |\n","|    total_timesteps    | 952000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.977    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11899    |\n","|    policy_loss        | -0.00583 |\n","|    value_loss         | 0.00607  |\n","------------------------------------\n","Eval num_timesteps=960000, episode_reward=3.20 +/- 3.43\n","Episode length: 990.40 +/- 462.40\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 990      |\n","|    mean_reward        | 3.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 836      |\n","|    ep_rew_mean        | 2.08     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 12000    |\n","|    time_elapsed       | 2032     |\n","|    total_timesteps    | 960000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.903    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11999    |\n","|    policy_loss        | 0.0354   |\n","|    value_loss         | 0.0149   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 869      |\n","|    ep_rew_mean        | 2.27     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 12100    |\n","|    time_elapsed       | 2046     |\n","|    total_timesteps    | 968000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.98     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12099    |\n","|    policy_loss        | -0.0405  |\n","|    value_loss         | 0.00382  |\n","------------------------------------\n","Eval num_timesteps=970000, episode_reward=1.40 +/- 1.96\n","Episode length: 757.00 +/- 343.55\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 757      |\n","|    mean_reward        | 1.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 833      |\n","|    ep_rew_mean        | 2.01     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 12200    |\n","|    time_elapsed       | 2064     |\n","|    total_timesteps    | 976000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.975    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12199    |\n","|    policy_loss        | 0.0204   |\n","|    value_loss         | 0.00194  |\n","------------------------------------\n","Eval num_timesteps=980000, episode_reward=1.20 +/- 0.98\n","Episode length: 690.60 +/- 144.45\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 691      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 820      |\n","|    ep_rew_mean        | 1.96     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 12300    |\n","|    time_elapsed       | 2081     |\n","|    total_timesteps    | 984000   |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.966    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12299    |\n","|    policy_loss        | 0.00755  |\n","|    value_loss         | 0.0099   |\n","------------------------------------\n","Eval num_timesteps=990000, episode_reward=1.20 +/- 1.60\n","Episode length: 732.60 +/- 301.86\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 733      |\n","|    mean_reward        | 1.2      |\n","| rollout/              |          |\n","|    ep_len_mean        | 839      |\n","|    ep_rew_mean        | 2.1      |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 12400    |\n","|    time_elapsed       | 2098     |\n","|    total_timesteps    | 992000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.996    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12399    |\n","|    policy_loss        | -0.0148  |\n","|    value_loss         | 0.000907 |\n","------------------------------------\n","Eval num_timesteps=1000000, episode_reward=2.40 +/- 3.88\n","Episode length: 825.20 +/- 472.55\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 825      |\n","|    mean_reward        | 2.4      |\n","| rollout/              |          |\n","|    ep_len_mean        | 788      |\n","|    ep_rew_mean        | 1.76     |\n","| time/                 |          |\n","|    fps                | 472      |\n","|    iterations         | 12500    |\n","|    time_elapsed       | 2115     |\n","|    total_timesteps    | 1000000  |\n","| train/                |          |\n","|    entropy_loss       | -1.34    |\n","|    explained_variance | 0.988    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12499    |\n","|    policy_loss        | 0.0199   |\n","|    value_loss         | 0.00421  |\n","------------------------------------\n","Saving to logs/a2c/BreakoutNoFrameskip-v4_1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zlnCCRe_AZLw"},"source":["#### Evaluate trained agent\n","\n","\n","You can remove the `--folder logs/` to evaluate pretrained agent."]},{"cell_type":"code","metadata":{"id":"T3_rl9B7AZLw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625076582264,"user_tz":-120,"elapsed":20569,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"c63a5c67-77bd-4886-9b2d-deaf44a9690d"},"source":["!python enjoy.py --algo a2c --env BreakoutNoFrameskip-v4 --no-render --n-timesteps 5000 --folder logs/"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Loading latest experiment, id=1\n","Loading logs/a2c/BreakoutNoFrameskip-v4_1/BreakoutNoFrameskip-v4.zip\n","Stacking 4 frames\n","Wrapping the env in a VecTransposeImage.\n","Atari Episode Score: 5.00\n","Atari Episode Length 1332\n","Atari Episode Score: 0.00\n","Atari Episode Length 528\n","Atari Episode Score: 0.00\n","Atari Episode Length 512\n","Atari Episode Score: 0.00\n","Atari Episode Length 541\n","Atari Episode Score: 0.00\n","Atari Episode Length 535\n","Atari Episode Score: 0.00\n","Atari Episode Length 537\n","Atari Episode Score: 6.00\n","Atari Episode Length 1606\n","Atari Episode Score: 2.00\n","Atari Episode Length 817\n","Atari Episode Score: 0.00\n","Atari Episode Length 513\n","Atari Episode Score: 0.00\n","Atari Episode Length 528\n","Atari Episode Score: 0.00\n","Atari Episode Length 535\n","Atari Episode Score: 0.00\n","Atari Episode Length 535\n","Atari Episode Score: 0.00\n","Atari Episode Length 543\n","Atari Episode Score: 0.00\n","Atari Episode Length 517\n","Atari Episode Score: 2.00\n","Atari Episode Length 833\n","Atari Episode Score: 0.00\n","Atari Episode Length 540\n","Atari Episode Score: 0.00\n","Atari Episode Length 509\n","Atari Episode Score: 5.00\n","Atari Episode Length 1423\n","Atari Episode Score: 0.00\n","Atari Episode Length 523\n","Atari Episode Score: 0.00\n","Atari Episode Length 532\n","Atari Episode Score: 5.00\n","Atari Episode Length 1398\n","Atari Episode Score: 0.00\n","Atari Episode Length 524\n","Atari Episode Score: 0.00\n","Atari Episode Length 524\n","Atari Episode Score: 9.00\n","Atari Episode Length 1632\n","Atari Episode Score: 0.00\n","Atari Episode Length 523\n","Atari Episode Score: 0.00\n","Atari Episode Length 532\n","Atari Episode Score: 0.00\n","Atari Episode Length 528\n","Atari Episode Score: 5.00\n","Atari Episode Length 1392\n","Atari Episode Score: 0.00\n","Atari Episode Length 537\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yqxfY9I5AZLw"},"source":["### Record  a Video"]},{"cell_type":"code","metadata":{"id":"GcEqCWUCAZLw","executionInfo":{"status":"ok","timestamp":1625076582265,"user_tz":-120,"elapsed":14,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}}},"source":["# Set up display; otherwise rendering will fail\n","import os\n","os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n","os.environ['DISPLAY'] = ':1'"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BFKW-mv6AZLw","executionInfo":{"status":"ok","timestamp":1625076594912,"user_tz":-120,"elapsed":12660,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"67c0d862-5be6-4fcc-df6c-b788622c0e56"},"source":["!python -m utils.record_video --algo a2c --env BreakoutNoFrameskip-v4 --exp-id 0 -f logs/ -n 1000"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Loading latest experiment, id=1\n","Stacking 4 frames\n","Saving video to /content/drive/My Drive/Colab Notebooks/TFM/Seaquest/rl-baselines3-zoo/logs/a2c/BreakoutNoFrameskip-v4_1/videos/final-model-a2c-BreakoutNoFrameskip-v4-step-0-to-step-1000.mp4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I-0DeRMQAnyW"},"source":["## Train an QbertNoFrameskip-v4 DQN\n","\n","Steps: 1M\n"]},{"cell_type":"code","metadata":{"id":"S8wMEZvnAnyW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625078943332,"user_tz":-120,"elapsed":2221806,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"d435ff6e-66fa-4c2b-88f0-e04035805c4c"},"source":["!python train.py --algo a2c --env QbertNoFrameskip-v4 --n-timesteps 1000000 -tb logs"],"execution_count":14,"outputs":[{"output_type":"stream","text":["========== QbertNoFrameskip-v4 ==========\n","Seed: 1728520980\n","Default hyperparameters for environment (ones being tuned will be overridden):\n","OrderedDict([('ent_coef', 0.01),\n","             ('env_wrapper',\n","              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n","             ('frame_stack', 4),\n","             ('n_envs', 16),\n","             ('n_timesteps', 10000000.0),\n","             ('policy', 'CnnPolicy'),\n","             ('policy_kwargs',\n","              'dict(optimizer_class=RMSpropTFLike, '\n","              'optimizer_kwargs=dict(eps=1e-5))'),\n","             ('vf_coef', 0.25)])\n","Using 16 environments\n","Overwriting n_timesteps with n=1000000\n","Creating test environment\n","Stacking 4 frames\n","Wrapping into a VecTransposeImage\n","Stacking 4 frames\n","Wrapping into a VecTransposeImage\n","Using cuda device\n","Log path: logs/a2c/QbertNoFrameskip-v4_1\n","2021-06-30 18:12:16.764805: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","Logging to logs/QbertNoFrameskip-v4/A2C_1\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.33e+03 |\n","|    ep_rew_mean        | 145      |\n","| time/                 |          |\n","|    fps                | 508      |\n","|    iterations         | 100      |\n","|    time_elapsed       | 15       |\n","|    total_timesteps    | 8000     |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.0574   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 99       |\n","|    policy_loss        | 0.205    |\n","|    value_loss         | 0.108    |\n","------------------------------------\n","Eval num_timesteps=10000, episode_reward=110.00 +/- 46.37\n","Episode length: 1241.60 +/- 193.53\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.24e+03 |\n","|    mean_reward        | 110      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.36e+03 |\n","|    ep_rew_mean        | 165      |\n","| time/                 |          |\n","|    fps                | 471      |\n","|    iterations         | 200      |\n","|    time_elapsed       | 33       |\n","|    total_timesteps    | 16000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | -0.0118  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 199      |\n","|    policy_loss        | 0.149    |\n","|    value_loss         | 0.102    |\n","------------------------------------\n","Eval num_timesteps=20000, episode_reward=210.00 +/- 183.44\n","Episode length: 1311.60 +/- 234.31\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.31e+03 |\n","|    mean_reward        | 210      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.34e+03 |\n","|    ep_rew_mean        | 155      |\n","| time/                 |          |\n","|    fps                | 458      |\n","|    iterations         | 300      |\n","|    time_elapsed       | 52       |\n","|    total_timesteps    | 24000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | -0.0394  |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 299      |\n","|    policy_loss        | 0.0462   |\n","|    value_loss         | 0.0724   |\n","------------------------------------\n","Eval num_timesteps=30000, episode_reward=120.00 +/- 53.39\n","Episode length: 1262.80 +/- 163.54\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.26e+03 |\n","|    mean_reward        | 120      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.35e+03 |\n","|    ep_rew_mean        | 162      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 400      |\n","|    time_elapsed       | 70       |\n","|    total_timesteps    | 32000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.0031   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 399      |\n","|    policy_loss        | -0.119   |\n","|    value_loss         | 0.0887   |\n","------------------------------------\n","Eval num_timesteps=40000, episode_reward=210.00 +/- 156.20\n","Episode length: 1428.00 +/- 122.98\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.43e+03 |\n","|    mean_reward        | 210      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.35e+03 |\n","|    ep_rew_mean        | 158      |\n","| time/                 |          |\n","|    fps                | 449      |\n","|    iterations         | 500      |\n","|    time_elapsed       | 89       |\n","|    total_timesteps    | 40000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | -0.00166 |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 499      |\n","|    policy_loss        | 0.146    |\n","|    value_loss         | 0.153    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.34e+03 |\n","|    ep_rew_mean        | 153      |\n","| time/                 |          |\n","|    fps                | 464      |\n","|    iterations         | 600      |\n","|    time_elapsed       | 103      |\n","|    total_timesteps    | 48000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.0325   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 599      |\n","|    policy_loss        | -0.0263  |\n","|    value_loss         | 0.000404 |\n","------------------------------------\n","Eval num_timesteps=50000, episode_reward=100.00 +/- 57.01\n","Episode length: 1340.80 +/- 111.59\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.34e+03 |\n","|    mean_reward        | 100      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.34e+03 |\n","|    ep_rew_mean        | 150      |\n","| time/                 |          |\n","|    fps                | 459      |\n","|    iterations         | 700      |\n","|    time_elapsed       | 121      |\n","|    total_timesteps    | 56000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.0433   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 699      |\n","|    policy_loss        | 0.0545   |\n","|    value_loss         | 0.0648   |\n","------------------------------------\n","Eval num_timesteps=60000, episode_reward=60.00 +/- 51.48\n","Episode length: 1220.20 +/- 78.54\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.22e+03 |\n","|    mean_reward        | 60       |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.33e+03 |\n","|    ep_rew_mean        | 140      |\n","| time/                 |          |\n","|    fps                | 458      |\n","|    iterations         | 800      |\n","|    time_elapsed       | 139      |\n","|    total_timesteps    | 64000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | -0.00999 |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 799      |\n","|    policy_loss        | 0.262    |\n","|    value_loss         | 0.162    |\n","------------------------------------\n","Eval num_timesteps=70000, episode_reward=205.00 +/- 214.13\n","Episode length: 1367.20 +/- 210.72\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.37e+03 |\n","|    mean_reward        | 205      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.28e+03 |\n","|    ep_rew_mean        | 122      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 900      |\n","|    time_elapsed       | 158      |\n","|    total_timesteps    | 72000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.418    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 899      |\n","|    policy_loss        | -0.0301  |\n","|    value_loss         | 0.00063  |\n","------------------------------------\n","Eval num_timesteps=80000, episode_reward=145.00 +/- 155.24\n","Episode length: 1319.20 +/- 254.25\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.32e+03 |\n","|    mean_reward        | 145      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.31e+03 |\n","|    ep_rew_mean        | 137      |\n","| time/                 |          |\n","|    fps                | 453      |\n","|    iterations         | 1000     |\n","|    time_elapsed       | 176      |\n","|    total_timesteps    | 80000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.101    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 999      |\n","|    policy_loss        | 0.128    |\n","|    value_loss         | 0.096    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.32e+03 |\n","|    ep_rew_mean        | 140      |\n","| time/                 |          |\n","|    fps                | 462      |\n","|    iterations         | 1100     |\n","|    time_elapsed       | 190      |\n","|    total_timesteps    | 88000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.112    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1099     |\n","|    policy_loss        | 0.0233   |\n","|    value_loss         | 0.0346   |\n","------------------------------------\n","Eval num_timesteps=90000, episode_reward=200.00 +/- 253.48\n","Episode length: 1364.40 +/- 249.75\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.36e+03 |\n","|    mean_reward        | 200      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.35e+03 |\n","|    ep_rew_mean        | 172      |\n","| time/                 |          |\n","|    fps                | 459      |\n","|    iterations         | 1200     |\n","|    time_elapsed       | 208      |\n","|    total_timesteps    | 96000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.0876   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1199     |\n","|    policy_loss        | -0.177   |\n","|    value_loss         | 0.0633   |\n","------------------------------------\n","Eval num_timesteps=100000, episode_reward=140.00 +/- 64.42\n","Episode length: 1371.00 +/- 135.55\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.37e+03 |\n","|    mean_reward        | 140      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.38e+03 |\n","|    ep_rew_mean        | 196      |\n","| time/                 |          |\n","|    fps                | 457      |\n","|    iterations         | 1300     |\n","|    time_elapsed       | 227      |\n","|    total_timesteps    | 104000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.398    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1299     |\n","|    policy_loss        | 0.255    |\n","|    value_loss         | 0.153    |\n","------------------------------------\n","Eval num_timesteps=110000, episode_reward=155.00 +/- 50.99\n","Episode length: 1377.80 +/- 234.27\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.38e+03 |\n","|    mean_reward        | 155      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.38e+03 |\n","|    ep_rew_mean        | 193      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 1400     |\n","|    time_elapsed       | 245      |\n","|    total_timesteps    | 112000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.115    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1399     |\n","|    policy_loss        | 0.0399   |\n","|    value_loss         | 0.0441   |\n","------------------------------------\n","Eval num_timesteps=120000, episode_reward=115.00 +/- 37.42\n","Episode length: 1280.20 +/- 101.66\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.28e+03 |\n","|    mean_reward        | 115      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.36e+03 |\n","|    ep_rew_mean        | 186      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 1500     |\n","|    time_elapsed       | 263      |\n","|    total_timesteps    | 120000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.0977   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1499     |\n","|    policy_loss        | -0.151   |\n","|    value_loss         | 0.202    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.33e+03 |\n","|    ep_rew_mean        | 146      |\n","| time/                 |          |\n","|    fps                | 460      |\n","|    iterations         | 1600     |\n","|    time_elapsed       | 277      |\n","|    total_timesteps    | 128000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | -0.00161 |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1599     |\n","|    policy_loss        | -0.195   |\n","|    value_loss         | 0.255    |\n","------------------------------------\n","Eval num_timesteps=130000, episode_reward=60.00 +/- 64.42\n","Episode length: 1167.60 +/- 40.24\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.17e+03 |\n","|    mean_reward        | 60       |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.33e+03 |\n","|    ep_rew_mean        | 142      |\n","| time/                 |          |\n","|    fps                | 459      |\n","|    iterations         | 1700     |\n","|    time_elapsed       | 295      |\n","|    total_timesteps    | 136000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.235    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1699     |\n","|    policy_loss        | 0.163    |\n","|    value_loss         | 0.0855   |\n","------------------------------------\n","Eval num_timesteps=140000, episode_reward=115.00 +/- 40.62\n","Episode length: 1284.40 +/- 141.02\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.28e+03 |\n","|    mean_reward        | 115      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.29e+03 |\n","|    ep_rew_mean        | 122      |\n","| time/                 |          |\n","|    fps                | 458      |\n","|    iterations         | 1800     |\n","|    time_elapsed       | 313      |\n","|    total_timesteps    | 144000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | -0.139   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1799     |\n","|    policy_loss        | -0.0856  |\n","|    value_loss         | 0.0369   |\n","------------------------------------\n","Eval num_timesteps=150000, episode_reward=100.00 +/- 61.24\n","Episode length: 1338.60 +/- 204.46\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.34e+03 |\n","|    mean_reward        | 100      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.32e+03 |\n","|    ep_rew_mean        | 140      |\n","| time/                 |          |\n","|    fps                | 457      |\n","|    iterations         | 1900     |\n","|    time_elapsed       | 332      |\n","|    total_timesteps    | 152000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.403    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1899     |\n","|    policy_loss        | -0.122   |\n","|    value_loss         | 0.0348   |\n","------------------------------------\n","Eval num_timesteps=160000, episode_reward=120.00 +/- 81.24\n","Episode length: 1331.80 +/- 150.25\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.33e+03 |\n","|    mean_reward        | 120      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.33e+03 |\n","|    ep_rew_mean        | 158      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 2000     |\n","|    time_elapsed       | 350      |\n","|    total_timesteps    | 160000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | -0.061   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1999     |\n","|    policy_loss        | -0.0385  |\n","|    value_loss         | 0.0719   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.36e+03 |\n","|    ep_rew_mean        | 174      |\n","| time/                 |          |\n","|    fps                | 460      |\n","|    iterations         | 2100     |\n","|    time_elapsed       | 364      |\n","|    total_timesteps    | 168000   |\n","| train/                |          |\n","|    entropy_loss       | -1.74    |\n","|    explained_variance | 0.237    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2099     |\n","|    policy_loss        | -0.0742  |\n","|    value_loss         | 0.569    |\n","------------------------------------\n","Eval num_timesteps=170000, episode_reward=315.00 +/- 238.54\n","Episode length: 1370.60 +/- 225.92\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.37e+03 |\n","|    mean_reward        | 315      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.38e+03 |\n","|    ep_rew_mean        | 189      |\n","| time/                 |          |\n","|    fps                | 459      |\n","|    iterations         | 2200     |\n","|    time_elapsed       | 383      |\n","|    total_timesteps    | 176000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.174    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2199     |\n","|    policy_loss        | -0.349   |\n","|    value_loss         | 0.286    |\n","------------------------------------\n","Eval num_timesteps=180000, episode_reward=70.00 +/- 81.24\n","Episode length: 1195.80 +/- 107.79\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.2e+03  |\n","|    mean_reward        | 70       |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.39e+03 |\n","|    ep_rew_mean        | 193      |\n","| time/                 |          |\n","|    fps                | 458      |\n","|    iterations         | 2300     |\n","|    time_elapsed       | 400      |\n","|    total_timesteps    | 184000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.641    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2299     |\n","|    policy_loss        | 0.083    |\n","|    value_loss         | 0.0515   |\n","------------------------------------\n","Eval num_timesteps=190000, episode_reward=265.00 +/- 163.25\n","Episode length: 1456.20 +/- 197.48\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.46e+03 |\n","|    mean_reward        | 265      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.39e+03 |\n","|    ep_rew_mean        | 203      |\n","| time/                 |          |\n","|    fps                | 457      |\n","|    iterations         | 2400     |\n","|    time_elapsed       | 419      |\n","|    total_timesteps    | 192000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.69     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2399     |\n","|    policy_loss        | 0.202    |\n","|    value_loss         | 0.11     |\n","------------------------------------\n","Eval num_timesteps=200000, episode_reward=235.00 +/- 175.07\n","Episode length: 1341.80 +/- 143.63\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.34e+03 |\n","|    mean_reward        | 235      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.39e+03 |\n","|    ep_rew_mean        | 200      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 2500     |\n","|    time_elapsed       | 437      |\n","|    total_timesteps    | 200000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.72     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2499     |\n","|    policy_loss        | -0.0367  |\n","|    value_loss         | 0.108    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.38e+03 |\n","|    ep_rew_mean        | 194      |\n","| time/                 |          |\n","|    fps                | 460      |\n","|    iterations         | 2600     |\n","|    time_elapsed       | 451      |\n","|    total_timesteps    | 208000   |\n","| train/                |          |\n","|    entropy_loss       | -1.77    |\n","|    explained_variance | 0.626    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2599     |\n","|    policy_loss        | 0.167    |\n","|    value_loss         | 0.104    |\n","------------------------------------\n","Eval num_timesteps=210000, episode_reward=265.00 +/- 128.06\n","Episode length: 1464.20 +/- 121.66\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.46e+03 |\n","|    mean_reward        | 265      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.39e+03 |\n","|    ep_rew_mean        | 201      |\n","| time/                 |          |\n","|    fps                | 458      |\n","|    iterations         | 2700     |\n","|    time_elapsed       | 470      |\n","|    total_timesteps    | 216000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.605    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2699     |\n","|    policy_loss        | 0.0793   |\n","|    value_loss         | 0.0518   |\n","------------------------------------\n","Eval num_timesteps=220000, episode_reward=205.00 +/- 87.18\n","Episode length: 1452.00 +/- 248.27\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.45e+03 |\n","|    mean_reward        | 205      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.39e+03 |\n","|    ep_rew_mean        | 194      |\n","| time/                 |          |\n","|    fps                | 457      |\n","|    iterations         | 2800     |\n","|    time_elapsed       | 489      |\n","|    total_timesteps    | 224000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.688    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2799     |\n","|    policy_loss        | -0.0949  |\n","|    value_loss         | 0.0653   |\n","------------------------------------\n","Eval num_timesteps=230000, episode_reward=125.00 +/- 126.49\n","Episode length: 1304.20 +/- 224.74\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.3e+03  |\n","|    mean_reward        | 125      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.38e+03 |\n","|    ep_rew_mean        | 189      |\n","| time/                 |          |\n","|    fps                | 457      |\n","|    iterations         | 2900     |\n","|    time_elapsed       | 507      |\n","|    total_timesteps    | 232000   |\n","| train/                |          |\n","|    entropy_loss       | -1.77    |\n","|    explained_variance | 0.778    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2899     |\n","|    policy_loss        | 0.127    |\n","|    value_loss         | 0.183    |\n","------------------------------------\n","Eval num_timesteps=240000, episode_reward=255.00 +/- 238.96\n","Episode length: 1532.00 +/- 297.37\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.53e+03 |\n","|    mean_reward        | 255      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.38e+03 |\n","|    ep_rew_mean        | 192      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 3000     |\n","|    time_elapsed       | 526      |\n","|    total_timesteps    | 240000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.854    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2999     |\n","|    policy_loss        | 0.000194 |\n","|    value_loss         | 0.0247   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.4e+03  |\n","|    ep_rew_mean        | 206      |\n","| time/                 |          |\n","|    fps                | 458      |\n","|    iterations         | 3100     |\n","|    time_elapsed       | 540      |\n","|    total_timesteps    | 248000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.481    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3099     |\n","|    policy_loss        | 0.159    |\n","|    value_loss         | 0.0965   |\n","------------------------------------\n","Eval num_timesteps=250000, episode_reward=330.00 +/- 171.32\n","Episode length: 1592.00 +/- 218.14\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.59e+03 |\n","|    mean_reward        | 330      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.38e+03 |\n","|    ep_rew_mean        | 191      |\n","| time/                 |          |\n","|    fps                | 457      |\n","|    iterations         | 3200     |\n","|    time_elapsed       | 559      |\n","|    total_timesteps    | 256000   |\n","| train/                |          |\n","|    entropy_loss       | -1.77    |\n","|    explained_variance | 0.786    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3199     |\n","|    policy_loss        | 0.0776   |\n","|    value_loss         | 0.123    |\n","------------------------------------\n","Eval num_timesteps=260000, episode_reward=250.00 +/- 148.32\n","Episode length: 1429.80 +/- 272.56\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.43e+03 |\n","|    mean_reward        | 250      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.38e+03 |\n","|    ep_rew_mean        | 188      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 3300     |\n","|    time_elapsed       | 578      |\n","|    total_timesteps    | 264000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.886    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3299     |\n","|    policy_loss        | -0.044   |\n","|    value_loss         | 0.00477  |\n","------------------------------------\n","Eval num_timesteps=270000, episode_reward=165.00 +/- 76.81\n","Episode length: 1416.00 +/- 167.54\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.42e+03 |\n","|    mean_reward        | 165      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.36e+03 |\n","|    ep_rew_mean        | 175      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 3400     |\n","|    time_elapsed       | 597      |\n","|    total_timesteps    | 272000   |\n","| train/                |          |\n","|    entropy_loss       | -1.74    |\n","|    explained_variance | -0.179   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3399     |\n","|    policy_loss        | -0.247   |\n","|    value_loss         | 0.605    |\n","------------------------------------\n","Eval num_timesteps=280000, episode_reward=175.00 +/- 68.92\n","Episode length: 1408.80 +/- 208.74\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.41e+03 |\n","|    mean_reward        | 175      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.34e+03 |\n","|    ep_rew_mean        | 174      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 3500     |\n","|    time_elapsed       | 615      |\n","|    total_timesteps    | 280000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.852    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3499     |\n","|    policy_loss        | -0.0339  |\n","|    value_loss         | 0.0263   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.4e+03  |\n","|    ep_rew_mean        | 224      |\n","| time/                 |          |\n","|    fps                | 457      |\n","|    iterations         | 3600     |\n","|    time_elapsed       | 629      |\n","|    total_timesteps    | 288000   |\n","| train/                |          |\n","|    entropy_loss       | -1.65    |\n","|    explained_variance | 0.872    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3599     |\n","|    policy_loss        | 0.0511   |\n","|    value_loss         | 0.0668   |\n","------------------------------------\n","Eval num_timesteps=290000, episode_reward=260.00 +/- 64.42\n","Episode length: 1396.60 +/- 172.02\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.4e+03  |\n","|    mean_reward        | 260      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.43e+03 |\n","|    ep_rew_mean        | 245      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 3700     |\n","|    time_elapsed       | 648      |\n","|    total_timesteps    | 296000   |\n","| train/                |          |\n","|    entropy_loss       | -1.71    |\n","|    explained_variance | 0.842    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3699     |\n","|    policy_loss        | -0.0351  |\n","|    value_loss         | 0.0511   |\n","------------------------------------\n","Eval num_timesteps=300000, episode_reward=250.00 +/- 77.46\n","Episode length: 1374.40 +/- 171.71\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.37e+03 |\n","|    mean_reward        | 250      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 272      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 3800     |\n","|    time_elapsed       | 666      |\n","|    total_timesteps    | 304000   |\n","| train/                |          |\n","|    entropy_loss       | -1.74    |\n","|    explained_variance | 0.768    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3799     |\n","|    policy_loss        | 0.12     |\n","|    value_loss         | 0.0589   |\n","------------------------------------\n","Eval num_timesteps=310000, episode_reward=240.00 +/- 43.59\n","Episode length: 1377.00 +/- 113.34\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.38e+03 |\n","|    mean_reward        | 240      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 284      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 3900     |\n","|    time_elapsed       | 684      |\n","|    total_timesteps    | 312000   |\n","| train/                |          |\n","|    entropy_loss       | -1.77    |\n","|    explained_variance | 0.869    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3899     |\n","|    policy_loss        | -0.0326  |\n","|    value_loss         | 0.0103   |\n","------------------------------------\n","Eval num_timesteps=320000, episode_reward=245.00 +/- 79.69\n","Episode length: 1346.80 +/- 133.79\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.35e+03 |\n","|    mean_reward        | 245      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 259      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 4000     |\n","|    time_elapsed       | 703      |\n","|    total_timesteps    | 320000   |\n","| train/                |          |\n","|    entropy_loss       | -1.68    |\n","|    explained_variance | 0.765    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3999     |\n","|    policy_loss        | -0.0377  |\n","|    value_loss         | 0.122    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.42e+03 |\n","|    ep_rew_mean        | 261      |\n","| time/                 |          |\n","|    fps                | 457      |\n","|    iterations         | 4100     |\n","|    time_elapsed       | 717      |\n","|    total_timesteps    | 328000   |\n","| train/                |          |\n","|    entropy_loss       | -1.62    |\n","|    explained_variance | 0.175    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4099     |\n","|    policy_loss        | -0.659   |\n","|    value_loss         | 0.772    |\n","------------------------------------\n","Eval num_timesteps=330000, episode_reward=210.00 +/- 116.83\n","Episode length: 1391.00 +/- 121.40\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.39e+03 |\n","|    mean_reward        | 210      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.42e+03 |\n","|    ep_rew_mean        | 264      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 4200     |\n","|    time_elapsed       | 735      |\n","|    total_timesteps    | 336000   |\n","| train/                |          |\n","|    entropy_loss       | -1.71    |\n","|    explained_variance | 0.844    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4199     |\n","|    policy_loss        | 0.0815   |\n","|    value_loss         | 0.0288   |\n","------------------------------------\n","Eval num_timesteps=340000, episode_reward=285.00 +/- 196.60\n","Episode length: 1396.00 +/- 210.27\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.4e+03  |\n","|    mean_reward        | 285      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.43e+03 |\n","|    ep_rew_mean        | 263      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 4300     |\n","|    time_elapsed       | 753      |\n","|    total_timesteps    | 344000   |\n","| train/                |          |\n","|    entropy_loss       | -1.69    |\n","|    explained_variance | 0.849    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4299     |\n","|    policy_loss        | 0.155    |\n","|    value_loss         | 0.0693   |\n","------------------------------------\n","Eval num_timesteps=350000, episode_reward=275.00 +/- 79.06\n","Episode length: 1439.80 +/- 195.04\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.44e+03 |\n","|    mean_reward        | 275      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.42e+03 |\n","|    ep_rew_mean        | 270      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 4400     |\n","|    time_elapsed       | 772      |\n","|    total_timesteps    | 352000   |\n","| train/                |          |\n","|    entropy_loss       | -1.57    |\n","|    explained_variance | 0.918    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4399     |\n","|    policy_loss        | 0.00368  |\n","|    value_loss         | 0.0657   |\n","------------------------------------\n","Eval num_timesteps=360000, episode_reward=235.00 +/- 40.62\n","Episode length: 1332.20 +/- 115.38\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.33e+03 |\n","|    mean_reward        | 235      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.43e+03 |\n","|    ep_rew_mean        | 258      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 4500     |\n","|    time_elapsed       | 790      |\n","|    total_timesteps    | 360000   |\n","| train/                |          |\n","|    entropy_loss       | -1.67    |\n","|    explained_variance | 0.835    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4499     |\n","|    policy_loss        | 0.0528   |\n","|    value_loss         | 0.0859   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 258      |\n","| time/                 |          |\n","|    fps                | 457      |\n","|    iterations         | 4600     |\n","|    time_elapsed       | 804      |\n","|    total_timesteps    | 368000   |\n","| train/                |          |\n","|    entropy_loss       | -1.71    |\n","|    explained_variance | 0.858    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4599     |\n","|    policy_loss        | 0.0689   |\n","|    value_loss         | 0.0676   |\n","------------------------------------\n","Eval num_timesteps=370000, episode_reward=270.00 +/- 53.39\n","Episode length: 1459.00 +/- 109.98\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.46e+03 |\n","|    mean_reward        | 270      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 260      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 4700     |\n","|    time_elapsed       | 823      |\n","|    total_timesteps    | 376000   |\n","| train/                |          |\n","|    entropy_loss       | -1.6     |\n","|    explained_variance | 0.768    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4699     |\n","|    policy_loss        | -0.0978  |\n","|    value_loss         | 0.203    |\n","------------------------------------\n","Eval num_timesteps=380000, episode_reward=310.00 +/- 51.48\n","Episode length: 1449.40 +/- 189.14\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.45e+03 |\n","|    mean_reward        | 310      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.43e+03 |\n","|    ep_rew_mean        | 268      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 4800     |\n","|    time_elapsed       | 842      |\n","|    total_timesteps    | 384000   |\n","| train/                |          |\n","|    entropy_loss       | -1.54    |\n","|    explained_variance | 0.581    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4799     |\n","|    policy_loss        | -0.0793  |\n","|    value_loss         | 0.211    |\n","------------------------------------\n","Eval num_timesteps=390000, episode_reward=315.00 +/- 158.59\n","Episode length: 1482.20 +/- 157.73\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.48e+03 |\n","|    mean_reward        | 315      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.46e+03 |\n","|    ep_rew_mean        | 290      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 4900     |\n","|    time_elapsed       | 860      |\n","|    total_timesteps    | 392000   |\n","| train/                |          |\n","|    entropy_loss       | -1.6     |\n","|    explained_variance | 0.899    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4899     |\n","|    policy_loss        | 0.0708   |\n","|    value_loss         | 0.0891   |\n","------------------------------------\n","Eval num_timesteps=400000, episode_reward=250.00 +/- 54.77\n","Episode length: 1332.20 +/- 165.85\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.33e+03 |\n","|    mean_reward        | 250      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 298      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 5000     |\n","|    time_elapsed       | 879      |\n","|    total_timesteps    | 400000   |\n","| train/                |          |\n","|    entropy_loss       | -1.69    |\n","|    explained_variance | 0.896    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4999     |\n","|    policy_loss        | -0.0508  |\n","|    value_loss         | 0.0292   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.43e+03 |\n","|    ep_rew_mean        | 290      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 5100     |\n","|    time_elapsed       | 893      |\n","|    total_timesteps    | 408000   |\n","| train/                |          |\n","|    entropy_loss       | -1.52    |\n","|    explained_variance | 0.828    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5099     |\n","|    policy_loss        | 0.0603   |\n","|    value_loss         | 0.235    |\n","------------------------------------\n","Eval num_timesteps=410000, episode_reward=315.00 +/- 37.42\n","Episode length: 1315.40 +/- 91.98\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.32e+03 |\n","|    mean_reward        | 315      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.42e+03 |\n","|    ep_rew_mean        | 286      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 5200     |\n","|    time_elapsed       | 911      |\n","|    total_timesteps    | 416000   |\n","| train/                |          |\n","|    entropy_loss       | -1.63    |\n","|    explained_variance | 0.808    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5199     |\n","|    policy_loss        | 0.14     |\n","|    value_loss         | 0.0859   |\n","------------------------------------\n","Eval num_timesteps=420000, episode_reward=265.00 +/- 48.99\n","Episode length: 1409.40 +/- 133.34\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.41e+03 |\n","|    mean_reward        | 265      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.42e+03 |\n","|    ep_rew_mean        | 291      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 5300     |\n","|    time_elapsed       | 930      |\n","|    total_timesteps    | 424000   |\n","| train/                |          |\n","|    entropy_loss       | -1.71    |\n","|    explained_variance | 0.956    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5299     |\n","|    policy_loss        | -0.0189  |\n","|    value_loss         | 0.0155   |\n","------------------------------------\n","Eval num_timesteps=430000, episode_reward=280.00 +/- 29.15\n","Episode length: 1375.40 +/- 146.15\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.38e+03 |\n","|    mean_reward        | 280      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.4e+03  |\n","|    ep_rew_mean        | 286      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 5400     |\n","|    time_elapsed       | 948      |\n","|    total_timesteps    | 432000   |\n","| train/                |          |\n","|    entropy_loss       | -1.65    |\n","|    explained_variance | 0.909    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5399     |\n","|    policy_loss        | 0.0148   |\n","|    value_loss         | 0.0382   |\n","------------------------------------\n","Eval num_timesteps=440000, episode_reward=320.00 +/- 43.01\n","Episode length: 1472.00 +/- 191.54\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.47e+03 |\n","|    mean_reward        | 320      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.41e+03 |\n","|    ep_rew_mean        | 290      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 5500     |\n","|    time_elapsed       | 967      |\n","|    total_timesteps    | 440000   |\n","| train/                |          |\n","|    entropy_loss       | -1.58    |\n","|    explained_variance | 0.335    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5499     |\n","|    policy_loss        | -0.0697  |\n","|    value_loss         | 0.625    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.41e+03 |\n","|    ep_rew_mean        | 286      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 5600     |\n","|    time_elapsed       | 981      |\n","|    total_timesteps    | 448000   |\n","| train/                |          |\n","|    entropy_loss       | -1.68    |\n","|    explained_variance | 0.961    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5599     |\n","|    policy_loss        | -0.0816  |\n","|    value_loss         | 0.034    |\n","------------------------------------\n","Eval num_timesteps=450000, episode_reward=330.00 +/- 60.00\n","Episode length: 1538.40 +/- 304.48\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.54e+03 |\n","|    mean_reward        | 330      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.39e+03 |\n","|    ep_rew_mean        | 286      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 5700     |\n","|    time_elapsed       | 1000     |\n","|    total_timesteps    | 456000   |\n","| train/                |          |\n","|    entropy_loss       | -1.68    |\n","|    explained_variance | 0.919    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5699     |\n","|    policy_loss        | 0.0148   |\n","|    value_loss         | 0.0324   |\n","------------------------------------\n","Eval num_timesteps=460000, episode_reward=325.00 +/- 59.16\n","Episode length: 1466.40 +/- 290.27\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.47e+03 |\n","|    mean_reward        | 325      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.39e+03 |\n","|    ep_rew_mean        | 292      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 5800     |\n","|    time_elapsed       | 1018     |\n","|    total_timesteps    | 464000   |\n","| train/                |          |\n","|    entropy_loss       | -1.65    |\n","|    explained_variance | 0.733    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5799     |\n","|    policy_loss        | -0.00685 |\n","|    value_loss         | 0.0494   |\n","------------------------------------\n","Eval num_timesteps=470000, episode_reward=250.00 +/- 54.77\n","Episode length: 1326.20 +/- 130.78\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.33e+03 |\n","|    mean_reward        | 250      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.4e+03  |\n","|    ep_rew_mean        | 300      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 5900     |\n","|    time_elapsed       | 1037     |\n","|    total_timesteps    | 472000   |\n","| train/                |          |\n","|    entropy_loss       | -1.62    |\n","|    explained_variance | 0.859    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5899     |\n","|    policy_loss        | 0.1      |\n","|    value_loss         | 0.162    |\n","------------------------------------\n","Eval num_timesteps=480000, episode_reward=285.00 +/- 78.42\n","Episode length: 1477.60 +/- 167.65\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.48e+03 |\n","|    mean_reward        | 285      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.41e+03 |\n","|    ep_rew_mean        | 306      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 6000     |\n","|    time_elapsed       | 1055     |\n","|    total_timesteps    | 480000   |\n","| train/                |          |\n","|    entropy_loss       | -1.67    |\n","|    explained_variance | 0.935    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5999     |\n","|    policy_loss        | 0.0626   |\n","|    value_loss         | 0.041    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 306      |\n","| time/                 |          |\n","|    fps                | 456      |\n","|    iterations         | 6100     |\n","|    time_elapsed       | 1069     |\n","|    total_timesteps    | 488000   |\n","| train/                |          |\n","|    entropy_loss       | -1.63    |\n","|    explained_variance | 0.915    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6099     |\n","|    policy_loss        | -0.0252  |\n","|    value_loss         | 0.0926   |\n","------------------------------------\n","Eval num_timesteps=490000, episode_reward=315.00 +/- 136.57\n","Episode length: 1514.20 +/- 277.99\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.51e+03 |\n","|    mean_reward        | 315      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 296      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 6200     |\n","|    time_elapsed       | 1088     |\n","|    total_timesteps    | 496000   |\n","| train/                |          |\n","|    entropy_loss       | -1.65    |\n","|    explained_variance | 0.979    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6199     |\n","|    policy_loss        | -0.136   |\n","|    value_loss         | 0.0197   |\n","------------------------------------\n","Eval num_timesteps=500000, episode_reward=260.00 +/- 46.37\n","Episode length: 1463.40 +/- 201.08\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.46e+03 |\n","|    mean_reward        | 260      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 293      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 6300     |\n","|    time_elapsed       | 1106     |\n","|    total_timesteps    | 504000   |\n","| train/                |          |\n","|    entropy_loss       | -1.69    |\n","|    explained_variance | 0.851    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6299     |\n","|    policy_loss        | -0.0081  |\n","|    value_loss         | 0.0648   |\n","------------------------------------\n","Eval num_timesteps=510000, episode_reward=265.00 +/- 51.48\n","Episode length: 1420.00 +/- 142.84\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.42e+03 |\n","|    mean_reward        | 265      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.46e+03 |\n","|    ep_rew_mean        | 290      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 6400     |\n","|    time_elapsed       | 1125     |\n","|    total_timesteps    | 512000   |\n","| train/                |          |\n","|    entropy_loss       | -1.71    |\n","|    explained_variance | 0.675    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6399     |\n","|    policy_loss        | 0.139    |\n","|    value_loss         | 0.138    |\n","------------------------------------\n","Eval num_timesteps=520000, episode_reward=275.00 +/- 70.71\n","Episode length: 1506.80 +/- 117.33\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.51e+03 |\n","|    mean_reward        | 275      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 289      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 6500     |\n","|    time_elapsed       | 1144     |\n","|    total_timesteps    | 520000   |\n","| train/                |          |\n","|    entropy_loss       | -1.57    |\n","|    explained_variance | 0.895    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6499     |\n","|    policy_loss        | -0.0411  |\n","|    value_loss         | 0.0749   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.49e+03 |\n","|    ep_rew_mean        | 294      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 6600     |\n","|    time_elapsed       | 1158     |\n","|    total_timesteps    | 528000   |\n","| train/                |          |\n","|    entropy_loss       | -1.7     |\n","|    explained_variance | 0.804    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6599     |\n","|    policy_loss        | -0.0264  |\n","|    value_loss         | 0.041    |\n","------------------------------------\n","Eval num_timesteps=530000, episode_reward=230.00 +/- 90.00\n","Episode length: 1333.20 +/- 200.19\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.33e+03 |\n","|    mean_reward        | 230      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 288      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 6700     |\n","|    time_elapsed       | 1176     |\n","|    total_timesteps    | 536000   |\n","| train/                |          |\n","|    entropy_loss       | -1.5     |\n","|    explained_variance | 0.751    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6699     |\n","|    policy_loss        | -0.138   |\n","|    value_loss         | 0.457    |\n","------------------------------------\n","Eval num_timesteps=540000, episode_reward=295.00 +/- 50.99\n","Episode length: 1384.60 +/- 158.24\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.38e+03 |\n","|    mean_reward        | 295      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 300      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 6800     |\n","|    time_elapsed       | 1194     |\n","|    total_timesteps    | 544000   |\n","| train/                |          |\n","|    entropy_loss       | -1.48    |\n","|    explained_variance | 0.953    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6799     |\n","|    policy_loss        | -0.107   |\n","|    value_loss         | 0.0857   |\n","------------------------------------\n","Eval num_timesteps=550000, episode_reward=305.00 +/- 69.64\n","Episode length: 1542.40 +/- 344.08\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.54e+03 |\n","|    mean_reward        | 305      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.46e+03 |\n","|    ep_rew_mean        | 304      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 6900     |\n","|    time_elapsed       | 1213     |\n","|    total_timesteps    | 552000   |\n","| train/                |          |\n","|    entropy_loss       | -1.57    |\n","|    explained_variance | 0.98     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6899     |\n","|    policy_loss        | -0.0428  |\n","|    value_loss         | 0.0261   |\n","------------------------------------\n","Eval num_timesteps=560000, episode_reward=300.00 +/- 85.15\n","Episode length: 1423.80 +/- 323.07\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.42e+03 |\n","|    mean_reward        | 300      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.43e+03 |\n","|    ep_rew_mean        | 283      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 7000     |\n","|    time_elapsed       | 1232     |\n","|    total_timesteps    | 560000   |\n","| train/                |          |\n","|    entropy_loss       | -1.4     |\n","|    explained_variance | 0.94     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6999     |\n","|    policy_loss        | -0.0616  |\n","|    value_loss         | 0.132    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 286      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 7100     |\n","|    time_elapsed       | 1246     |\n","|    total_timesteps    | 568000   |\n","| train/                |          |\n","|    entropy_loss       | -1.66    |\n","|    explained_variance | 0.841    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7099     |\n","|    policy_loss        | 0.0431   |\n","|    value_loss         | 0.07     |\n","------------------------------------\n","Eval num_timesteps=570000, episode_reward=235.00 +/- 46.37\n","Episode length: 1309.40 +/- 157.37\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.31e+03 |\n","|    mean_reward        | 235      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 291      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 7200     |\n","|    time_elapsed       | 1264     |\n","|    total_timesteps    | 576000   |\n","| train/                |          |\n","|    entropy_loss       | -1.71    |\n","|    explained_variance | 0.929    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7199     |\n","|    policy_loss        | -0.0398  |\n","|    value_loss         | 0.0378   |\n","------------------------------------\n","Eval num_timesteps=580000, episode_reward=335.00 +/- 105.59\n","Episode length: 1483.40 +/- 321.24\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.48e+03 |\n","|    mean_reward        | 335      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 283      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 7300     |\n","|    time_elapsed       | 1283     |\n","|    total_timesteps    | 584000   |\n","| train/                |          |\n","|    entropy_loss       | -1.61    |\n","|    explained_variance | 0.853    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7299     |\n","|    policy_loss        | 0.296    |\n","|    value_loss         | 0.274    |\n","------------------------------------\n","Eval num_timesteps=590000, episode_reward=435.00 +/- 235.90\n","Episode length: 1588.40 +/- 349.65\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.59e+03 |\n","|    mean_reward        | 435      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 288      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 7400     |\n","|    time_elapsed       | 1302     |\n","|    total_timesteps    | 592000   |\n","| train/                |          |\n","|    entropy_loss       | -1.66    |\n","|    explained_variance | 0.967    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7399     |\n","|    policy_loss        | 0.0329   |\n","|    value_loss         | 0.0539   |\n","------------------------------------\n","Eval num_timesteps=600000, episode_reward=295.00 +/- 71.41\n","Episode length: 1545.00 +/- 115.89\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.54e+03 |\n","|    mean_reward        | 295      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.46e+03 |\n","|    ep_rew_mean        | 298      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 7500     |\n","|    time_elapsed       | 1321     |\n","|    total_timesteps    | 600000   |\n","| train/                |          |\n","|    entropy_loss       | -1.64    |\n","|    explained_variance | 0.849    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7499     |\n","|    policy_loss        | -0.0584  |\n","|    value_loss         | 0.13     |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.48e+03 |\n","|    ep_rew_mean        | 322      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 7600     |\n","|    time_elapsed       | 1335     |\n","|    total_timesteps    | 608000   |\n","| train/                |          |\n","|    entropy_loss       | -1.67    |\n","|    explained_variance | 0.909    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7599     |\n","|    policy_loss        | 0.23     |\n","|    value_loss         | 0.151    |\n","------------------------------------\n","Eval num_timesteps=610000, episode_reward=280.00 +/- 67.82\n","Episode length: 1425.40 +/- 178.91\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.43e+03 |\n","|    mean_reward        | 280      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.5e+03  |\n","|    ep_rew_mean        | 331      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 7700     |\n","|    time_elapsed       | 1353     |\n","|    total_timesteps    | 616000   |\n","| train/                |          |\n","|    entropy_loss       | -1.7     |\n","|    explained_variance | 0.949    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7699     |\n","|    policy_loss        | -0.00553 |\n","|    value_loss         | 0.0484   |\n","------------------------------------\n","Eval num_timesteps=620000, episode_reward=285.00 +/- 70.00\n","Episode length: 1486.00 +/- 118.34\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.49e+03 |\n","|    mean_reward        | 285      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.5e+03  |\n","|    ep_rew_mean        | 318      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 7800     |\n","|    time_elapsed       | 1372     |\n","|    total_timesteps    | 624000   |\n","| train/                |          |\n","|    entropy_loss       | -1.64    |\n","|    explained_variance | 0.949    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7799     |\n","|    policy_loss        | -0.0169  |\n","|    value_loss         | 0.112    |\n","------------------------------------\n","Eval num_timesteps=630000, episode_reward=445.00 +/- 199.00\n","Episode length: 1439.60 +/- 164.92\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.44e+03 |\n","|    mean_reward        | 445      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.48e+03 |\n","|    ep_rew_mean        | 309      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 7900     |\n","|    time_elapsed       | 1390     |\n","|    total_timesteps    | 632000   |\n","| train/                |          |\n","|    entropy_loss       | -1.65    |\n","|    explained_variance | 0.907    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7899     |\n","|    policy_loss        | 0.13     |\n","|    value_loss         | 0.0853   |\n","------------------------------------\n","Eval num_timesteps=640000, episode_reward=370.00 +/- 163.10\n","Episode length: 1442.40 +/- 153.75\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.44e+03 |\n","|    mean_reward        | 370      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.48e+03 |\n","|    ep_rew_mean        | 288      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 8000     |\n","|    time_elapsed       | 1409     |\n","|    total_timesteps    | 640000   |\n","| train/                |          |\n","|    entropy_loss       | -1.61    |\n","|    explained_variance | 0.968    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7999     |\n","|    policy_loss        | -0.0306  |\n","|    value_loss         | 0.0274   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.48e+03 |\n","|    ep_rew_mean        | 291      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 8100     |\n","|    time_elapsed       | 1423     |\n","|    total_timesteps    | 648000   |\n","| train/                |          |\n","|    entropy_loss       | -1.68    |\n","|    explained_variance | 0.91     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8099     |\n","|    policy_loss        | 0.116    |\n","|    value_loss         | 0.0689   |\n","------------------------------------\n","Eval num_timesteps=650000, episode_reward=295.00 +/- 57.88\n","Episode length: 1402.60 +/- 66.67\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.4e+03  |\n","|    mean_reward        | 295      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.46e+03 |\n","|    ep_rew_mean        | 287      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 8200     |\n","|    time_elapsed       | 1442     |\n","|    total_timesteps    | 656000   |\n","| train/                |          |\n","|    entropy_loss       | -1.65    |\n","|    explained_variance | 0.911    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8199     |\n","|    policy_loss        | 0.108    |\n","|    value_loss         | 0.0573   |\n","------------------------------------\n","Eval num_timesteps=660000, episode_reward=305.00 +/- 134.54\n","Episode length: 1492.40 +/- 175.94\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.49e+03 |\n","|    mean_reward        | 305      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.48e+03 |\n","|    ep_rew_mean        | 301      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 8300     |\n","|    time_elapsed       | 1460     |\n","|    total_timesteps    | 664000   |\n","| train/                |          |\n","|    entropy_loss       | -1.65    |\n","|    explained_variance | 0.978    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8299     |\n","|    policy_loss        | -0.114   |\n","|    value_loss         | 0.0266   |\n","------------------------------------\n","Eval num_timesteps=670000, episode_reward=255.00 +/- 76.49\n","Episode length: 1401.20 +/- 352.01\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.4e+03  |\n","|    mean_reward        | 255      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 304      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 8400     |\n","|    time_elapsed       | 1479     |\n","|    total_timesteps    | 672000   |\n","| train/                |          |\n","|    entropy_loss       | -1.62    |\n","|    explained_variance | 0.986    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8399     |\n","|    policy_loss        | -0.134   |\n","|    value_loss         | 0.0253   |\n","------------------------------------\n","Eval num_timesteps=680000, episode_reward=265.00 +/- 90.28\n","Episode length: 1275.20 +/- 61.62\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.28e+03 |\n","|    mean_reward        | 265      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 304      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 8500     |\n","|    time_elapsed       | 1497     |\n","|    total_timesteps    | 680000   |\n","| train/                |          |\n","|    entropy_loss       | -1.69    |\n","|    explained_variance | 0.93     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8499     |\n","|    policy_loss        | -0.0108  |\n","|    value_loss         | 0.058    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.46e+03 |\n","|    ep_rew_mean        | 301      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 8600     |\n","|    time_elapsed       | 1511     |\n","|    total_timesteps    | 688000   |\n","| train/                |          |\n","|    entropy_loss       | -1.63    |\n","|    explained_variance | 0.921    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8599     |\n","|    policy_loss        | 0.161    |\n","|    value_loss         | 0.107    |\n","------------------------------------\n","Eval num_timesteps=690000, episode_reward=290.00 +/- 43.59\n","Episode length: 1587.60 +/- 57.86\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.59e+03 |\n","|    mean_reward        | 290      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 312      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 8700     |\n","|    time_elapsed       | 1530     |\n","|    total_timesteps    | 696000   |\n","| train/                |          |\n","|    entropy_loss       | -1.57    |\n","|    explained_variance | 0.958    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8699     |\n","|    policy_loss        | 0.082    |\n","|    value_loss         | 0.0769   |\n","------------------------------------\n","Eval num_timesteps=700000, episode_reward=235.00 +/- 71.76\n","Episode length: 1309.80 +/- 160.70\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.31e+03 |\n","|    mean_reward        | 235      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.46e+03 |\n","|    ep_rew_mean        | 308      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 8800     |\n","|    time_elapsed       | 1548     |\n","|    total_timesteps    | 704000   |\n","| train/                |          |\n","|    entropy_loss       | -1.71    |\n","|    explained_variance | 0.867    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8799     |\n","|    policy_loss        | -0.219   |\n","|    value_loss         | 0.0677   |\n","------------------------------------\n","Eval num_timesteps=710000, episode_reward=425.00 +/- 165.08\n","Episode length: 1635.20 +/- 245.81\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.64e+03 |\n","|    mean_reward        | 425      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.46e+03 |\n","|    ep_rew_mean        | 313      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 8900     |\n","|    time_elapsed       | 1567     |\n","|    total_timesteps    | 712000   |\n","| train/                |          |\n","|    entropy_loss       | -1.69    |\n","|    explained_variance | 0.779    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8899     |\n","|    policy_loss        | 0.146    |\n","|    value_loss         | 0.175    |\n","------------------------------------\n","Eval num_timesteps=720000, episode_reward=280.00 +/- 87.18\n","Episode length: 1643.60 +/- 443.94\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.64e+03 |\n","|    mean_reward        | 280      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 305      |\n","| time/                 |          |\n","|    fps                | 453      |\n","|    iterations         | 9000     |\n","|    time_elapsed       | 1586     |\n","|    total_timesteps    | 720000   |\n","| train/                |          |\n","|    entropy_loss       | -1.64    |\n","|    explained_variance | 0.978    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8999     |\n","|    policy_loss        | -0.133   |\n","|    value_loss         | 0.0298   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 300      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 9100     |\n","|    time_elapsed       | 1600     |\n","|    total_timesteps    | 728000   |\n","| train/                |          |\n","|    entropy_loss       | -1.73    |\n","|    explained_variance | 0.976    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9099     |\n","|    policy_loss        | -0.0669  |\n","|    value_loss         | 0.0148   |\n","------------------------------------\n","Eval num_timesteps=730000, episode_reward=250.00 +/- 61.24\n","Episode length: 1406.40 +/- 246.74\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.41e+03 |\n","|    mean_reward        | 250      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.5e+03  |\n","|    ep_rew_mean        | 308      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 9200     |\n","|    time_elapsed       | 1618     |\n","|    total_timesteps    | 736000   |\n","| train/                |          |\n","|    entropy_loss       | -1.62    |\n","|    explained_variance | 0.849    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9199     |\n","|    policy_loss        | 0.163    |\n","|    value_loss         | 0.124    |\n","------------------------------------\n","Eval num_timesteps=740000, episode_reward=315.00 +/- 46.37\n","Episode length: 1535.80 +/- 211.49\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.54e+03 |\n","|    mean_reward        | 315      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.52e+03 |\n","|    ep_rew_mean        | 322      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 9300     |\n","|    time_elapsed       | 1637     |\n","|    total_timesteps    | 744000   |\n","| train/                |          |\n","|    entropy_loss       | -1.7     |\n","|    explained_variance | 0.9      |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9299     |\n","|    policy_loss        | 0.0611   |\n","|    value_loss         | 0.0524   |\n","------------------------------------\n","Eval num_timesteps=750000, episode_reward=245.00 +/- 18.71\n","Episode length: 1389.00 +/- 196.93\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.39e+03 |\n","|    mean_reward        | 245      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.5e+03  |\n","|    ep_rew_mean        | 316      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 9400     |\n","|    time_elapsed       | 1655     |\n","|    total_timesteps    | 752000   |\n","| train/                |          |\n","|    entropy_loss       | -1.6     |\n","|    explained_variance | 0.964    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9399     |\n","|    policy_loss        | 0.0401   |\n","|    value_loss         | 0.0931   |\n","------------------------------------\n","Eval num_timesteps=760000, episode_reward=290.00 +/- 48.99\n","Episode length: 1438.80 +/- 215.54\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.44e+03 |\n","|    mean_reward        | 290      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.53e+03 |\n","|    ep_rew_mean        | 332      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 9500     |\n","|    time_elapsed       | 1674     |\n","|    total_timesteps    | 760000   |\n","| train/                |          |\n","|    entropy_loss       | -1.64    |\n","|    explained_variance | 0.968    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9499     |\n","|    policy_loss        | 0.135    |\n","|    value_loss         | 0.0433   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.49e+03 |\n","|    ep_rew_mean        | 319      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 9600     |\n","|    time_elapsed       | 1687     |\n","|    total_timesteps    | 768000   |\n","| train/                |          |\n","|    entropy_loss       | -1.67    |\n","|    explained_variance | 0.921    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9599     |\n","|    policy_loss        | -0.0136  |\n","|    value_loss         | 0.0499   |\n","------------------------------------\n","Eval num_timesteps=770000, episode_reward=220.00 +/- 50.99\n","Episode length: 1255.60 +/- 196.82\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.26e+03 |\n","|    mean_reward        | 220      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.48e+03 |\n","|    ep_rew_mean        | 320      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 9700     |\n","|    time_elapsed       | 1705     |\n","|    total_timesteps    | 776000   |\n","| train/                |          |\n","|    entropy_loss       | -1.55    |\n","|    explained_variance | 0.988    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9699     |\n","|    policy_loss        | -0.0607  |\n","|    value_loss         | 0.0192   |\n","------------------------------------\n","Eval num_timesteps=780000, episode_reward=355.00 +/- 163.10\n","Episode length: 1520.20 +/- 211.21\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.52e+03 |\n","|    mean_reward        | 355      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 304      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 9800     |\n","|    time_elapsed       | 1724     |\n","|    total_timesteps    | 784000   |\n","| train/                |          |\n","|    entropy_loss       | -1.68    |\n","|    explained_variance | 0.86     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9799     |\n","|    policy_loss        | 0.0195   |\n","|    value_loss         | 0.0879   |\n","------------------------------------\n","Eval num_timesteps=790000, episode_reward=395.00 +/- 234.73\n","Episode length: 1417.00 +/- 259.47\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.42e+03 |\n","|    mean_reward        | 395      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.43e+03 |\n","|    ep_rew_mean        | 291      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 9900     |\n","|    time_elapsed       | 1742     |\n","|    total_timesteps    | 792000   |\n","| train/                |          |\n","|    entropy_loss       | -1.64    |\n","|    explained_variance | 0.977    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9899     |\n","|    policy_loss        | 0.0204   |\n","|    value_loss         | 0.0233   |\n","------------------------------------\n","Eval num_timesteps=800000, episode_reward=265.00 +/- 40.62\n","Episode length: 1420.20 +/- 168.77\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.42e+03 |\n","|    mean_reward        | 265      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 295      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 10000    |\n","|    time_elapsed       | 1761     |\n","|    total_timesteps    | 800000   |\n","| train/                |          |\n","|    entropy_loss       | -1.63    |\n","|    explained_variance | 0.975    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9999     |\n","|    policy_loss        | -0.00273 |\n","|    value_loss         | 0.0316   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 297      |\n","| time/                 |          |\n","|    fps                | 455      |\n","|    iterations         | 10100    |\n","|    time_elapsed       | 1775     |\n","|    total_timesteps    | 808000   |\n","| train/                |          |\n","|    entropy_loss       | -1.72    |\n","|    explained_variance | 0.939    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10099    |\n","|    policy_loss        | 0.0382   |\n","|    value_loss         | 0.044    |\n","------------------------------------\n","Eval num_timesteps=810000, episode_reward=250.00 +/- 67.08\n","Episode length: 1441.20 +/- 101.79\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.44e+03 |\n","|    mean_reward        | 250      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 307      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 10200    |\n","|    time_elapsed       | 1793     |\n","|    total_timesteps    | 816000   |\n","| train/                |          |\n","|    entropy_loss       | -1.7     |\n","|    explained_variance | 0.763    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10199    |\n","|    policy_loss        | -0.0856  |\n","|    value_loss         | 0.148    |\n","------------------------------------\n","Eval num_timesteps=820000, episode_reward=290.00 +/- 64.42\n","Episode length: 1469.00 +/- 239.99\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.47e+03 |\n","|    mean_reward        | 290      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.51e+03 |\n","|    ep_rew_mean        | 310      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 10300    |\n","|    time_elapsed       | 1812     |\n","|    total_timesteps    | 824000   |\n","| train/                |          |\n","|    entropy_loss       | -1.52    |\n","|    explained_variance | 0.966    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10299    |\n","|    policy_loss        | 0.00597  |\n","|    value_loss         | 0.0677   |\n","------------------------------------\n","Eval num_timesteps=830000, episode_reward=330.00 +/- 45.83\n","Episode length: 1740.40 +/- 237.82\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.74e+03 |\n","|    mean_reward        | 330      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.49e+03 |\n","|    ep_rew_mean        | 313      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 10400    |\n","|    time_elapsed       | 1831     |\n","|    total_timesteps    | 832000   |\n","| train/                |          |\n","|    entropy_loss       | -1.6     |\n","|    explained_variance | 0.965    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10399    |\n","|    policy_loss        | 0.0595   |\n","|    value_loss         | 0.0734   |\n","------------------------------------\n","Eval num_timesteps=840000, episode_reward=245.00 +/- 43.01\n","Episode length: 1314.40 +/- 153.86\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.31e+03 |\n","|    mean_reward        | 245      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.48e+03 |\n","|    ep_rew_mean        | 320      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 10500    |\n","|    time_elapsed       | 1849     |\n","|    total_timesteps    | 840000   |\n","| train/                |          |\n","|    entropy_loss       | -1.55    |\n","|    explained_variance | 0.982    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10499    |\n","|    policy_loss        | -0.0527  |\n","|    value_loss         | 0.0354   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.5e+03  |\n","|    ep_rew_mean        | 325      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 10600    |\n","|    time_elapsed       | 1863     |\n","|    total_timesteps    | 848000   |\n","| train/                |          |\n","|    entropy_loss       | -1.5     |\n","|    explained_variance | 0.944    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10599    |\n","|    policy_loss        | 0.14     |\n","|    value_loss         | 0.0938   |\n","------------------------------------\n","Eval num_timesteps=850000, episode_reward=260.00 +/- 64.42\n","Episode length: 1371.80 +/- 130.08\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.37e+03 |\n","|    mean_reward        | 260      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.48e+03 |\n","|    ep_rew_mean        | 322      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 10700    |\n","|    time_elapsed       | 1881     |\n","|    total_timesteps    | 856000   |\n","| train/                |          |\n","|    entropy_loss       | -1.62    |\n","|    explained_variance | 0.97     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10699    |\n","|    policy_loss        | -0.018   |\n","|    value_loss         | 0.0377   |\n","------------------------------------\n","Eval num_timesteps=860000, episode_reward=290.00 +/- 66.33\n","Episode length: 1442.00 +/- 228.43\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.44e+03 |\n","|    mean_reward        | 290      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 316      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 10800    |\n","|    time_elapsed       | 1900     |\n","|    total_timesteps    | 864000   |\n","| train/                |          |\n","|    entropy_loss       | -1.58    |\n","|    explained_variance | 0.736    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10799    |\n","|    policy_loss        | 0.273    |\n","|    value_loss         | 0.187    |\n","------------------------------------\n","Eval num_timesteps=870000, episode_reward=425.00 +/- 101.24\n","Episode length: 1762.00 +/- 77.49\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.76e+03 |\n","|    mean_reward        | 425      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 311      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 10900    |\n","|    time_elapsed       | 1920     |\n","|    total_timesteps    | 872000   |\n","| train/                |          |\n","|    entropy_loss       | -1.58    |\n","|    explained_variance | 0.92     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10899    |\n","|    policy_loss        | 0.152    |\n","|    value_loss         | 0.0706   |\n","------------------------------------\n","Eval num_timesteps=880000, episode_reward=350.00 +/- 27.39\n","Episode length: 1679.60 +/- 51.91\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.68e+03 |\n","|    mean_reward        | 350      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 316      |\n","| time/                 |          |\n","|    fps                | 453      |\n","|    iterations         | 11000    |\n","|    time_elapsed       | 1939     |\n","|    total_timesteps    | 880000   |\n","| train/                |          |\n","|    entropy_loss       | -1.61    |\n","|    explained_variance | 0.891    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10999    |\n","|    policy_loss        | 0.0718   |\n","|    value_loss         | 0.102    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 308      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 11100    |\n","|    time_elapsed       | 1953     |\n","|    total_timesteps    | 888000   |\n","| train/                |          |\n","|    entropy_loss       | -1.52    |\n","|    explained_variance | 0.983    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11099    |\n","|    policy_loss        | -0.121   |\n","|    value_loss         | 0.0238   |\n","------------------------------------\n","Eval num_timesteps=890000, episode_reward=280.00 +/- 92.74\n","Episode length: 1455.00 +/- 257.58\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.46e+03 |\n","|    mean_reward        | 280      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 316      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 11200    |\n","|    time_elapsed       | 1971     |\n","|    total_timesteps    | 896000   |\n","| train/                |          |\n","|    entropy_loss       | -1.44    |\n","|    explained_variance | 0.956    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11199    |\n","|    policy_loss        | -0.0534  |\n","|    value_loss         | 0.0908   |\n","------------------------------------\n","Eval num_timesteps=900000, episode_reward=315.00 +/- 33.91\n","Episode length: 1474.20 +/- 316.08\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.47e+03 |\n","|    mean_reward        | 315      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.47e+03 |\n","|    ep_rew_mean        | 318      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 11300    |\n","|    time_elapsed       | 1990     |\n","|    total_timesteps    | 904000   |\n","| train/                |          |\n","|    entropy_loss       | -1.61    |\n","|    explained_variance | 0.912    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11299    |\n","|    policy_loss        | -0.155   |\n","|    value_loss         | 0.093    |\n","------------------------------------\n","Eval num_timesteps=910000, episode_reward=220.00 +/- 67.82\n","Episode length: 1304.80 +/- 175.50\n","-------------------------------------\n","| eval/                 |           |\n","|    mean_ep_length     | 1.3e+03   |\n","|    mean_reward        | 220       |\n","| rollout/              |           |\n","|    ep_len_mean        | 1.47e+03  |\n","|    ep_rew_mean        | 324       |\n","| time/                 |           |\n","|    fps                | 454       |\n","|    iterations         | 11400     |\n","|    time_elapsed       | 2008      |\n","|    total_timesteps    | 912000    |\n","| train/                |           |\n","|    entropy_loss       | -1.7      |\n","|    explained_variance | 0.959     |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 11399     |\n","|    policy_loss        | -0.000853 |\n","|    value_loss         | 0.0256    |\n","-------------------------------------\n","Eval num_timesteps=920000, episode_reward=340.00 +/- 90.28\n","Episode length: 1612.40 +/- 121.67\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.61e+03 |\n","|    mean_reward        | 340      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.48e+03 |\n","|    ep_rew_mean        | 322      |\n","| time/                 |          |\n","|    fps                | 453      |\n","|    iterations         | 11500    |\n","|    time_elapsed       | 2027     |\n","|    total_timesteps    | 920000   |\n","| train/                |          |\n","|    entropy_loss       | -1.69    |\n","|    explained_variance | 0.766    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11499    |\n","|    policy_loss        | 0.152    |\n","|    value_loss         | 0.142    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.49e+03 |\n","|    ep_rew_mean        | 320      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 11600    |\n","|    time_elapsed       | 2041     |\n","|    total_timesteps    | 928000   |\n","| train/                |          |\n","|    entropy_loss       | -1.57    |\n","|    explained_variance | 0.967    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11599    |\n","|    policy_loss        | -0.105   |\n","|    value_loss         | 0.0696   |\n","------------------------------------\n","Eval num_timesteps=930000, episode_reward=260.00 +/- 46.37\n","Episode length: 1321.60 +/- 117.97\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.32e+03 |\n","|    mean_reward        | 260      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.52e+03 |\n","|    ep_rew_mean        | 320      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 11700    |\n","|    time_elapsed       | 2059     |\n","|    total_timesteps    | 936000   |\n","| train/                |          |\n","|    entropy_loss       | -1.59    |\n","|    explained_variance | 0.975    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11699    |\n","|    policy_loss        | -0.023   |\n","|    value_loss         | 0.0427   |\n","------------------------------------\n","Eval num_timesteps=940000, episode_reward=335.00 +/- 163.25\n","Episode length: 1556.20 +/- 234.91\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.56e+03 |\n","|    mean_reward        | 335      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.51e+03 |\n","|    ep_rew_mean        | 312      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 11800    |\n","|    time_elapsed       | 2078     |\n","|    total_timesteps    | 944000   |\n","| train/                |          |\n","|    entropy_loss       | -1.68    |\n","|    explained_variance | 0.826    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11799    |\n","|    policy_loss        | -0.00933 |\n","|    value_loss         | 0.0571   |\n","------------------------------------\n","Eval num_timesteps=950000, episode_reward=360.00 +/- 145.43\n","Episode length: 1421.40 +/- 257.00\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.42e+03 |\n","|    mean_reward        | 360      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.49e+03 |\n","|    ep_rew_mean        | 310      |\n","| time/                 |          |\n","|    fps                | 453      |\n","|    iterations         | 11900    |\n","|    time_elapsed       | 2097     |\n","|    total_timesteps    | 952000   |\n","| train/                |          |\n","|    entropy_loss       | -1.53    |\n","|    explained_variance | 0.986    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11899    |\n","|    policy_loss        | -0.052   |\n","|    value_loss         | 0.0185   |\n","------------------------------------\n","Eval num_timesteps=960000, episode_reward=265.00 +/- 30.00\n","Episode length: 1438.40 +/- 183.61\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.44e+03 |\n","|    mean_reward        | 265      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.48e+03 |\n","|    ep_rew_mean        | 310      |\n","| time/                 |          |\n","|    fps                | 453      |\n","|    iterations         | 12000    |\n","|    time_elapsed       | 2115     |\n","|    total_timesteps    | 960000   |\n","| train/                |          |\n","|    entropy_loss       | -1.52    |\n","|    explained_variance | 0.935    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11999    |\n","|    policy_loss        | -0.121   |\n","|    value_loss         | 0.0793   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 301      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 12100    |\n","|    time_elapsed       | 2129     |\n","|    total_timesteps    | 968000   |\n","| train/                |          |\n","|    entropy_loss       | -1.63    |\n","|    explained_variance | 0.822    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12099    |\n","|    policy_loss        | 0.0642   |\n","|    value_loss         | 0.061    |\n","------------------------------------\n","Eval num_timesteps=970000, episode_reward=290.00 +/- 33.91\n","Episode length: 1501.80 +/- 259.40\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.5e+03  |\n","|    mean_reward        | 290      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.44e+03 |\n","|    ep_rew_mean        | 296      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 12200    |\n","|    time_elapsed       | 2148     |\n","|    total_timesteps    | 976000   |\n","| train/                |          |\n","|    entropy_loss       | -1.67    |\n","|    explained_variance | 0.923    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12199    |\n","|    policy_loss        | -0.0304  |\n","|    value_loss         | 0.0352   |\n","------------------------------------\n","Eval num_timesteps=980000, episode_reward=280.00 +/- 67.82\n","Episode length: 1493.80 +/- 97.70\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.49e+03 |\n","|    mean_reward        | 280      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.41e+03 |\n","|    ep_rew_mean        | 290      |\n","| time/                 |          |\n","|    fps                | 454      |\n","|    iterations         | 12300    |\n","|    time_elapsed       | 2167     |\n","|    total_timesteps    | 984000   |\n","| train/                |          |\n","|    entropy_loss       | -1.63    |\n","|    explained_variance | 0.967    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12299    |\n","|    policy_loss        | 0.0208   |\n","|    value_loss         | 0.0327   |\n","------------------------------------\n","Eval num_timesteps=990000, episode_reward=415.00 +/- 123.09\n","Episode length: 1744.80 +/- 222.89\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.74e+03 |\n","|    mean_reward        | 415      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.41e+03 |\n","|    ep_rew_mean        | 298      |\n","| time/                 |          |\n","|    fps                | 453      |\n","|    iterations         | 12400    |\n","|    time_elapsed       | 2186     |\n","|    total_timesteps    | 992000   |\n","| train/                |          |\n","|    entropy_loss       | -1.68    |\n","|    explained_variance | 0.736    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12399    |\n","|    policy_loss        | 0.0525   |\n","|    value_loss         | 0.0657   |\n","------------------------------------\n","Eval num_timesteps=1000000, episode_reward=340.00 +/- 25.50\n","Episode length: 1562.60 +/- 262.62\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.56e+03 |\n","|    mean_reward        | 340      |\n","| rollout/              |          |\n","|    ep_len_mean        | 1.45e+03 |\n","|    ep_rew_mean        | 318      |\n","| time/                 |          |\n","|    fps                | 453      |\n","|    iterations         | 12500    |\n","|    time_elapsed       | 2205     |\n","|    total_timesteps    | 1000000  |\n","| train/                |          |\n","|    entropy_loss       | -1.48    |\n","|    explained_variance | 0.92     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12499    |\n","|    policy_loss        | 0.0178   |\n","|    value_loss         | 0.0882   |\n","------------------------------------\n","Saving to logs/a2c/QbertNoFrameskip-v4_1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4Tm6ghfRAnyX"},"source":["#### Evaluate trained agent\n","\n","\n","You can remove the `--folder logs/` to evaluate pretrained agent."]},{"cell_type":"code","metadata":{"id":"t22hBTOzAnyX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625078963221,"user_tz":-120,"elapsed":19895,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"af1fd55c-f134-409f-f3a4-b410143b8ce2"},"source":["!python enjoy.py --algo a2c --env QbertNoFrameskip-v4 --no-render --n-timesteps 5000 --folder logs/"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Loading latest experiment, id=1\n","Loading logs/a2c/QbertNoFrameskip-v4_1/QbertNoFrameskip-v4.zip\n","Stacking 4 frames\n","Wrapping the env in a VecTransposeImage.\n","Atari Episode Score: 350.00\n","Atari Episode Length 1690\n","Atari Episode Score: 400.00\n","Atari Episode Length 1710\n","Atari Episode Score: 300.00\n","Atari Episode Length 1365\n","Atari Episode Score: 275.00\n","Atari Episode Length 1199\n","Atari Episode Score: 300.00\n","Atari Episode Length 1613\n","Atari Episode Score: 250.00\n","Atari Episode Length 1392\n","Atari Episode Score: 250.00\n","Atari Episode Length 1250\n","Atari Episode Score: 300.00\n","Atari Episode Length 1437\n","Atari Episode Score: 350.00\n","Atari Episode Length 1420\n","Atari Episode Score: 275.00\n","Atari Episode Length 1310\n","Atari Episode Score: 275.00\n","Atari Episode Length 1240\n","Atari Episode Score: 400.00\n","Atari Episode Length 1795\n","Atari Episode Score: 350.00\n","Atari Episode Length 1235\n","Atari Episode Score: 250.00\n","Atari Episode Length 1225\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o4gLvXt6AnyX"},"source":["### Record  a Video"]},{"cell_type":"code","metadata":{"id":"Vz3cxJ6JAnyX","executionInfo":{"status":"ok","timestamp":1625078963221,"user_tz":-120,"elapsed":19,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}}},"source":["# Set up display; otherwise rendering will fail\n","import os\n","os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n","os.environ['DISPLAY'] = ':1'"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"pukXWsJlAnyX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625078975685,"user_tz":-120,"elapsed":12481,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"e3e2184f-37c5-4874-8a8d-9c4ec1a96dcb"},"source":["!python -m utils.record_video --algo a2c --env QbertNoFrameskip-v4 --exp-id 0 -f logs/ -n 1000"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Loading latest experiment, id=1\n","Stacking 4 frames\n","Saving video to /content/drive/My Drive/Colab Notebooks/TFM/Seaquest/rl-baselines3-zoo/logs/a2c/QbertNoFrameskip-v4_1/videos/final-model-a2c-QbertNoFrameskip-v4-step-0-to-step-1000.mp4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qbprQDFGBCW9"},"source":["## Train an SpaceInvadersNoFrameskip-v4 DQN\n","\n","Steps: 1M\n"]},{"cell_type":"code","metadata":{"id":"NlqVA3ztBCW_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625081789881,"user_tz":-120,"elapsed":2561662,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"c4da25c5-d818-473a-a87c-22d1442eaade"},"source":["!python train.py --algo a2c --env SpaceInvadersNoFrameskip-v4 --n-timesteps 1000000 -tb logs"],"execution_count":18,"outputs":[{"output_type":"stream","text":["========== SpaceInvadersNoFrameskip-v4 ==========\n","Seed: 1537782062\n","Default hyperparameters for environment (ones being tuned will be overridden):\n","OrderedDict([('ent_coef', 0.01),\n","             ('env_wrapper',\n","              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n","             ('frame_stack', 4),\n","             ('n_envs', 16),\n","             ('n_timesteps', 10000000.0),\n","             ('policy', 'CnnPolicy'),\n","             ('policy_kwargs',\n","              'dict(optimizer_class=RMSpropTFLike, '\n","              'optimizer_kwargs=dict(eps=1e-5))'),\n","             ('vf_coef', 0.25)])\n","Using 16 environments\n","Overwriting n_timesteps with n=1000000\n","Creating test environment\n","Stacking 4 frames\n","Wrapping into a VecTransposeImage\n","Stacking 4 frames\n","Wrapping into a VecTransposeImage\n","Using cuda device\n","Log path: logs/a2c/SpaceInvadersNoFrameskip-v4_1\n","2021-06-30 18:54:03.343127: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","Logging to logs/SpaceInvadersNoFrameskip-v4/A2C_1\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 1.57e+03 |\n","|    ep_rew_mean        | 110      |\n","| time/                 |          |\n","|    fps                | 518      |\n","|    iterations         | 100      |\n","|    time_elapsed       | 15       |\n","|    total_timesteps    | 8000     |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.0208   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 99       |\n","|    policy_loss        | 0.2      |\n","|    value_loss         | 0.112    |\n","------------------------------------\n","Eval num_timesteps=10000, episode_reward=139.00 +/- 62.80\n","Episode length: 2026.60 +/- 537.21\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.03e+03 |\n","|    mean_reward        | 139      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2e+03    |\n","|    ep_rew_mean        | 138      |\n","| time/                 |          |\n","|    fps                | 445      |\n","|    iterations         | 200      |\n","|    time_elapsed       | 35       |\n","|    total_timesteps    | 16000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.65     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 199      |\n","|    policy_loss        | 0.0153   |\n","|    value_loss         | 0.000733 |\n","------------------------------------\n","Eval num_timesteps=20000, episode_reward=266.00 +/- 155.42\n","Episode length: 3096.20 +/- 1182.33\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.1e+03  |\n","|    mean_reward        | 266      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.06e+03 |\n","|    ep_rew_mean        | 147      |\n","| time/                 |          |\n","|    fps                | 401      |\n","|    iterations         | 300      |\n","|    time_elapsed       | 59       |\n","|    total_timesteps    | 24000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.0768   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 299      |\n","|    policy_loss        | 0.185    |\n","|    value_loss         | 0.105    |\n","------------------------------------\n","Eval num_timesteps=30000, episode_reward=153.00 +/- 89.36\n","Episode length: 2235.80 +/- 916.59\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.24e+03 |\n","|    mean_reward        | 153      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.1e+03  |\n","|    ep_rew_mean        | 146      |\n","| time/                 |          |\n","|    fps                | 396      |\n","|    iterations         | 400      |\n","|    time_elapsed       | 80       |\n","|    total_timesteps    | 32000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | -0.111   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 399      |\n","|    policy_loss        | 0.0266   |\n","|    value_loss         | 0.0275   |\n","------------------------------------\n","Eval num_timesteps=40000, episode_reward=136.00 +/- 142.35\n","Episode length: 1986.60 +/- 1159.06\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.99e+03 |\n","|    mean_reward        | 136      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.11e+03 |\n","|    ep_rew_mean        | 148      |\n","| time/                 |          |\n","|    fps                | 397      |\n","|    iterations         | 500      |\n","|    time_elapsed       | 100      |\n","|    total_timesteps    | 40000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.0533   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 499      |\n","|    policy_loss        | 0.181    |\n","|    value_loss         | 0.168    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.14e+03 |\n","|    ep_rew_mean        | 154      |\n","| time/                 |          |\n","|    fps                | 419      |\n","|    iterations         | 600      |\n","|    time_elapsed       | 114      |\n","|    total_timesteps    | 48000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.0753   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 599      |\n","|    policy_loss        | -0.104   |\n","|    value_loss         | 0.159    |\n","------------------------------------\n","Eval num_timesteps=50000, episode_reward=151.00 +/- 88.28\n","Episode length: 2036.20 +/- 597.45\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.04e+03 |\n","|    mean_reward        | 151      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.15e+03 |\n","|    ep_rew_mean        | 156      |\n","| time/                 |          |\n","|    fps                | 415      |\n","|    iterations         | 700      |\n","|    time_elapsed       | 134      |\n","|    total_timesteps    | 56000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.51     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 699      |\n","|    policy_loss        | -0.0537  |\n","|    value_loss         | 0.0251   |\n","------------------------------------\n","Eval num_timesteps=60000, episode_reward=148.00 +/- 57.32\n","Episode length: 2210.20 +/- 739.38\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.21e+03 |\n","|    mean_reward        | 148      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.19e+03 |\n","|    ep_rew_mean        | 159      |\n","| time/                 |          |\n","|    fps                | 412      |\n","|    iterations         | 800      |\n","|    time_elapsed       | 155      |\n","|    total_timesteps    | 64000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | -0.377   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 799      |\n","|    policy_loss        | -0.202   |\n","|    value_loss         | 0.164    |\n","------------------------------------\n","Eval num_timesteps=70000, episode_reward=191.00 +/- 112.89\n","Episode length: 2287.00 +/- 458.27\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.29e+03 |\n","|    mean_reward        | 191      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.16e+03 |\n","|    ep_rew_mean        | 159      |\n","| time/                 |          |\n","|    fps                | 408      |\n","|    iterations         | 900      |\n","|    time_elapsed       | 176      |\n","|    total_timesteps    | 72000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.943    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 899      |\n","|    policy_loss        | -0.0752  |\n","|    value_loss         | 0.00683  |\n","------------------------------------\n","Eval num_timesteps=80000, episode_reward=133.00 +/- 52.12\n","Episode length: 2122.60 +/- 262.37\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.12e+03 |\n","|    mean_reward        | 133      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.11e+03 |\n","|    ep_rew_mean        | 150      |\n","| time/                 |          |\n","|    fps                | 406      |\n","|    iterations         | 1000     |\n","|    time_elapsed       | 196      |\n","|    total_timesteps    | 80000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.639    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 999      |\n","|    policy_loss        | 0.128    |\n","|    value_loss         | 0.0906   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.16e+03 |\n","|    ep_rew_mean        | 156      |\n","| time/                 |          |\n","|    fps                | 418      |\n","|    iterations         | 1100     |\n","|    time_elapsed       | 210      |\n","|    total_timesteps    | 88000    |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.968    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1099     |\n","|    policy_loss        | -0.0717  |\n","|    value_loss         | 0.00638  |\n","------------------------------------\n","Eval num_timesteps=90000, episode_reward=112.00 +/- 54.09\n","Episode length: 1663.40 +/- 421.87\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.66e+03 |\n","|    mean_reward        | 112      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.1e+03  |\n","|    ep_rew_mean        | 149      |\n","| time/                 |          |\n","|    fps                | 418      |\n","|    iterations         | 1200     |\n","|    time_elapsed       | 229      |\n","|    total_timesteps    | 96000    |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.424    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1199     |\n","|    policy_loss        | 0.0108   |\n","|    value_loss         | 0.143    |\n","------------------------------------\n","Eval num_timesteps=100000, episode_reward=139.00 +/- 31.69\n","Episode length: 2281.80 +/- 216.74\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.28e+03 |\n","|    mean_reward        | 139      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.06e+03 |\n","|    ep_rew_mean        | 142      |\n","| time/                 |          |\n","|    fps                | 415      |\n","|    iterations         | 1300     |\n","|    time_elapsed       | 250      |\n","|    total_timesteps    | 104000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.787    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1299     |\n","|    policy_loss        | 0.00881  |\n","|    value_loss         | 0.0501   |\n","------------------------------------\n","Eval num_timesteps=110000, episode_reward=86.00 +/- 27.46\n","Episode length: 1673.00 +/- 337.80\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.67e+03 |\n","|    mean_reward        | 86       |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.1e+03  |\n","|    ep_rew_mean        | 140      |\n","| time/                 |          |\n","|    fps                | 415      |\n","|    iterations         | 1400     |\n","|    time_elapsed       | 269      |\n","|    total_timesteps    | 112000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.814    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1399     |\n","|    policy_loss        | -0.0139  |\n","|    value_loss         | 0.0587   |\n","------------------------------------\n","Eval num_timesteps=120000, episode_reward=122.00 +/- 55.46\n","Episode length: 2059.00 +/- 511.87\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.06e+03 |\n","|    mean_reward        | 122      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.12e+03 |\n","|    ep_rew_mean        | 146      |\n","| time/                 |          |\n","|    fps                | 413      |\n","|    iterations         | 1500     |\n","|    time_elapsed       | 289      |\n","|    total_timesteps    | 120000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.858    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1499     |\n","|    policy_loss        | -0.0372  |\n","|    value_loss         | 0.0239   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.1e+03  |\n","|    ep_rew_mean        | 148      |\n","| time/                 |          |\n","|    fps                | 421      |\n","|    iterations         | 1600     |\n","|    time_elapsed       | 303      |\n","|    total_timesteps    | 128000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | -0.056   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1599     |\n","|    policy_loss        | -0.361   |\n","|    value_loss         | 0.22     |\n","------------------------------------\n","Eval num_timesteps=130000, episode_reward=200.00 +/- 170.76\n","Episode length: 2071.00 +/- 839.05\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.07e+03 |\n","|    mean_reward        | 200      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.07e+03 |\n","|    ep_rew_mean        | 149      |\n","| time/                 |          |\n","|    fps                | 419      |\n","|    iterations         | 1700     |\n","|    time_elapsed       | 324      |\n","|    total_timesteps    | 136000   |\n","| train/                |          |\n","|    entropy_loss       | -1.79    |\n","|    explained_variance | 0.768    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1699     |\n","|    policy_loss        | 0.0557   |\n","|    value_loss         | 0.0593   |\n","------------------------------------\n","Eval num_timesteps=140000, episode_reward=92.00 +/- 29.09\n","Episode length: 1801.00 +/- 539.40\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.8e+03  |\n","|    mean_reward        | 92       |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.1e+03  |\n","|    ep_rew_mean        | 154      |\n","| time/                 |          |\n","|    fps                | 418      |\n","|    iterations         | 1800     |\n","|    time_elapsed       | 344      |\n","|    total_timesteps    | 144000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.751    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1799     |\n","|    policy_loss        | 0.0425   |\n","|    value_loss         | 0.0366   |\n","------------------------------------\n","Eval num_timesteps=150000, episode_reward=177.00 +/- 91.58\n","Episode length: 2237.80 +/- 540.87\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.24e+03 |\n","|    mean_reward        | 177      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.16e+03 |\n","|    ep_rew_mean        | 166      |\n","| time/                 |          |\n","|    fps                | 415      |\n","|    iterations         | 1900     |\n","|    time_elapsed       | 365      |\n","|    total_timesteps    | 152000   |\n","| train/                |          |\n","|    entropy_loss       | -1.77    |\n","|    explained_variance | 0.742    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1899     |\n","|    policy_loss        | -0.0588  |\n","|    value_loss         | 0.172    |\n","------------------------------------\n","Eval num_timesteps=160000, episode_reward=149.00 +/- 45.43\n","Episode length: 2182.20 +/- 226.58\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.18e+03 |\n","|    mean_reward        | 149      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.16e+03 |\n","|    ep_rew_mean        | 175      |\n","| time/                 |          |\n","|    fps                | 413      |\n","|    iterations         | 2000     |\n","|    time_elapsed       | 386      |\n","|    total_timesteps    | 160000   |\n","| train/                |          |\n","|    entropy_loss       | -1.77    |\n","|    explained_variance | 0.744    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1999     |\n","|    policy_loss        | 0.0476   |\n","|    value_loss         | 0.0558   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.18e+03 |\n","|    ep_rew_mean        | 181      |\n","| time/                 |          |\n","|    fps                | 419      |\n","|    iterations         | 2100     |\n","|    time_elapsed       | 400      |\n","|    total_timesteps    | 168000   |\n","| train/                |          |\n","|    entropy_loss       | -1.76    |\n","|    explained_variance | 0.901    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2099     |\n","|    policy_loss        | 0.00449  |\n","|    value_loss         | 0.0474   |\n","------------------------------------\n","Eval num_timesteps=170000, episode_reward=157.00 +/- 54.37\n","Episode length: 2083.80 +/- 433.38\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.08e+03 |\n","|    mean_reward        | 157      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.2e+03  |\n","|    ep_rew_mean        | 182      |\n","| time/                 |          |\n","|    fps                | 418      |\n","|    iterations         | 2200     |\n","|    time_elapsed       | 421      |\n","|    total_timesteps    | 176000   |\n","| train/                |          |\n","|    entropy_loss       | -1.73    |\n","|    explained_variance | 0.99     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2199     |\n","|    policy_loss        | -0.125   |\n","|    value_loss         | 0.0101   |\n","------------------------------------\n","Eval num_timesteps=180000, episode_reward=102.00 +/- 45.23\n","Episode length: 1937.80 +/- 476.56\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 1.94e+03 |\n","|    mean_reward        | 102      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.25e+03 |\n","|    ep_rew_mean        | 190      |\n","| time/                 |          |\n","|    fps                | 417      |\n","|    iterations         | 2300     |\n","|    time_elapsed       | 440      |\n","|    total_timesteps    | 184000   |\n","| train/                |          |\n","|    entropy_loss       | -1.76    |\n","|    explained_variance | 0.91     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2299     |\n","|    policy_loss        | 0.0587   |\n","|    value_loss         | 0.0698   |\n","------------------------------------\n","Eval num_timesteps=190000, episode_reward=153.00 +/- 38.42\n","Episode length: 2314.60 +/- 219.72\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.31e+03 |\n","|    mean_reward        | 153      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.29e+03 |\n","|    ep_rew_mean        | 195      |\n","| time/                 |          |\n","|    fps                | 415      |\n","|    iterations         | 2400     |\n","|    time_elapsed       | 462      |\n","|    total_timesteps    | 192000   |\n","| train/                |          |\n","|    entropy_loss       | -1.72    |\n","|    explained_variance | 0.954    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2399     |\n","|    policy_loss        | -0.0991  |\n","|    value_loss         | 0.0316   |\n","------------------------------------\n","Eval num_timesteps=200000, episode_reward=133.00 +/- 25.42\n","Episode length: 2032.60 +/- 316.15\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.03e+03 |\n","|    mean_reward        | 133      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.33e+03 |\n","|    ep_rew_mean        | 193      |\n","| time/                 |          |\n","|    fps                | 414      |\n","|    iterations         | 2500     |\n","|    time_elapsed       | 482      |\n","|    total_timesteps    | 200000   |\n","| train/                |          |\n","|    entropy_loss       | -1.77    |\n","|    explained_variance | 0.896    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2499     |\n","|    policy_loss        | -0.0078  |\n","|    value_loss         | 0.0443   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.28e+03 |\n","|    ep_rew_mean        | 183      |\n","| time/                 |          |\n","|    fps                | 419      |\n","|    iterations         | 2600     |\n","|    time_elapsed       | 496      |\n","|    total_timesteps    | 208000   |\n","| train/                |          |\n","|    entropy_loss       | -1.74    |\n","|    explained_variance | 0.945    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2599     |\n","|    policy_loss        | -0.0199  |\n","|    value_loss         | 0.0247   |\n","------------------------------------\n","Eval num_timesteps=210000, episode_reward=184.00 +/- 105.94\n","Episode length: 2471.00 +/- 516.55\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.47e+03 |\n","|    mean_reward        | 184      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.28e+03 |\n","|    ep_rew_mean        | 179      |\n","| time/                 |          |\n","|    fps                | 417      |\n","|    iterations         | 2700     |\n","|    time_elapsed       | 517      |\n","|    total_timesteps    | 216000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.818    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2699     |\n","|    policy_loss        | 0.106    |\n","|    value_loss         | 0.0759   |\n","------------------------------------\n","Eval num_timesteps=220000, episode_reward=219.00 +/- 117.87\n","Episode length: 2331.00 +/- 342.05\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.33e+03 |\n","|    mean_reward        | 219      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.28e+03 |\n","|    ep_rew_mean        | 173      |\n","| time/                 |          |\n","|    fps                | 415      |\n","|    iterations         | 2800     |\n","|    time_elapsed       | 538      |\n","|    total_timesteps    | 224000   |\n","| train/                |          |\n","|    entropy_loss       | -1.78    |\n","|    explained_variance | 0.729    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2799     |\n","|    policy_loss        | -0.143   |\n","|    value_loss         | 0.111    |\n","------------------------------------\n","Eval num_timesteps=230000, episode_reward=125.00 +/- 44.83\n","Episode length: 2071.00 +/- 525.70\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.07e+03 |\n","|    mean_reward        | 125      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.29e+03 |\n","|    ep_rew_mean        | 177      |\n","| time/                 |          |\n","|    fps                | 415      |\n","|    iterations         | 2900     |\n","|    time_elapsed       | 558      |\n","|    total_timesteps    | 232000   |\n","| train/                |          |\n","|    entropy_loss       | -1.77    |\n","|    explained_variance | 0.727    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2899     |\n","|    policy_loss        | 0.0855   |\n","|    value_loss         | 0.0647   |\n","------------------------------------\n","Eval num_timesteps=240000, episode_reward=137.00 +/- 48.74\n","Episode length: 2188.20 +/- 469.61\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.19e+03 |\n","|    mean_reward        | 137      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.28e+03 |\n","|    ep_rew_mean        | 172      |\n","| time/                 |          |\n","|    fps                | 414      |\n","|    iterations         | 3000     |\n","|    time_elapsed       | 579      |\n","|    total_timesteps    | 240000   |\n","| train/                |          |\n","|    entropy_loss       | -1.75    |\n","|    explained_variance | 0.802    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2999     |\n","|    policy_loss        | 0.0259   |\n","|    value_loss         | 0.0915   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.31e+03 |\n","|    ep_rew_mean        | 177      |\n","| time/                 |          |\n","|    fps                | 418      |\n","|    iterations         | 3100     |\n","|    time_elapsed       | 593      |\n","|    total_timesteps    | 248000   |\n","| train/                |          |\n","|    entropy_loss       | -1.74    |\n","|    explained_variance | 0.863    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3099     |\n","|    policy_loss        | -0.182   |\n","|    value_loss         | 0.102    |\n","------------------------------------\n","Eval num_timesteps=250000, episode_reward=204.00 +/- 48.93\n","Episode length: 2582.60 +/- 187.13\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.58e+03 |\n","|    mean_reward        | 204      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.33e+03 |\n","|    ep_rew_mean        | 181      |\n","| time/                 |          |\n","|    fps                | 416      |\n","|    iterations         | 3200     |\n","|    time_elapsed       | 615      |\n","|    total_timesteps    | 256000   |\n","| train/                |          |\n","|    entropy_loss       | -1.72    |\n","|    explained_variance | 0.987    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3199     |\n","|    policy_loss        | -0.0642  |\n","|    value_loss         | 0.0132   |\n","------------------------------------\n","Eval num_timesteps=260000, episode_reward=183.00 +/- 25.81\n","Episode length: 2557.00 +/- 201.99\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.56e+03 |\n","|    mean_reward        | 183      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.4e+03  |\n","|    ep_rew_mean        | 190      |\n","| time/                 |          |\n","|    fps                | 414      |\n","|    iterations         | 3300     |\n","|    time_elapsed       | 636      |\n","|    total_timesteps    | 264000   |\n","| train/                |          |\n","|    entropy_loss       | -1.65    |\n","|    explained_variance | 0.989    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3299     |\n","|    policy_loss        | -0.0735  |\n","|    value_loss         | 0.0111   |\n","------------------------------------\n","Eval num_timesteps=270000, episode_reward=269.00 +/- 88.45\n","Episode length: 2686.60 +/- 288.43\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.69e+03 |\n","|    mean_reward        | 269      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.5e+03  |\n","|    ep_rew_mean        | 198      |\n","| time/                 |          |\n","|    fps                | 412      |\n","|    iterations         | 3400     |\n","|    time_elapsed       | 659      |\n","|    total_timesteps    | 272000   |\n","| train/                |          |\n","|    entropy_loss       | -1.62    |\n","|    explained_variance | 0.952    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3399     |\n","|    policy_loss        | 0.0466   |\n","|    value_loss         | 0.0584   |\n","------------------------------------\n","Eval num_timesteps=280000, episode_reward=165.00 +/- 34.35\n","Episode length: 2393.40 +/- 210.86\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.39e+03 |\n","|    mean_reward        | 165      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.53e+03 |\n","|    ep_rew_mean        | 204      |\n","| time/                 |          |\n","|    fps                | 411      |\n","|    iterations         | 3500     |\n","|    time_elapsed       | 680      |\n","|    total_timesteps    | 280000   |\n","| train/                |          |\n","|    entropy_loss       | -1.69    |\n","|    explained_variance | 0.973    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3499     |\n","|    policy_loss        | 0.00625  |\n","|    value_loss         | 0.0139   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.58e+03 |\n","|    ep_rew_mean        | 210      |\n","| time/                 |          |\n","|    fps                | 414      |\n","|    iterations         | 3600     |\n","|    time_elapsed       | 694      |\n","|    total_timesteps    | 288000   |\n","| train/                |          |\n","|    entropy_loss       | -1.63    |\n","|    explained_variance | 0.94     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3599     |\n","|    policy_loss        | 0.108    |\n","|    value_loss         | 0.0561   |\n","------------------------------------\n","Eval num_timesteps=290000, episode_reward=188.00 +/- 53.16\n","Episode length: 2491.00 +/- 327.74\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.49e+03 |\n","|    mean_reward        | 188      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.63e+03 |\n","|    ep_rew_mean        | 219      |\n","| time/                 |          |\n","|    fps                | 413      |\n","|    iterations         | 3700     |\n","|    time_elapsed       | 715      |\n","|    total_timesteps    | 296000   |\n","| train/                |          |\n","|    entropy_loss       | -1.63    |\n","|    explained_variance | 0.948    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3699     |\n","|    policy_loss        | 0.0847   |\n","|    value_loss         | 0.0361   |\n","------------------------------------\n","Eval num_timesteps=300000, episode_reward=237.00 +/- 38.42\n","Episode length: 2856.60 +/- 238.73\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.86e+03 |\n","|    mean_reward        | 237      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.66e+03 |\n","|    ep_rew_mean        | 226      |\n","| time/                 |          |\n","|    fps                | 411      |\n","|    iterations         | 3800     |\n","|    time_elapsed       | 738      |\n","|    total_timesteps    | 304000   |\n","| train/                |          |\n","|    entropy_loss       | -1.67    |\n","|    explained_variance | 0.977    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3799     |\n","|    policy_loss        | -0.0186  |\n","|    value_loss         | 0.0279   |\n","------------------------------------\n","Eval num_timesteps=310000, episode_reward=196.00 +/- 25.18\n","Episode length: 2578.60 +/- 155.09\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.58e+03 |\n","|    mean_reward        | 196      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.66e+03 |\n","|    ep_rew_mean        | 224      |\n","| time/                 |          |\n","|    fps                | 410      |\n","|    iterations         | 3900     |\n","|    time_elapsed       | 760      |\n","|    total_timesteps    | 312000   |\n","| train/                |          |\n","|    entropy_loss       | -1.67    |\n","|    explained_variance | 0.982    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3899     |\n","|    policy_loss        | 0.05     |\n","|    value_loss         | 0.0157   |\n","------------------------------------\n","Eval num_timesteps=320000, episode_reward=255.00 +/- 73.35\n","Episode length: 2593.80 +/- 156.27\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.59e+03 |\n","|    mean_reward        | 255      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.68e+03 |\n","|    ep_rew_mean        | 223      |\n","| time/                 |          |\n","|    fps                | 409      |\n","|    iterations         | 4000     |\n","|    time_elapsed       | 781      |\n","|    total_timesteps    | 320000   |\n","| train/                |          |\n","|    entropy_loss       | -1.64    |\n","|    explained_variance | 0.967    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3999     |\n","|    policy_loss        | 0.00584  |\n","|    value_loss         | 0.0449   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.67e+03 |\n","|    ep_rew_mean        | 223      |\n","| time/                 |          |\n","|    fps                | 412      |\n","|    iterations         | 4100     |\n","|    time_elapsed       | 795      |\n","|    total_timesteps    | 328000   |\n","| train/                |          |\n","|    entropy_loss       | -1.6     |\n","|    explained_variance | 0.901    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4099     |\n","|    policy_loss        | 0.0757   |\n","|    value_loss         | 0.0832   |\n","------------------------------------\n","Eval num_timesteps=330000, episode_reward=222.00 +/- 69.61\n","Episode length: 2485.80 +/- 46.14\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.49e+03 |\n","|    mean_reward        | 222      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.65e+03 |\n","|    ep_rew_mean        | 222      |\n","| time/                 |          |\n","|    fps                | 411      |\n","|    iterations         | 4200     |\n","|    time_elapsed       | 816      |\n","|    total_timesteps    | 336000   |\n","| train/                |          |\n","|    entropy_loss       | -1.7     |\n","|    explained_variance | 0.98     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4199     |\n","|    policy_loss        | 0.0123   |\n","|    value_loss         | 0.0125   |\n","------------------------------------\n","Eval num_timesteps=340000, episode_reward=269.00 +/- 126.43\n","Episode length: 2884.60 +/- 637.92\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.88e+03 |\n","|    mean_reward        | 269      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.67e+03 |\n","|    ep_rew_mean        | 226      |\n","| time/                 |          |\n","|    fps                | 409      |\n","|    iterations         | 4300     |\n","|    time_elapsed       | 839      |\n","|    total_timesteps    | 344000   |\n","| train/                |          |\n","|    entropy_loss       | -1.6     |\n","|    explained_variance | 0.907    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4299     |\n","|    policy_loss        | 0.0534   |\n","|    value_loss         | 0.0944   |\n","------------------------------------\n","Eval num_timesteps=350000, episode_reward=197.00 +/- 36.00\n","Episode length: 2583.80 +/- 166.63\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.58e+03 |\n","|    mean_reward        | 197      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.67e+03 |\n","|    ep_rew_mean        | 229      |\n","| time/                 |          |\n","|    fps                | 408      |\n","|    iterations         | 4400     |\n","|    time_elapsed       | 861      |\n","|    total_timesteps    | 352000   |\n","| train/                |          |\n","|    entropy_loss       | -1.62    |\n","|    explained_variance | 0.987    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4399     |\n","|    policy_loss        | 0.0397   |\n","|    value_loss         | 0.0151   |\n","------------------------------------\n","Eval num_timesteps=360000, episode_reward=216.00 +/- 55.62\n","Episode length: 2829.80 +/- 578.85\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.83e+03 |\n","|    mean_reward        | 216      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.67e+03 |\n","|    ep_rew_mean        | 228      |\n","| time/                 |          |\n","|    fps                | 407      |\n","|    iterations         | 4500     |\n","|    time_elapsed       | 883      |\n","|    total_timesteps    | 360000   |\n","| train/                |          |\n","|    entropy_loss       | -1.67    |\n","|    explained_variance | 0.944    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4499     |\n","|    policy_loss        | -0.039   |\n","|    value_loss         | 0.0622   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.66e+03 |\n","|    ep_rew_mean        | 224      |\n","| time/                 |          |\n","|    fps                | 410      |\n","|    iterations         | 4600     |\n","|    time_elapsed       | 897      |\n","|    total_timesteps    | 368000   |\n","| train/                |          |\n","|    entropy_loss       | -1.62    |\n","|    explained_variance | 0.835    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4599     |\n","|    policy_loss        | -0.236   |\n","|    value_loss         | 0.255    |\n","------------------------------------\n","Eval num_timesteps=370000, episode_reward=176.00 +/- 50.14\n","Episode length: 2489.40 +/- 304.14\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.49e+03 |\n","|    mean_reward        | 176      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.65e+03 |\n","|    ep_rew_mean        | 224      |\n","| time/                 |          |\n","|    fps                | 409      |\n","|    iterations         | 4700     |\n","|    time_elapsed       | 918      |\n","|    total_timesteps    | 376000   |\n","| train/                |          |\n","|    entropy_loss       | -1.58    |\n","|    explained_variance | 0.948    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4699     |\n","|    policy_loss        | -0.0178  |\n","|    value_loss         | 0.0779   |\n","------------------------------------\n","Eval num_timesteps=380000, episode_reward=213.00 +/- 38.42\n","Episode length: 2605.00 +/- 162.45\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.60e+03 |\n","|    mean_reward        | 213      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.66e+03 |\n","|    ep_rew_mean        | 231      |\n","| time/                 |          |\n","|    fps                | 408      |\n","|    iterations         | 4800     |\n","|    time_elapsed       | 940      |\n","|    total_timesteps    | 384000   |\n","| train/                |          |\n","|    entropy_loss       | -1.49    |\n","|    explained_variance | 0.987    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4799     |\n","|    policy_loss        | -0.0179  |\n","|    value_loss         | 0.0236   |\n","------------------------------------\n","Eval num_timesteps=390000, episode_reward=296.00 +/- 86.39\n","Episode length: 2663.80 +/- 263.61\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.66e+03 |\n","|    mean_reward        | 296      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.66e+03 |\n","|    ep_rew_mean        | 232      |\n","| time/                 |          |\n","|    fps                | 407      |\n","|    iterations         | 4900     |\n","|    time_elapsed       | 962      |\n","|    total_timesteps    | 392000   |\n","| train/                |          |\n","|    entropy_loss       | -1.5     |\n","|    explained_variance | 0.996    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4899     |\n","|    policy_loss        | 0.00389  |\n","|    value_loss         | 0.00694  |\n","------------------------------------\n","Eval num_timesteps=400000, episode_reward=220.00 +/- 20.00\n","Episode length: 2616.60 +/- 147.57\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.62e+03 |\n","|    mean_reward        | 220      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.65e+03 |\n","|    ep_rew_mean        | 239      |\n","| time/                 |          |\n","|    fps                | 406      |\n","|    iterations         | 5000     |\n","|    time_elapsed       | 984      |\n","|    total_timesteps    | 400000   |\n","| train/                |          |\n","|    entropy_loss       | -1.55    |\n","|    explained_variance | 0.984    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 4999     |\n","|    policy_loss        | -0.124   |\n","|    value_loss         | 0.0227   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.64e+03 |\n","|    ep_rew_mean        | 238      |\n","| time/                 |          |\n","|    fps                | 408      |\n","|    iterations         | 5100     |\n","|    time_elapsed       | 998      |\n","|    total_timesteps    | 408000   |\n","| train/                |          |\n","|    entropy_loss       | -1.64    |\n","|    explained_variance | 0.957    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5099     |\n","|    policy_loss        | 0.0692   |\n","|    value_loss         | 0.0372   |\n","------------------------------------\n","Eval num_timesteps=410000, episode_reward=168.00 +/- 59.55\n","Episode length: 2355.00 +/- 390.03\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.36e+03 |\n","|    mean_reward        | 168      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.66e+03 |\n","|    ep_rew_mean        | 239      |\n","| time/                 |          |\n","|    fps                | 407      |\n","|    iterations         | 5200     |\n","|    time_elapsed       | 1019     |\n","|    total_timesteps    | 416000   |\n","| train/                |          |\n","|    entropy_loss       | -1.56    |\n","|    explained_variance | 0.984    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5199     |\n","|    policy_loss        | -0.135   |\n","|    value_loss         | 0.0233   |\n","------------------------------------\n","Eval num_timesteps=420000, episode_reward=246.00 +/- 69.74\n","Episode length: 3087.40 +/- 675.74\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.09e+03 |\n","|    mean_reward        | 246      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.71e+03 |\n","|    ep_rew_mean        | 245      |\n","| time/                 |          |\n","|    fps                | 406      |\n","|    iterations         | 5300     |\n","|    time_elapsed       | 1042     |\n","|    total_timesteps    | 424000   |\n","| train/                |          |\n","|    entropy_loss       | -1.6     |\n","|    explained_variance | 0.979    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5299     |\n","|    policy_loss        | -0.0141  |\n","|    value_loss         | 0.0152   |\n","------------------------------------\n","Eval num_timesteps=430000, episode_reward=255.00 +/- 66.03\n","Episode length: 2610.20 +/- 165.40\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.61e+03 |\n","|    mean_reward        | 255      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.7e+03  |\n","|    ep_rew_mean        | 241      |\n","| time/                 |          |\n","|    fps                | 405      |\n","|    iterations         | 5400     |\n","|    time_elapsed       | 1064     |\n","|    total_timesteps    | 432000   |\n","| train/                |          |\n","|    entropy_loss       | -1.6     |\n","|    explained_variance | 0.99     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5399     |\n","|    policy_loss        | -0.0388  |\n","|    value_loss         | 0.0087   |\n","------------------------------------\n","Eval num_timesteps=440000, episode_reward=242.00 +/- 26.19\n","Episode length: 2823.00 +/- 193.90\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.82e+03 |\n","|    mean_reward        | 242      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.75e+03 |\n","|    ep_rew_mean        | 244      |\n","| time/                 |          |\n","|    fps                | 404      |\n","|    iterations         | 5500     |\n","|    time_elapsed       | 1087     |\n","|    total_timesteps    | 440000   |\n","| train/                |          |\n","|    entropy_loss       | -1.6     |\n","|    explained_variance | 0.983    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5499     |\n","|    policy_loss        | 0.0856   |\n","|    value_loss         | 0.0288   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.76e+03 |\n","|    ep_rew_mean        | 236      |\n","| time/                 |          |\n","|    fps                | 406      |\n","|    iterations         | 5600     |\n","|    time_elapsed       | 1101     |\n","|    total_timesteps    | 448000   |\n","| train/                |          |\n","|    entropy_loss       | -1.6     |\n","|    explained_variance | 0.982    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5599     |\n","|    policy_loss        | -0.109   |\n","|    value_loss         | 0.027    |\n","------------------------------------\n","Eval num_timesteps=450000, episode_reward=199.00 +/- 15.62\n","Episode length: 3190.60 +/- 1206.74\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.19e+03 |\n","|    mean_reward        | 199      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.76e+03 |\n","|    ep_rew_mean        | 241      |\n","| time/                 |          |\n","|    fps                | 405      |\n","|    iterations         | 5700     |\n","|    time_elapsed       | 1124     |\n","|    total_timesteps    | 456000   |\n","| train/                |          |\n","|    entropy_loss       | -1.57    |\n","|    explained_variance | 0.988    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5699     |\n","|    policy_loss        | -0.0981  |\n","|    value_loss         | 0.0222   |\n","------------------------------------\n","Eval num_timesteps=460000, episode_reward=220.00 +/- 38.34\n","Episode length: 2681.40 +/- 216.42\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.68e+03 |\n","|    mean_reward        | 220      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.75e+03 |\n","|    ep_rew_mean        | 237      |\n","| time/                 |          |\n","|    fps                | 404      |\n","|    iterations         | 5800     |\n","|    time_elapsed       | 1146     |\n","|    total_timesteps    | 464000   |\n","| train/                |          |\n","|    entropy_loss       | -1.46    |\n","|    explained_variance | 0.972    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5799     |\n","|    policy_loss        | 0.0374   |\n","|    value_loss         | 0.0618   |\n","------------------------------------\n","Eval num_timesteps=470000, episode_reward=217.00 +/- 26.00\n","Episode length: 2792.60 +/- 287.09\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.79e+03 |\n","|    mean_reward        | 217      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.77e+03 |\n","|    ep_rew_mean        | 238      |\n","| time/                 |          |\n","|    fps                | 403      |\n","|    iterations         | 5900     |\n","|    time_elapsed       | 1169     |\n","|    total_timesteps    | 472000   |\n","| train/                |          |\n","|    entropy_loss       | -1.54    |\n","|    explained_variance | 0.993    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5899     |\n","|    policy_loss        | -0.0338  |\n","|    value_loss         | 0.00469  |\n","------------------------------------\n","Eval num_timesteps=480000, episode_reward=214.00 +/- 43.29\n","Episode length: 2725.00 +/- 242.35\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.72e+03 |\n","|    mean_reward        | 214      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.76e+03 |\n","|    ep_rew_mean        | 236      |\n","| time/                 |          |\n","|    fps                | 402      |\n","|    iterations         | 6000     |\n","|    time_elapsed       | 1191     |\n","|    total_timesteps    | 480000   |\n","| train/                |          |\n","|    entropy_loss       | -1.48    |\n","|    explained_variance | 0.996    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 5999     |\n","|    policy_loss        | -0.0118  |\n","|    value_loss         | 0.00817  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.78e+03 |\n","|    ep_rew_mean        | 237      |\n","| time/                 |          |\n","|    fps                | 404      |\n","|    iterations         | 6100     |\n","|    time_elapsed       | 1205     |\n","|    total_timesteps    | 488000   |\n","| train/                |          |\n","|    entropy_loss       | -1.51    |\n","|    explained_variance | 0.985    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6099     |\n","|    policy_loss        | 0.0152   |\n","|    value_loss         | 0.0151   |\n","------------------------------------\n","Eval num_timesteps=490000, episode_reward=246.00 +/- 31.37\n","Episode length: 2873.80 +/- 290.63\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.87e+03 |\n","|    mean_reward        | 246      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.77e+03 |\n","|    ep_rew_mean        | 235      |\n","| time/                 |          |\n","|    fps                | 404      |\n","|    iterations         | 6200     |\n","|    time_elapsed       | 1227     |\n","|    total_timesteps    | 496000   |\n","| train/                |          |\n","|    entropy_loss       | -1.49    |\n","|    explained_variance | 0.987    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6199     |\n","|    policy_loss        | 0.00343  |\n","|    value_loss         | 0.0225   |\n","------------------------------------\n","Eval num_timesteps=500000, episode_reward=226.00 +/- 47.16\n","Episode length: 2767.40 +/- 284.59\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.77e+03 |\n","|    mean_reward        | 226      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.75e+03 |\n","|    ep_rew_mean        | 234      |\n","| time/                 |          |\n","|    fps                | 403      |\n","|    iterations         | 6300     |\n","|    time_elapsed       | 1249     |\n","|    total_timesteps    | 504000   |\n","| train/                |          |\n","|    entropy_loss       | -1.56    |\n","|    explained_variance | 0.978    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6299     |\n","|    policy_loss        | -0.011   |\n","|    value_loss         | 0.0192   |\n","------------------------------------\n","Eval num_timesteps=510000, episode_reward=210.00 +/- 41.35\n","Episode length: 2624.60 +/- 169.18\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.62e+03 |\n","|    mean_reward        | 210      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.75e+03 |\n","|    ep_rew_mean        | 239      |\n","| time/                 |          |\n","|    fps                | 402      |\n","|    iterations         | 6400     |\n","|    time_elapsed       | 1271     |\n","|    total_timesteps    | 512000   |\n","| train/                |          |\n","|    entropy_loss       | -1.54    |\n","|    explained_variance | 0.98     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6399     |\n","|    policy_loss        | 0.0131   |\n","|    value_loss         | 0.0221   |\n","------------------------------------\n","Eval num_timesteps=520000, episode_reward=226.00 +/- 33.82\n","Episode length: 2742.60 +/- 214.39\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.74e+03 |\n","|    mean_reward        | 226      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.76e+03 |\n","|    ep_rew_mean        | 244      |\n","| time/                 |          |\n","|    fps                | 401      |\n","|    iterations         | 6500     |\n","|    time_elapsed       | 1293     |\n","|    total_timesteps    | 520000   |\n","| train/                |          |\n","|    entropy_loss       | -1.51    |\n","|    explained_variance | 0.977    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6499     |\n","|    policy_loss        | 0.0503   |\n","|    value_loss         | 0.0368   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.79e+03 |\n","|    ep_rew_mean        | 246      |\n","| time/                 |          |\n","|    fps                | 403      |\n","|    iterations         | 6600     |\n","|    time_elapsed       | 1307     |\n","|    total_timesteps    | 528000   |\n","| train/                |          |\n","|    entropy_loss       | -1.48    |\n","|    explained_variance | 0.99     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6599     |\n","|    policy_loss        | -0.0422  |\n","|    value_loss         | 0.00972  |\n","------------------------------------\n","Eval num_timesteps=530000, episode_reward=230.00 +/- 30.98\n","Episode length: 2793.80 +/- 163.10\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.79e+03 |\n","|    mean_reward        | 230      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 246      |\n","| time/                 |          |\n","|    fps                | 403      |\n","|    iterations         | 6700     |\n","|    time_elapsed       | 1329     |\n","|    total_timesteps    | 536000   |\n","| train/                |          |\n","|    entropy_loss       | -1.49    |\n","|    explained_variance | 0.966    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6699     |\n","|    policy_loss        | 0.0699   |\n","|    value_loss         | 0.0209   |\n","------------------------------------\n","Eval num_timesteps=540000, episode_reward=298.00 +/- 56.62\n","Episode length: 2882.60 +/- 162.13\n","New best mean reward!\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.88e+03 |\n","|    mean_reward        | 298      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 244      |\n","| time/                 |          |\n","|    fps                | 402      |\n","|    iterations         | 6800     |\n","|    time_elapsed       | 1352     |\n","|    total_timesteps    | 544000   |\n","| train/                |          |\n","|    entropy_loss       | -1.46    |\n","|    explained_variance | 0.969    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6799     |\n","|    policy_loss        | 0.0182   |\n","|    value_loss         | 0.0306   |\n","------------------------------------\n","Eval num_timesteps=550000, episode_reward=211.00 +/- 28.00\n","Episode length: 2712.20 +/- 234.44\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.71e+03 |\n","|    mean_reward        | 211      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.83e+03 |\n","|    ep_rew_mean        | 246      |\n","| time/                 |          |\n","|    fps                | 401      |\n","|    iterations         | 6900     |\n","|    time_elapsed       | 1374     |\n","|    total_timesteps    | 552000   |\n","| train/                |          |\n","|    entropy_loss       | -1.45    |\n","|    explained_variance | 0.982    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6899     |\n","|    policy_loss        | 0.00215  |\n","|    value_loss         | 0.0301   |\n","------------------------------------\n","Eval num_timesteps=560000, episode_reward=260.00 +/- 28.98\n","Episode length: 2958.60 +/- 233.49\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.96e+03 |\n","|    mean_reward        | 260      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.82e+03 |\n","|    ep_rew_mean        | 245      |\n","| time/                 |          |\n","|    fps                | 400      |\n","|    iterations         | 7000     |\n","|    time_elapsed       | 1397     |\n","|    total_timesteps    | 560000   |\n","| train/                |          |\n","|    entropy_loss       | -1.48    |\n","|    explained_variance | 0.982    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 6999     |\n","|    policy_loss        | 0.0389   |\n","|    value_loss         | 0.0132   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.82e+03 |\n","|    ep_rew_mean        | 246      |\n","| time/                 |          |\n","|    fps                | 402      |\n","|    iterations         | 7100     |\n","|    time_elapsed       | 1411     |\n","|    total_timesteps    | 568000   |\n","| train/                |          |\n","|    entropy_loss       | -1.42    |\n","|    explained_variance | 0.993    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7099     |\n","|    policy_loss        | -0.104   |\n","|    value_loss         | 0.0144   |\n","------------------------------------\n","Eval num_timesteps=570000, episode_reward=240.00 +/- 39.50\n","Episode length: 2891.40 +/- 257.68\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.89e+03 |\n","|    mean_reward        | 240      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.82e+03 |\n","|    ep_rew_mean        | 249      |\n","| time/                 |          |\n","|    fps                | 401      |\n","|    iterations         | 7200     |\n","|    time_elapsed       | 1434     |\n","|    total_timesteps    | 576000   |\n","| train/                |          |\n","|    entropy_loss       | -1.46    |\n","|    explained_variance | 0.992    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7199     |\n","|    policy_loss        | 0.0259   |\n","|    value_loss         | 0.011    |\n","------------------------------------\n","Eval num_timesteps=580000, episode_reward=236.00 +/- 37.60\n","Episode length: 2856.60 +/- 210.37\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.86e+03 |\n","|    mean_reward        | 236      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.84e+03 |\n","|    ep_rew_mean        | 250      |\n","| time/                 |          |\n","|    fps                | 400      |\n","|    iterations         | 7300     |\n","|    time_elapsed       | 1456     |\n","|    total_timesteps    | 584000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.956    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7299     |\n","|    policy_loss        | 0.0572   |\n","|    value_loss         | 0.0792   |\n","------------------------------------\n","Eval num_timesteps=590000, episode_reward=282.00 +/- 74.94\n","Episode length: 2922.20 +/- 263.36\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.92e+03 |\n","|    mean_reward        | 282      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 240      |\n","| time/                 |          |\n","|    fps                | 400      |\n","|    iterations         | 7400     |\n","|    time_elapsed       | 1479     |\n","|    total_timesteps    | 592000   |\n","| train/                |          |\n","|    entropy_loss       | -1.46    |\n","|    explained_variance | 0.988    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7399     |\n","|    policy_loss        | 0.00986  |\n","|    value_loss         | 0.0169   |\n","------------------------------------\n","Eval num_timesteps=600000, episode_reward=254.00 +/- 42.12\n","Episode length: 3110.60 +/- 554.28\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.11e+03 |\n","|    mean_reward        | 254      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.81e+03 |\n","|    ep_rew_mean        | 239      |\n","| time/                 |          |\n","|    fps                | 399      |\n","|    iterations         | 7500     |\n","|    time_elapsed       | 1502     |\n","|    total_timesteps    | 600000   |\n","| train/                |          |\n","|    entropy_loss       | -1.48    |\n","|    explained_variance | 0.991    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7499     |\n","|    policy_loss        | 0.0538   |\n","|    value_loss         | 0.0154   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.83e+03 |\n","|    ep_rew_mean        | 242      |\n","| time/                 |          |\n","|    fps                | 400      |\n","|    iterations         | 7600     |\n","|    time_elapsed       | 1516     |\n","|    total_timesteps    | 608000   |\n","| train/                |          |\n","|    entropy_loss       | -1.5     |\n","|    explained_variance | 0.959    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7599     |\n","|    policy_loss        | 0.0501   |\n","|    value_loss         | 0.0532   |\n","------------------------------------\n","Eval num_timesteps=610000, episode_reward=234.00 +/- 30.56\n","Episode length: 2962.20 +/- 197.44\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.96e+03 |\n","|    mean_reward        | 234      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 238      |\n","| time/                 |          |\n","|    fps                | 400      |\n","|    iterations         | 7700     |\n","|    time_elapsed       | 1539     |\n","|    total_timesteps    | 616000   |\n","| train/                |          |\n","|    entropy_loss       | -1.46    |\n","|    explained_variance | 0.933    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7699     |\n","|    policy_loss        | -0.00417 |\n","|    value_loss         | 0.0532   |\n","------------------------------------\n","Eval num_timesteps=620000, episode_reward=245.00 +/- 44.50\n","Episode length: 2875.40 +/- 265.80\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.88e+03 |\n","|    mean_reward        | 245      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 238      |\n","| time/                 |          |\n","|    fps                | 399      |\n","|    iterations         | 7800     |\n","|    time_elapsed       | 1562     |\n","|    total_timesteps    | 624000   |\n","| train/                |          |\n","|    entropy_loss       | -1.44    |\n","|    explained_variance | 0.947    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7799     |\n","|    policy_loss        | 0.0419   |\n","|    value_loss         | 0.035    |\n","------------------------------------\n","Eval num_timesteps=630000, episode_reward=258.00 +/- 26.19\n","Episode length: 2899.40 +/- 210.10\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.9e+03  |\n","|    mean_reward        | 258      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.81e+03 |\n","|    ep_rew_mean        | 239      |\n","| time/                 |          |\n","|    fps                | 398      |\n","|    iterations         | 7900     |\n","|    time_elapsed       | 1584     |\n","|    total_timesteps    | 632000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.988    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7899     |\n","|    policy_loss        | 0.0137   |\n","|    value_loss         | 0.0226   |\n","------------------------------------\n","Eval num_timesteps=640000, episode_reward=258.00 +/- 25.42\n","Episode length: 2872.20 +/- 160.82\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.87e+03 |\n","|    mean_reward        | 258      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 238      |\n","| time/                 |          |\n","|    fps                | 398      |\n","|    iterations         | 8000     |\n","|    time_elapsed       | 1607     |\n","|    total_timesteps    | 640000   |\n","| train/                |          |\n","|    entropy_loss       | -1.39    |\n","|    explained_variance | 0.985    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 7999     |\n","|    policy_loss        | -0.00841 |\n","|    value_loss         | 0.0282   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.82e+03 |\n","|    ep_rew_mean        | 236      |\n","| time/                 |          |\n","|    fps                | 399      |\n","|    iterations         | 8100     |\n","|    time_elapsed       | 1620     |\n","|    total_timesteps    | 648000   |\n","| train/                |          |\n","|    entropy_loss       | -1.42    |\n","|    explained_variance | 0.975    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8099     |\n","|    policy_loss        | 0.0199   |\n","|    value_loss         | 0.034    |\n","------------------------------------\n","Eval num_timesteps=650000, episode_reward=247.00 +/- 48.33\n","Episode length: 3077.40 +/- 650.95\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.08e+03 |\n","|    mean_reward        | 247      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.82e+03 |\n","|    ep_rew_mean        | 236      |\n","| time/                 |          |\n","|    fps                | 398      |\n","|    iterations         | 8200     |\n","|    time_elapsed       | 1644     |\n","|    total_timesteps    | 656000   |\n","| train/                |          |\n","|    entropy_loss       | -1.44    |\n","|    explained_variance | 0.996    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8199     |\n","|    policy_loss        | 0.013    |\n","|    value_loss         | 0.00162  |\n","------------------------------------\n","Eval num_timesteps=660000, episode_reward=241.00 +/- 43.06\n","Episode length: 2901.00 +/- 142.63\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.9e+03  |\n","|    mean_reward        | 241      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.83e+03 |\n","|    ep_rew_mean        | 236      |\n","| time/                 |          |\n","|    fps                | 398      |\n","|    iterations         | 8300     |\n","|    time_elapsed       | 1666     |\n","|    total_timesteps    | 664000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.981    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8299     |\n","|    policy_loss        | 0.000234 |\n","|    value_loss         | 0.0246   |\n","------------------------------------\n","Eval num_timesteps=670000, episode_reward=236.00 +/- 34.99\n","Episode length: 2809.40 +/- 199.40\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.81e+03 |\n","|    mean_reward        | 236      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.82e+03 |\n","|    ep_rew_mean        | 237      |\n","| time/                 |          |\n","|    fps                | 397      |\n","|    iterations         | 8400     |\n","|    time_elapsed       | 1689     |\n","|    total_timesteps    | 672000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.998    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8399     |\n","|    policy_loss        | -0.00665 |\n","|    value_loss         | 0.00457  |\n","------------------------------------\n","Eval num_timesteps=680000, episode_reward=226.00 +/- 33.82\n","Episode length: 2715.80 +/- 185.87\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.72e+03 |\n","|    mean_reward        | 226      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.83e+03 |\n","|    ep_rew_mean        | 240      |\n","| time/                 |          |\n","|    fps                | 397      |\n","|    iterations         | 8500     |\n","|    time_elapsed       | 1711     |\n","|    total_timesteps    | 680000   |\n","| train/                |          |\n","|    entropy_loss       | -1.45    |\n","|    explained_variance | 0.98     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8499     |\n","|    policy_loss        | -0.00507 |\n","|    value_loss         | 0.0248   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.83e+03 |\n","|    ep_rew_mean        | 240      |\n","| time/                 |          |\n","|    fps                | 398      |\n","|    iterations         | 8600     |\n","|    time_elapsed       | 1725     |\n","|    total_timesteps    | 688000   |\n","| train/                |          |\n","|    entropy_loss       | -1.41    |\n","|    explained_variance | 0.978    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8599     |\n","|    policy_loss        | -0.0338  |\n","|    value_loss         | 0.0339   |\n","------------------------------------\n","Eval num_timesteps=690000, episode_reward=248.00 +/- 37.63\n","Episode length: 2879.40 +/- 122.53\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.88e+03 |\n","|    mean_reward        | 248      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.86e+03 |\n","|    ep_rew_mean        | 244      |\n","| time/                 |          |\n","|    fps                | 398      |\n","|    iterations         | 8700     |\n","|    time_elapsed       | 1747     |\n","|    total_timesteps    | 696000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.992    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8699     |\n","|    policy_loss        | 0.00583  |\n","|    value_loss         | 0.00982  |\n","------------------------------------\n","Eval num_timesteps=700000, episode_reward=255.00 +/- 23.24\n","Episode length: 2920.60 +/- 173.10\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.92e+03 |\n","|    mean_reward        | 255      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.86e+03 |\n","|    ep_rew_mean        | 245      |\n","| time/                 |          |\n","|    fps                | 397      |\n","|    iterations         | 8800     |\n","|    time_elapsed       | 1770     |\n","|    total_timesteps    | 704000   |\n","| train/                |          |\n","|    entropy_loss       | -1.41    |\n","|    explained_variance | 0.968    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8799     |\n","|    policy_loss        | -0.0101  |\n","|    value_loss         | 0.0277   |\n","------------------------------------\n","Eval num_timesteps=710000, episode_reward=204.00 +/- 22.45\n","Episode length: 2645.80 +/- 143.36\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.65e+03 |\n","|    mean_reward        | 204      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.9e+03  |\n","|    ep_rew_mean        | 249      |\n","| time/                 |          |\n","|    fps                | 397      |\n","|    iterations         | 8900     |\n","|    time_elapsed       | 1792     |\n","|    total_timesteps    | 712000   |\n","| train/                |          |\n","|    entropy_loss       | -1.44    |\n","|    explained_variance | 0.962    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8899     |\n","|    policy_loss        | 0.0489   |\n","|    value_loss         | 0.0526   |\n","------------------------------------\n","Eval num_timesteps=720000, episode_reward=241.00 +/- 43.17\n","Episode length: 2857.40 +/- 232.86\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.86e+03 |\n","|    mean_reward        | 241      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.88e+03 |\n","|    ep_rew_mean        | 247      |\n","| time/                 |          |\n","|    fps                | 396      |\n","|    iterations         | 9000     |\n","|    time_elapsed       | 1814     |\n","|    total_timesteps    | 720000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.952    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 8999     |\n","|    policy_loss        | 0.0374   |\n","|    value_loss         | 0.047    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.9e+03  |\n","|    ep_rew_mean        | 249      |\n","| time/                 |          |\n","|    fps                | 398      |\n","|    iterations         | 9100     |\n","|    time_elapsed       | 1828     |\n","|    total_timesteps    | 728000   |\n","| train/                |          |\n","|    entropy_loss       | -1.44    |\n","|    explained_variance | 0.969    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9099     |\n","|    policy_loss        | -0.0154  |\n","|    value_loss         | 0.0227   |\n","------------------------------------\n","Eval num_timesteps=730000, episode_reward=198.00 +/- 14.70\n","Episode length: 2567.00 +/- 7.16\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.57e+03 |\n","|    mean_reward        | 198      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.88e+03 |\n","|    ep_rew_mean        | 250      |\n","| time/                 |          |\n","|    fps                | 397      |\n","|    iterations         | 9200     |\n","|    time_elapsed       | 1850     |\n","|    total_timesteps    | 736000   |\n","| train/                |          |\n","|    entropy_loss       | -1.44    |\n","|    explained_variance | 0.99     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9199     |\n","|    policy_loss        | 0.00815  |\n","|    value_loss         | 0.0155   |\n","------------------------------------\n","Eval num_timesteps=740000, episode_reward=228.00 +/- 40.69\n","Episode length: 2793.00 +/- 174.25\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.79e+03 |\n","|    mean_reward        | 228      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.92e+03 |\n","|    ep_rew_mean        | 249      |\n","| time/                 |          |\n","|    fps                | 397      |\n","|    iterations         | 9300     |\n","|    time_elapsed       | 1873     |\n","|    total_timesteps    | 744000   |\n","| train/                |          |\n","|    entropy_loss       | -1.4     |\n","|    explained_variance | 0.996    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9299     |\n","|    policy_loss        | 0.0245   |\n","|    value_loss         | 0.00466  |\n","------------------------------------\n","Eval num_timesteps=750000, episode_reward=256.00 +/- 24.78\n","Episode length: 2881.80 +/- 163.41\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.88e+03 |\n","|    mean_reward        | 256      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.9e+03  |\n","|    ep_rew_mean        | 250      |\n","| time/                 |          |\n","|    fps                | 396      |\n","|    iterations         | 9400     |\n","|    time_elapsed       | 1895     |\n","|    total_timesteps    | 752000   |\n","| train/                |          |\n","|    entropy_loss       | -1.41    |\n","|    explained_variance | 0.973    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9399     |\n","|    policy_loss        | 0.00742  |\n","|    value_loss         | 0.0608   |\n","------------------------------------\n","Eval num_timesteps=760000, episode_reward=245.00 +/- 53.10\n","Episode length: 2777.80 +/- 178.93\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.78e+03 |\n","|    mean_reward        | 245      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.9e+03  |\n","|    ep_rew_mean        | 251      |\n","| time/                 |          |\n","|    fps                | 396      |\n","|    iterations         | 9500     |\n","|    time_elapsed       | 1917     |\n","|    total_timesteps    | 760000   |\n","| train/                |          |\n","|    entropy_loss       | -1.38    |\n","|    explained_variance | 0.973    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9499     |\n","|    policy_loss        | 0.0425   |\n","|    value_loss         | 0.0373   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.89e+03 |\n","|    ep_rew_mean        | 248      |\n","| time/                 |          |\n","|    fps                | 397      |\n","|    iterations         | 9600     |\n","|    time_elapsed       | 1931     |\n","|    total_timesteps    | 768000   |\n","| train/                |          |\n","|    entropy_loss       | -1.37    |\n","|    explained_variance | 0.978    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9599     |\n","|    policy_loss        | 0.0401   |\n","|    value_loss         | 0.0423   |\n","------------------------------------\n","Eval num_timesteps=770000, episode_reward=230.00 +/- 40.99\n","Episode length: 2854.60 +/- 201.51\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.85e+03 |\n","|    mean_reward        | 230      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.87e+03 |\n","|    ep_rew_mean        | 244      |\n","| time/                 |          |\n","|    fps                | 397      |\n","|    iterations         | 9700     |\n","|    time_elapsed       | 1953     |\n","|    total_timesteps    | 776000   |\n","| train/                |          |\n","|    entropy_loss       | -1.42    |\n","|    explained_variance | 0.959    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9699     |\n","|    policy_loss        | 0.0283   |\n","|    value_loss         | 0.0651   |\n","------------------------------------\n","Eval num_timesteps=780000, episode_reward=254.00 +/- 31.37\n","Episode length: 2849.80 +/- 149.71\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.85e+03 |\n","|    mean_reward        | 254      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.85e+03 |\n","|    ep_rew_mean        | 240      |\n","| time/                 |          |\n","|    fps                | 396      |\n","|    iterations         | 9800     |\n","|    time_elapsed       | 1976     |\n","|    total_timesteps    | 784000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.991    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9799     |\n","|    policy_loss        | -0.0224  |\n","|    value_loss         | 0.0114   |\n","------------------------------------\n","Eval num_timesteps=790000, episode_reward=262.00 +/- 68.16\n","Episode length: 3237.00 +/- 628.27\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.24e+03 |\n","|    mean_reward        | 262      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.86e+03 |\n","|    ep_rew_mean        | 244      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 9900     |\n","|    time_elapsed       | 2000     |\n","|    total_timesteps    | 792000   |\n","| train/                |          |\n","|    entropy_loss       | -1.36    |\n","|    explained_variance | 0.972    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9899     |\n","|    policy_loss        | 0.093    |\n","|    value_loss         | 0.0361   |\n","------------------------------------\n","Eval num_timesteps=800000, episode_reward=265.00 +/- 75.76\n","Episode length: 2782.20 +/- 292.08\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.78e+03 |\n","|    mean_reward        | 265      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.85e+03 |\n","|    ep_rew_mean        | 242      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 10000    |\n","|    time_elapsed       | 2022     |\n","|    total_timesteps    | 800000   |\n","| train/                |          |\n","|    entropy_loss       | -1.41    |\n","|    explained_variance | 0.987    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 9999     |\n","|    policy_loss        | 0.0307   |\n","|    value_loss         | 0.0179   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.86e+03 |\n","|    ep_rew_mean        | 241      |\n","| time/                 |          |\n","|    fps                | 396      |\n","|    iterations         | 10100    |\n","|    time_elapsed       | 2036     |\n","|    total_timesteps    | 808000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.988    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10099    |\n","|    policy_loss        | 0.0121   |\n","|    value_loss         | 0.0166   |\n","------------------------------------\n","Eval num_timesteps=810000, episode_reward=238.00 +/- 24.82\n","Episode length: 2824.20 +/- 201.41\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.82e+03 |\n","|    mean_reward        | 238      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.83e+03 |\n","|    ep_rew_mean        | 243      |\n","| time/                 |          |\n","|    fps                | 396      |\n","|    iterations         | 10200    |\n","|    time_elapsed       | 2058     |\n","|    total_timesteps    | 816000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.991    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10199    |\n","|    policy_loss        | -0.0998  |\n","|    value_loss         | 0.0157   |\n","------------------------------------\n","Eval num_timesteps=820000, episode_reward=258.00 +/- 26.38\n","Episode length: 2953.00 +/- 228.83\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.95e+03 |\n","|    mean_reward        | 258      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.83e+03 |\n","|    ep_rew_mean        | 241      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 10300    |\n","|    time_elapsed       | 2081     |\n","|    total_timesteps    | 824000   |\n","| train/                |          |\n","|    entropy_loss       | -1.45    |\n","|    explained_variance | 0.972    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10299    |\n","|    policy_loss        | -0.0288  |\n","|    value_loss         | 0.0454   |\n","------------------------------------\n","Eval num_timesteps=830000, episode_reward=260.00 +/- 27.57\n","Episode length: 2983.40 +/- 264.05\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.98e+03 |\n","|    mean_reward        | 260      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.82e+03 |\n","|    ep_rew_mean        | 238      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 10400    |\n","|    time_elapsed       | 2104     |\n","|    total_timesteps    | 832000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.985    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10399    |\n","|    policy_loss        | 0.0733   |\n","|    value_loss         | 0.0272   |\n","------------------------------------\n","Eval num_timesteps=840000, episode_reward=212.00 +/- 39.19\n","Episode length: 2727.00 +/- 183.93\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.73e+03 |\n","|    mean_reward        | 212      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.81e+03 |\n","|    ep_rew_mean        | 234      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 10500    |\n","|    time_elapsed       | 2126     |\n","|    total_timesteps    | 840000   |\n","| train/                |          |\n","|    entropy_loss       | -1.41    |\n","|    explained_variance | 0.988    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10499    |\n","|    policy_loss        | 0.0182   |\n","|    value_loss         | 0.0161   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.81e+03 |\n","|    ep_rew_mean        | 236      |\n","| time/                 |          |\n","|    fps                | 396      |\n","|    iterations         | 10600    |\n","|    time_elapsed       | 2140     |\n","|    total_timesteps    | 848000   |\n","| train/                |          |\n","|    entropy_loss       | -1.41    |\n","|    explained_variance | 0.99     |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10599    |\n","|    policy_loss        | -0.00749 |\n","|    value_loss         | 0.0132   |\n","------------------------------------\n","Eval num_timesteps=850000, episode_reward=235.00 +/- 31.62\n","Episode length: 2944.60 +/- 202.33\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.94e+03 |\n","|    mean_reward        | 235      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.81e+03 |\n","|    ep_rew_mean        | 236      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 10700    |\n","|    time_elapsed       | 2162     |\n","|    total_timesteps    | 856000   |\n","| train/                |          |\n","|    entropy_loss       | -1.47    |\n","|    explained_variance | 0.983    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10699    |\n","|    policy_loss        | -0.0254  |\n","|    value_loss         | 0.00868  |\n","------------------------------------\n","Eval num_timesteps=860000, episode_reward=209.00 +/- 46.73\n","Episode length: 2728.60 +/- 189.97\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.73e+03 |\n","|    mean_reward        | 209      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 232      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 10800    |\n","|    time_elapsed       | 2184     |\n","|    total_timesteps    | 864000   |\n","| train/                |          |\n","|    entropy_loss       | -1.41    |\n","|    explained_variance | 0.992    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10799    |\n","|    policy_loss        | -0.046   |\n","|    value_loss         | 0.0148   |\n","------------------------------------\n","Eval num_timesteps=870000, episode_reward=234.00 +/- 33.23\n","Episode length: 2826.20 +/- 165.00\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.83e+03 |\n","|    mean_reward        | 234      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 235      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 10900    |\n","|    time_elapsed       | 2206     |\n","|    total_timesteps    | 872000   |\n","| train/                |          |\n","|    entropy_loss       | -1.42    |\n","|    explained_variance | 0.998    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10899    |\n","|    policy_loss        | -0.00913 |\n","|    value_loss         | 0.00372  |\n","------------------------------------\n","Eval num_timesteps=880000, episode_reward=233.00 +/- 39.95\n","Episode length: 2734.20 +/- 177.00\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.73e+03 |\n","|    mean_reward        | 233      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.79e+03 |\n","|    ep_rew_mean        | 236      |\n","| time/                 |          |\n","|    fps                | 394      |\n","|    iterations         | 11000    |\n","|    time_elapsed       | 2229     |\n","|    total_timesteps    | 880000   |\n","| train/                |          |\n","|    entropy_loss       | -1.44    |\n","|    explained_variance | 0.998    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 10999    |\n","|    policy_loss        | -0.0656  |\n","|    value_loss         | 0.00491  |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.81e+03 |\n","|    ep_rew_mean        | 236      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 11100    |\n","|    time_elapsed       | 2242     |\n","|    total_timesteps    | 888000   |\n","| train/                |          |\n","|    entropy_loss       | -1.4     |\n","|    explained_variance | 0.987    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11099    |\n","|    policy_loss        | -0.137   |\n","|    value_loss         | 0.0355   |\n","------------------------------------\n","Eval num_timesteps=890000, episode_reward=269.00 +/- 8.60\n","Episode length: 3195.40 +/- 466.96\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.2e+03  |\n","|    mean_reward        | 269      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 236      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 11200    |\n","|    time_elapsed       | 2265     |\n","|    total_timesteps    | 896000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.981    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11199    |\n","|    policy_loss        | 0.0306   |\n","|    value_loss         | 0.0158   |\n","------------------------------------\n","Eval num_timesteps=900000, episode_reward=242.00 +/- 40.07\n","Episode length: 2808.60 +/- 176.52\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.81e+03 |\n","|    mean_reward        | 242      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 240      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 11300    |\n","|    time_elapsed       | 2288     |\n","|    total_timesteps    | 904000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.997    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11299    |\n","|    policy_loss        | 0.0437   |\n","|    value_loss         | 0.0081   |\n","------------------------------------\n","Eval num_timesteps=910000, episode_reward=279.00 +/- 59.19\n","Episode length: 2811.40 +/- 300.59\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.81e+03 |\n","|    mean_reward        | 279      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 240      |\n","| time/                 |          |\n","|    fps                | 394      |\n","|    iterations         | 11400    |\n","|    time_elapsed       | 2310     |\n","|    total_timesteps    | 912000   |\n","| train/                |          |\n","|    entropy_loss       | -1.44    |\n","|    explained_variance | 0.996    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11399    |\n","|    policy_loss        | 0.0264   |\n","|    value_loss         | 0.00808  |\n","------------------------------------\n","Eval num_timesteps=920000, episode_reward=221.00 +/- 39.55\n","Episode length: 2757.80 +/- 156.53\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.76e+03 |\n","|    mean_reward        | 221      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.81e+03 |\n","|    ep_rew_mean        | 241      |\n","| time/                 |          |\n","|    fps                | 394      |\n","|    iterations         | 11500    |\n","|    time_elapsed       | 2332     |\n","|    total_timesteps    | 920000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.988    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11499    |\n","|    policy_loss        | -0.00656 |\n","|    value_loss         | 0.0152   |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.81e+03 |\n","|    ep_rew_mean        | 244      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 11600    |\n","|    time_elapsed       | 2345     |\n","|    total_timesteps    | 928000   |\n","| train/                |          |\n","|    entropy_loss       | -1.42    |\n","|    explained_variance | 0.996    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11599    |\n","|    policy_loss        | -0.0149  |\n","|    value_loss         | 0.00726  |\n","------------------------------------\n","Eval num_timesteps=930000, episode_reward=250.00 +/- 35.07\n","Episode length: 2883.80 +/- 148.42\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.88e+03 |\n","|    mean_reward        | 250      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 240      |\n","| time/                 |          |\n","|    fps                | 395      |\n","|    iterations         | 11700    |\n","|    time_elapsed       | 2368     |\n","|    total_timesteps    | 936000   |\n","| train/                |          |\n","|    entropy_loss       | -1.46    |\n","|    explained_variance | 0.979    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11699    |\n","|    policy_loss        | 0.0366   |\n","|    value_loss         | 0.0158   |\n","------------------------------------\n","Eval num_timesteps=940000, episode_reward=273.00 +/- 14.00\n","Episode length: 3087.00 +/- 181.04\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.09e+03 |\n","|    mean_reward        | 273      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 242      |\n","| time/                 |          |\n","|    fps                | 394      |\n","|    iterations         | 11800    |\n","|    time_elapsed       | 2392     |\n","|    total_timesteps    | 944000   |\n","| train/                |          |\n","|    entropy_loss       | -1.35    |\n","|    explained_variance | 0.987    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11799    |\n","|    policy_loss        | -0.0934  |\n","|    value_loss         | 0.0236   |\n","------------------------------------\n","Eval num_timesteps=950000, episode_reward=269.00 +/- 18.00\n","Episode length: 3054.60 +/- 125.43\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.05e+03 |\n","|    mean_reward        | 269      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 240      |\n","| time/                 |          |\n","|    fps                | 394      |\n","|    iterations         | 11900    |\n","|    time_elapsed       | 2415     |\n","|    total_timesteps    | 952000   |\n","| train/                |          |\n","|    entropy_loss       | -1.42    |\n","|    explained_variance | 0.995    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11899    |\n","|    policy_loss        | 0.00794  |\n","|    value_loss         | 0.01     |\n","------------------------------------\n","Eval num_timesteps=960000, episode_reward=255.00 +/- 24.49\n","Episode length: 2877.00 +/- 140.56\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.88e+03 |\n","|    mean_reward        | 255      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 241      |\n","| time/                 |          |\n","|    fps                | 393      |\n","|    iterations         | 12000    |\n","|    time_elapsed       | 2438     |\n","|    total_timesteps    | 960000   |\n","| train/                |          |\n","|    entropy_loss       | -1.42    |\n","|    explained_variance | 0.994    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 11999    |\n","|    policy_loss        | 0.00387  |\n","|    value_loss         | 0.011    |\n","------------------------------------\n","------------------------------------\n","| rollout/              |          |\n","|    ep_len_mean        | 2.82e+03 |\n","|    ep_rew_mean        | 244      |\n","| time/                 |          |\n","|    fps                | 394      |\n","|    iterations         | 12100    |\n","|    time_elapsed       | 2453     |\n","|    total_timesteps    | 968000   |\n","| train/                |          |\n","|    entropy_loss       | -1.42    |\n","|    explained_variance | 0.989    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12099    |\n","|    policy_loss        | 0.00211  |\n","|    value_loss         | 0.0101   |\n","------------------------------------\n","Eval num_timesteps=970000, episode_reward=246.00 +/- 48.00\n","Episode length: 3039.40 +/- 642.19\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.04e+03 |\n","|    mean_reward        | 246      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.82e+03 |\n","|    ep_rew_mean        | 245      |\n","| time/                 |          |\n","|    fps                | 393      |\n","|    iterations         | 12200    |\n","|    time_elapsed       | 2477     |\n","|    total_timesteps    | 976000   |\n","| train/                |          |\n","|    entropy_loss       | -1.43    |\n","|    explained_variance | 0.985    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12199    |\n","|    policy_loss        | 0.0107   |\n","|    value_loss         | 0.0129   |\n","------------------------------------\n","Eval num_timesteps=980000, episode_reward=274.00 +/- 15.94\n","Episode length: 3015.00 +/- 124.08\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 3.02e+03 |\n","|    mean_reward        | 274      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.8e+03  |\n","|    ep_rew_mean        | 242      |\n","| time/                 |          |\n","|    fps                | 393      |\n","|    iterations         | 12300    |\n","|    time_elapsed       | 2500     |\n","|    total_timesteps    | 984000   |\n","| train/                |          |\n","|    entropy_loss       | -1.4     |\n","|    explained_variance | 0.985    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12299    |\n","|    policy_loss        | 0.0194   |\n","|    value_loss         | 0.018    |\n","------------------------------------\n","Eval num_timesteps=990000, episode_reward=212.00 +/- 39.19\n","Episode length: 2776.20 +/- 161.01\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.78e+03 |\n","|    mean_reward        | 212      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.79e+03 |\n","|    ep_rew_mean        | 240      |\n","| time/                 |          |\n","|    fps                | 393      |\n","|    iterations         | 12400    |\n","|    time_elapsed       | 2522     |\n","|    total_timesteps    | 992000   |\n","| train/                |          |\n","|    entropy_loss       | -1.41    |\n","|    explained_variance | 0.995    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12399    |\n","|    policy_loss        | -0.0313  |\n","|    value_loss         | 0.00786  |\n","------------------------------------\n","Eval num_timesteps=1000000, episode_reward=249.00 +/- 36.39\n","Episode length: 2930.20 +/- 311.66\n","------------------------------------\n","| eval/                 |          |\n","|    mean_ep_length     | 2.93e+03 |\n","|    mean_reward        | 249      |\n","| rollout/              |          |\n","|    ep_len_mean        | 2.81e+03 |\n","|    ep_rew_mean        | 242      |\n","| time/                 |          |\n","|    fps                | 392      |\n","|    iterations         | 12500    |\n","|    time_elapsed       | 2545     |\n","|    total_timesteps    | 1000000  |\n","| train/                |          |\n","|    entropy_loss       | -1.44    |\n","|    explained_variance | 0.986    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 12499    |\n","|    policy_loss        | 0.054    |\n","|    value_loss         | 0.0205   |\n","------------------------------------\n","Saving to logs/a2c/SpaceInvadersNoFrameskip-v4_1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RFI5fwobBCXA"},"source":["#### Evaluate trained agent\n","\n","\n","You can remove the `--folder logs/` to evaluate pretrained agent."]},{"cell_type":"code","metadata":{"id":"iRW6D55yBCXA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625081809264,"user_tz":-120,"elapsed":19398,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"198bfb17-1ae4-452a-8df4-67c3dda72665"},"source":["!python enjoy.py --algo a2c --env SpaceInvadersNoFrameskip-v4 --no-render --n-timesteps 5000 --folder logs/"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Loading latest experiment, id=1\n","Loading logs/a2c/SpaceInvadersNoFrameskip-v4_1/SpaceInvadersNoFrameskip-v4.zip\n","Stacking 4 frames\n","Wrapping the env in a VecTransposeImage.\n","Atari Episode Score: 180.00\n","Atari Episode Length 2657\n","Atari Episode Score: 240.00\n","Atari Episode Length 2921\n","Atari Episode Score: 265.00\n","Atari Episode Length 2953\n","Atari Episode Score: 250.00\n","Atari Episode Length 2871\n","Atari Episode Score: 290.00\n","Atari Episode Length 2941\n","Atari Episode Score: 260.00\n","Atari Episode Length 2955\n","Atari Episode Score: 260.00\n","Atari Episode Length 3045\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dii4qSVGBCXA"},"source":["### Record  a Video"]},{"cell_type":"code","metadata":{"id":"BvDDniLIBCXA","executionInfo":{"status":"ok","timestamp":1625081809265,"user_tz":-120,"elapsed":13,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}}},"source":["# Set up display; otherwise rendering will fail\n","import os\n","os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n","os.environ['DISPLAY'] = ':1'"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"ENbGgUL2BCXB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625081821864,"user_tz":-120,"elapsed":12612,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"872e5757-1b89-4cff-dce6-f93f2639ab31"},"source":["!python -m utils.record_video --algo a2c --env SpaceInvadersNoFrameskip-v4 --exp-id 0 -f logs/ -n 1000"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Loading latest experiment, id=1\n","Stacking 4 frames\n","Saving video to /content/drive/My Drive/Colab Notebooks/TFM/Seaquest/rl-baselines3-zoo/logs/a2c/SpaceInvadersNoFrameskip-v4_1/videos/final-model-a2c-SpaceInvadersNoFrameskip-v4-step-0-to-step-1000.mp4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PEzAiugH6za-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625081822423,"user_tz":-120,"elapsed":212,"user":{"displayName":"Rubén Tobar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihkIY022z__Nquv-FiXCIpD9spENwXjv78UcNDpg=s64","userId":"07315864653075453767"}},"outputId":"987a9363-f284-4494-ccd1-7868d7c2f3b2"},"source":["!nvidia-smi"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Wed Jun 30 19:37:01 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]}]}